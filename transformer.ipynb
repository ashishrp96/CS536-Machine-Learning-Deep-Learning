{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"transformer.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"agricultural-subsection"},"source":["---\n","# Language Modeling with Transformers\n","\n","In this assignment, you will implement Transformers for language modeling.\n","\n","As in previous assignments, you will see code blocks that look like this:\n","```python\n","###############################################################################\n","# TODO: Create a variable x with value 536.                                   #\n","###############################################################################\n","# Replace \"pass\" statement with your code\n","pass\n","# END OF YOUR CODE\n","```\n","\n","You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n","```python\n","###############################################################################\n","# TODO: Create a variable x with value 536.                                   #\n","###############################################################################\n","# Replace \"pass\" statement with your code\n","x = 536\n","# END OF YOUR CODE\n","```\n","\n","Also, please remember:\n","- Do not write or modify any code outside of code blocks unless otherwise stated.\n","- Do not delete any cells from the notebook. You may add new cells to perform scratch work, but delete them before submitting.\n","- Run all cells before submitting. You will only get credit for code that has been run.\n","- Submit your notebook as `netid.ipynb`, where `netid` is your actual netid.\n","- Your submission will be graded with PyTorch 1.7.0 and Python 3.6, which are the default versions in Google Colab."],"id":"agricultural-subsection"},{"cell_type":"code","metadata":{"id":"dynamic-scanning","executionInfo":{"status":"ok","timestamp":1618620110217,"user_tz":240,"elapsed":3829,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["# You may import additional packages as appropriate\n","from __future__ import print_function\n","from __future__ import division\n","\n","import math\n","import pickle\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from collections import Counter\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.nn.utils.rnn import pack_padded_sequence, invert_permutation\n","from torch.nn.utils import clip_grad_norm_\n","\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)"],"id":"dynamic-scanning","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"pregnant-government","executionInfo":{"status":"ok","timestamp":1618620110223,"user_tz":240,"elapsed":3821,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["assert torch.cuda.is_available(), 'GPU unavailable'"],"id":"pregnant-government","execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"interested-transfer"},"source":["## Data Preparation\n","\n","A preprocessed dataset is provided in `data.pickle`. It contains 50K English sentences, each consisting of 5-8 tokens. We will use 40K sentences for training, and the remaining 10K for validation. Please put the file in the same directory as this notebook. In Google Colab, you have the option to upload files."],"id":"interested-transfer"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"55e73ae999b8942d37b609e25bd6f643","grade":false,"grade_id":"cell-8a2c00bfecd1cc35","locked":true,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"thirty-coffee","executionInfo":{"status":"ok","timestamp":1618620110454,"user_tz":240,"elapsed":4040,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"89cd3544-82f8-4db8-b036-d168c359c919"},"source":["# Load data\n","with open('data.pickle', 'rb') as f:\n","  target_text = pickle.load(f)\n","print(f'There are {len(target_text)} sentences.')\n","\n","# Show a random sample\n","idx = random.randrange(len(target_text))\n","print('Here is a random sample:')\n","print(target_text[idx])\n","\n","# Split sentences into words\n","target_text = [sentence.split(' ') for sentence in target_text]\n","MAX_LEN = max([len(sentence) for sentence in target_text])\n","print(f'The maximum sentence length is {MAX_LEN}.')"],"id":"thirty-coffee","execution_count":3,"outputs":[{"output_type":"stream","text":["There are 50000 sentences.\n","Here is a random sample:\n","is there an information counter ?\n","The maximum sentence length is 8.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"f168879510844b3e1cf178a300c0157c","grade":false,"grade_id":"cell-2a5e07425c6be94e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"designed-surgeon","executionInfo":{"status":"ok","timestamp":1618620110455,"user_tz":240,"elapsed":4027,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class Vocab:\n","  \n","  def __init__(self, text, min_freq=5):\n","    \n","    # Initialize the vocabulary with special tokens\n","    self.index2word = {\n","      0: 'pad',  # padding\n","      1: 'bos',  # begin of sentence\n","      2: 'eos',  # end of sentence\n","      3: 'unk',  # unknown\n","    }\n","    self.word2index = {v: k for k, v in self.index2word.items()}\n","    self.size = 4  # initial vocabulary size\n","    \n","    # Only add tokens that appear more than `min_freq` times in the training data\n","    tokens = [token for sentence in text for token in sentence]\n","    token_freq = Counter(tokens)\n","    tokens = [token for token in token_freq if token_freq[token] >= min_freq]\n","    self._build_vocab(tokens)\n","  \n","  \n","  def _build_vocab(self, tokens):\n","    \n","    for token in tokens:\n","      if token not in self.word2index:\n","        self.word2index[token] = self.size\n","        self.index2word[self.size] = token\n","        self.size += 1\n","  \n","  \n","  def __getitem__(self, tokens):\n","    \n","    if not isinstance(tokens, (list, tuple)):\n","      return self.word2index.get(tokens, self.word2index['unk'])\n","    else:\n","      return [self.__getitem__(token) for token in tokens]"],"id":"designed-surgeon","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"56cb0e72af0392e45ba99d633ed723f4","grade":false,"grade_id":"cell-8fdcf94b4e6d2486","locked":true,"schema_version":3,"solution":false,"task":false},"id":"sustainable-petroleum","executionInfo":{"status":"ok","timestamp":1618620110457,"user_tz":240,"elapsed":4018,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["def get_data_loader(target_text, batch_size):\n","  \"\"\"\n","  Build vocabulary and prepare data.\n","  \"\"\"\n","  # Use the first 40K sentences as training data, and the rest for validation\n","  target_vocab = Vocab(target_text[:40000])\n","  \n","  # Convert each word to its index in the vocabulary\n","  target_text = target_vocab[target_text]\n","  # Add 'bos' and 'eos' tokens\n","  target_text = [[target_vocab['bos']] + sentence + [target_vocab['eos']]\n","                 for sentence in target_text]\n","  # Record the length of each sentence\n","  target_len = [len(sentence) for sentence in target_text]\n","  # Pad each sentence to `MAX_LEN` (+2 for 'bos' and 'eos')\n","  target_text = [sentence + [target_vocab['pad']] * (MAX_LEN + 2 - len(sentence))\n","                 for sentence in target_text]\n","  # Convert to PyTorch tensors\n","  target_text = torch.LongTensor(target_text)  # shape: (num_sentences, MAX_LEN+2)\n","  target_len = torch.LongTensor(target_len)  # shape: (num_sentences,)\n","  \n","  train_set = TensorDataset(\n","    target_text[:40000], target_len[:40000])\n","  \n","  val_set = TensorDataset(\n","    target_text[40000:], target_len[40000:])\n","  \n","  loader_kwargs = {\n","    'batch_size': batch_size,\n","    'shuffle': True,\n","    'num_workers': 4,\n","    'pin_memory': True,\n","    'drop_last': True,\n","  }\n","  \n","  train_loader = DataLoader(train_set, **loader_kwargs)\n","  val_loader = DataLoader(val_set, **loader_kwargs)\n","  \n","  return target_vocab, train_loader, val_loader"],"id":"sustainable-petroleum","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"retained-mouth","executionInfo":{"status":"ok","timestamp":1618620111023,"user_tz":240,"elapsed":4567,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"d70552e9-a0b3-4fb9-e9db-4d2007e5bdd5"},"source":["target_vocab, train_loader, val_loader = get_data_loader(target_text, batch_size=100)\n","print('vocab size:', target_vocab.size)"],"id":"retained-mouth","execution_count":6,"outputs":[{"output_type":"stream","text":["vocab size: 2591\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"cognitive-currency"},"source":["## Language Modeling\n","\n","The goal of language modeling is to learn the conditional distribution of the next word $x_t$ given all the previous words $x_{<t}$:\n","$$\n","P(x_t \\!\\mid\\! x_{<t}).\n","$$\n","This allows us to compute the log-likelihood of a given sentence $x_{1:T}$ as follows:\n","$$\n","\\log P(x_{1:T}) = \\sum_{t=1}^T \\log P(x_t \\!\\mid\\! x_{<t}).\n","$$\n","Given a language model, we can also generate new sentences by conditioning on the first $C$ words $x_{1:C}$ of the sentence:\n","$$\n","\\begin{align*}\n","x_{C+1} &\\sim P(x_{C+1} \\!\\mid\\! x_{1:C}), \\\\\n","x_{C+2} &\\sim P(x_{C+2} \\!\\mid\\! x_{1:C}, x_{C+1}), \\\\\n","&\\cdots\n","\\end{align*}\n","$$\n","If $C=1$ and $x_1 = \\text{'bos'}$, then the model is free to generate any sentence from the distribution it has learned."],"id":"cognitive-currency"},{"cell_type":"markdown","metadata":{"id":"nearby-beginning"},"source":["## Language Modeling with Transformers\n","\n","The Transformer uses a stack of blocks to compute an embedding of all the previous words:\n","$$\n","e_t = f(x_{<t}).\n","$$\n","The next word probability is then computed by a linear projection followed by softmax:\n","$$\n","P(x_t \\!\\mid\\! x_{<t}) = \\text{softmax}(W e_t).\n","$$"],"id":"nearby-beginning"},{"cell_type":"markdown","metadata":{"id":"behind-china"},"source":["## Self-Attention in Transformers\n","\n","A core module in each Transformer block is the multi-head causal self-attention, also known as multi-head masked self-attention. It only attends to positions before or at the current position, as opposed to a regular self-attention that can attend to all positions.\n","\n","<img src=\"https://jalammar.github.io/images/gpt2/self-attention-and-masked-self-attention.png\" width=600>\n","\n","The exact form of multi-head self-attention is defined in this [paper](https://arxiv.org/pdf/1706.03762.pdf). Please read it carefully as it also describes many implementation details."],"id":"behind-china"},{"cell_type":"markdown","metadata":{"id":"written-crowd"},"source":["Implement `MultiHeadCausalSelfAttention` below:"],"id":"written-crowd"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"96f3b5e3ce7e07c7537888b9111893c6","grade":false,"grade_id":"cell-cc9b8ebc3255f311","locked":false,"schema_version":3,"solution":true,"task":false},"id":"natural-factory","executionInfo":{"status":"ok","timestamp":1618620111024,"user_tz":240,"elapsed":4552,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class MultiHeadCausalSelfAttention(nn.Module):\n","  \n","  def __init__(self, d_model, num_heads, dropout=0.):\n","    super().__init__()\n","    \n","    assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n","    self.d_model = d_model\n","    self.num_heads = num_heads\n","    \n","    # This dropout is to be applied to the attention weights\n","    # immediately after the softmax. It was not mentioned\n","    # in the paper, but is used in the actual implementation.\n","    self.dropout = nn.Dropout(dropout)\n","    \n","    # The weight matrices of all heads are concatenated together\n","    self.proj_q = nn.Linear(d_model, d_model, bias=False)\n","    self.proj_k = nn.Linear(d_model, d_model, bias=False)\n","    self.proj_v = nn.Linear(d_model, d_model, bias=False)\n","    \n","    self.proj_o = nn.Linear(d_model, d_model, bias=False)\n","  \n","  \n","  def forward(self, input):\n","    \"\"\"\n","    Inputs:\n","    - input: A Tensor of shape (batch_size, seq_len, d_model)\n","      containing sequences of embeddings.\n","    \n","    Returns:\n","    - output: A Tensor of shape (batch_size, seq_len, d_model)\n","      containing the updated embeddings.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass. No for loop is allowed.                   #\n","    # You may not use PyTorch's built-in multi-head attention.              #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    bs= input.size(0)\n","    Wq = self.proj_q(input)\n","    Wk = self.proj_k(input)\n","    Wv = self.proj_v(input)\n","    hd = int(self.d_model/self.num_heads)\n","    mask = ~torch.triu(input.new_ones((input.size(1),input.size(1)), dtype=torch.bool), diagonal=1)\n","\n","    Wq = Wq.view(bs, -1, self.num_heads, hd).permute(0, 2, 1, 3)\n","    Wk = Wk.view(bs, -1, self.num_heads, hd).permute(0, 2, 1, 3)\n","    Wv = Wv.view(bs, -1, self.num_heads, hd).permute(0, 2, 1, 3)\n","\n","    e = torch.matmul(Wq, Wk.permute(0, 1, 3, 2)) / math.sqrt(hd)\n","    if mask is not None:\n","            e = e.masked_fill(mask == 0, -1e10)\n","    attention = torch.softmax(e, dim = -1)\n","    output = torch.matmul(self.dropout(attention), Wv)\n","    output = output.permute(0, 2, 1, 3).contiguous()\n","    output = output.view(bs, -1, self.d_model)\n","    output = self.proj_o(output)                     \n","    # END OF YOUR CODE\n","    return output"],"id":"natural-factory","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"universal-owner"},"source":["We will use the following helper functions to check your results:"],"id":"universal-owner"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"5673df498d5bba86185d4051698a8a91","grade":false,"grade_id":"cell-b65efd57ea05fcaf","locked":true,"schema_version":3,"solution":false,"task":false},"id":"middle-america","executionInfo":{"status":"ok","timestamp":1618620111025,"user_tz":240,"elapsed":4542,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["def max_diff(actual, expected):\n","  return (actual - expected).abs().max().item()"],"id":"middle-america","execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"a5df51176c451af40b8a32985d41b10e","grade":false,"grade_id":"cell-ec20e98325c74fe2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"necessary-latino","executionInfo":{"status":"ok","timestamp":1618620111026,"user_tz":240,"elapsed":4531,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["def check_attention(batch_size, seq_len, d_model, num_heads, dropout):\n","  \n","  attn = MultiHeadCausalSelfAttention(d_model, num_heads, dropout)\n","  input = torch.empty(batch_size, seq_len, d_model).normal_()\n","  torch.manual_seed(0)\n","  actual_output = attn(input)\n","  \n","  input = input.transpose(0, 1)\n","  in_proj_weight = torch.cat(\n","    (attn.proj_q.weight,\n","     attn.proj_k.weight,\n","     attn.proj_v.weight), dim=0)\n","  attn_mask = torch.triu(\n","    input.new_ones((seq_len, seq_len), dtype=torch.bool),\n","    diagonal=1)\n","  torch.manual_seed(0)\n","  expected_output = F.multi_head_attention_forward(\n","    input, input, input,\n","    d_model, num_heads,\n","    in_proj_weight, None,\n","    None, None, False, dropout,\n","    attn.proj_o.weight, None,\n","    need_weights=False,\n","    attn_mask=attn_mask)[0].transpose(0, 1)\n","  \n","  assert actual_output.shape == expected_output.shape, 'incorrect shape'\n","  \n","  diff = max_diff(actual_output, expected_output)\n","  print('diff = %e' % diff)\n","  assert diff < 1e-5, 'incorrect output'"],"id":"necessary-latino","execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"748e1b21309fdd1414f5deb66a7a31fc","grade":true,"grade_id":"cell-06012883b4f46b9f","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"female-worse","executionInfo":{"status":"ok","timestamp":1618620111918,"user_tz":240,"elapsed":5408,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"2c2dd604-c0aa-4ec8-bbe7-6b717ea17362"},"source":["check_attention(batch_size=64, seq_len=128, d_model=512, num_heads=1, dropout=0.)"],"id":"female-worse","execution_count":10,"outputs":[{"output_type":"stream","text":["diff = 3.613532e-07\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"79b8f6955baa8d43719580f7718e6ff9","grade":true,"grade_id":"cell-75d2cc7c4d1373b5","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"theoretical-western","executionInfo":{"status":"ok","timestamp":1618620112732,"user_tz":240,"elapsed":6204,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"db600dd0-2f86-408c-bd36-fd135e860cd5"},"source":["check_attention(batch_size=64, seq_len=128, d_model=512, num_heads=8, dropout=0.)"],"id":"theoretical-western","execution_count":11,"outputs":[{"output_type":"stream","text":["diff = 1.192093e-07\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"2e7bc0ddd71117e3d829cfb475778817","grade":true,"grade_id":"cell-6f7353e90da5bc5b","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"tropical-organ","executionInfo":{"status":"ok","timestamp":1618620113397,"user_tz":240,"elapsed":6853,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"490182df-ef29-4c6f-a6b9-98fb9bcd15a2"},"source":["check_attention(batch_size=64, seq_len=128, d_model=512, num_heads=1, dropout=0.1)"],"id":"tropical-organ","execution_count":12,"outputs":[{"output_type":"stream","text":["diff = 2.980232e-07\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"87f9fb2c91e722d7113dbf73cc9b9d9f","grade":true,"grade_id":"cell-945b963c20c8af02","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"colab":{"base_uri":"https://localhost:8080/"},"id":"atmospheric-console","executionInfo":{"status":"ok","timestamp":1618620114264,"user_tz":240,"elapsed":7707,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"64e07c93-2f52-40c1-99ae-83c670afc51e"},"source":["check_attention(batch_size=64, seq_len=128, d_model=512, num_heads=8, dropout=0.1)"],"id":"atmospheric-console","execution_count":13,"outputs":[{"output_type":"stream","text":["diff = 1.490116e-07\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"painted-pacific"},"source":["## Transformer Blocks\n","\n","A Transformer block contains the following modules:\n","1. Multi-head causal self-attention\n","2. Dropout\n","3. Residual connection\n","4. Layer normalization\n","5. Position-wise feed-forward network (with hidden size `d_ff`)\n","6. Dropout\n","7. Residual connection\n","8. Layer normalization\n","\n","Please refer to the [paper](https://arxiv.org/pdf/1706.03762.pdf) for more details."],"id":"painted-pacific"},{"cell_type":"markdown","metadata":{"id":"widespread-pressing"},"source":["Implement `TransformerBlock` below:"],"id":"widespread-pressing"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"c6dafef5edd8bf1a54d170f26c1142ca","grade":false,"grade_id":"cell-f2b92acd309534a2","locked":false,"schema_version":3,"solution":true,"task":false},"id":"tutorial-hampton","executionInfo":{"status":"ok","timestamp":1618620114265,"user_tz":240,"elapsed":7699,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class TransformerBlock(nn.Module):\n","  \n","  def __init__(self, d_model, num_heads, d_ff, dropout=0.):\n","    super().__init__()\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Set up the Transformer block components.                              #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    self.d_model= d_model\n","    self.num_heads = num_heads\n","    self.d_ff = d_ff\n","    self.dropout = nn.Dropout(dropout)\n","    self.mhcsa = MultiHeadCausalSelfAttention(d_model, num_heads, dropout)\n","    self.layernorm = nn.LayerNorm(d_model)\n","    self.psffl = nn.Sequential(\n","        nn.Linear(d_model, d_ff),\n","        nn.ReLU(),\n","        nn.Linear(d_ff, d_model),\n","        nn.Dropout(dropout),\n","        nn.LayerNorm(d_model))\n","    # END OF YOUR CODE\n","  \n","  \n","  def forward(self, input):\n","    \"\"\"\n","    Inputs:\n","    - input: A Tensor of shape (batch_size, seq_len, d_model)\n","      containing sequences of embeddings.\n","    \n","    Returns:\n","    - output: A Tensor of shape (batch_size, seq_len, d_model)\n","      containing the updated embeddings.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass.                                           #\n","    # You may not use PyTorch's built-in Transformer layers.                #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    x = self.mhcsa(input)\n","    x = self.dropout(input)\n","    x = x+input\n","    xl = self.layernorm(x)\n","    xy = self.psffl(xl)\n","    xi = self.dropout(xl)\n","    xi = xi+xy\n","    output = self.layernorm(xi)\n","    # END OF YOUR CODE\n","    return output"],"id":"tutorial-hampton","execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"collect-church"},"source":["## Positional Encoding\n","\n","Self-attention is permutation invariant, meaning it loses the order information. To make use of the order of the sequence, Transformers add positional encodings to the word embeddings before feeding them to the Transformer blocks:\n","\n","$$\n","\\begin{align*}\n","PE_{(pos, 2i)} &= \\text{sin}(pos / 10000^{2i/d_{model}}), \\\\\n","PE_{(pos, 2i+1)} &= \\text{cos}(pos / 10000^{2i/d_{model}}),\n","\\end{align*}\n","$$\n","where $pos$ is the position and $i$ is the dimension.\n","\n","For efficiency, the positional encodings are precomputed for $pos \\in [0, \\text{max_len})$, and the sentences are assumed to have length no greater than $\\text{max_len}$."],"id":"collect-church"},{"cell_type":"markdown","metadata":{"id":"turkish-whole"},"source":["Implement `PositionalEncoding` below:"],"id":"turkish-whole"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"c3a4181842863c34d5926cdbe9189fa6","grade":false,"grade_id":"cell-aff8acaf70d47322","locked":false,"schema_version":3,"solution":true,"task":false},"id":"academic-prototype","executionInfo":{"status":"ok","timestamp":1618620114266,"user_tz":240,"elapsed":7690,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class PositionalEncoding(nn.Module):\n","  \n","  def __init__(self, max_len, d_model, dropout=0.):\n","    super().__init__()\n","    \n","    self.dropout = nn.Dropout(dropout)\n","    \n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Precompute the positional encodings.                                  #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    pe = torch.zeros(max_len, d_model)\n","    pos = torch.arange(0, max_len).unsqueeze(1)\n","    div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n","    \n","    pe[:, 0::2] = torch.sin(pos * div_term)\n","    pe[:, 1::2] = torch.cos(pos * div_term)\n","\n","    pe = pe.unsqueeze(0)\n","    self.register_buffer('pe', pe)\n","    \n","    # END OF YOUR CODE\n","  \n","  \n","  def forward(self, input):\n","    \"\"\"\n","    Inputs:\n","    - input: A Tensor of shape (batch_size, seq_len, d_model)\n","      containing sequences of word embeddings.\n","    \n","    Returns:\n","    - output: A Tensor of shape (batch_size, seq_len, d_model)\n","      containing dropout(word embeddings + positional encodings).\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass.                                           #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    batch_size, seq_len, d_model = input.size()\n","    x = input\n","    x = input + self.pe[:, :seq_len]\n","    output = self.dropout(x)\n","    # END OF YOUR CODE\n","    return output"],"id":"academic-prototype","execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"proprietary-discrimination"},"source":["## Transformer Model\n","\n","Implement `Transformer` below. The model is trained by minimizing cross-entropy for next word prediction. Please refer to the [paper](https://arxiv.org/pdf/1706.03762.pdf) for more details, especially how the weights are shared between the embedding layer and the pre-softmax linear transformation."],"id":"proprietary-discrimination"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"93f338ae2f405427c8fe0dd5464bb2c9","grade":false,"grade_id":"cell-93007f53bbf3301b","locked":false,"schema_version":3,"solution":true,"task":false},"id":"fluid-raise","executionInfo":{"status":"ok","timestamp":1618620114418,"user_tz":240,"elapsed":7834,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class Transformer(nn.Module):\n","  \n","  def __init__(self, num_blocks, vocab_size, max_len, d_model, num_heads, d_ff, dropout=0.):\n","    super().__init__()\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Set up the Transformer components.                                    #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    self.num_blocks= num_blocks\n","    self.num_heads= num_heads\n","    self.token_emb = nn.Embedding(vocab_size, d_model)\n","    self.pos_encod = PositionalEncoding(max_len, d_model)\n","    self.Tb = TransformerBlock(d_model, num_heads, d_ff)\n","    self.linear = nn.Linear(d_model, vocab_size)\n","    self.softmax = nn.Softmax(dim=-1)\n","    self.dropout = nn.Dropout(dropout)\n","  \n","  \n","  def forward(self, input_text, input_len):\n","    \"\"\"\n","    Perform next word prediction and compute loss.\n","    \n","    Inputs:\n","    - input_text: A LongTensor of shape (batch_size, seq_len)\n","      containing a mini-batch of sentences padded to `seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - input_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each sentence (before being padded).\n","    \n","    Returns:\n","    - loss: A PyTorch scalar containing the loss for the mini-batch.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass.                                           #\n","    # You may not use PyTorch's built-in Transformer layers.                #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    tokens = self.token_emb(input_text)\n","    pos_emb = self.pos_encod(tokens)\n","    y = self.dropout(pos_emb)\n","    for _ in range(self.num_blocks):\n","      tb_out=self.Tb(y)\n","      y=tb_out\n","    y = self.linear(y)\n","\n","    input_len, sorted_indices = torch.sort(input_len, descending=True)\n","    input_text = input_text.index_select(0, sorted_indices)\n","    y = y.index_select(0, sorted_indices)\n","    input_len = input_len.cpu()\n","\n","    input_text_packed = pack_padded_sequence(\n","      input_text[:, 1:], input_len - 1,  # notice the shift in `target_text`\n","      batch_first=True, enforce_sorted=True)\n","    \n","    pred_packed = pack_padded_sequence(\n","      y, input_len - 1,\n","      batch_first=True, enforce_sorted=True)\n","    loss = F.cross_entropy(pred_packed.data, input_text_packed.data, ignore_index=0)\n","\n","    # END OF YOUR CODE\n","    return loss\n","  \n","  \n","  def predict(self, cond_text, pred_len):\n","    \"\"\"\n","    Perform sentence completion given the first few words in the sentence.\n","    This is achieved by iteratively predicting the next word for `pred_len` steps,\n","    where the predicted word at step t is concatenated to the input for step t+1.\n","    \n","    Inputs:\n","    - cond_text: A LongTensor of shape (batch_size, cond_len)\n","      containing a mini-batch of sentences to be completed.\n","      Each token is represented by its index in the vocabulary.\n","    - pred_len: (int) number of prediction steps.\n","    \n","    Returns:\n","    - pred: A LongTensor of shape (batch_size, cond_len + pred_len)\n","      containing the completed sentences.\n","      Each token is represented by its index in the vocabulary.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement sentence completion.                                        #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    for _ in range(pred_len):\n","      tokens = self.token_emb(cond_text)\n","      pos_emb = self.pos_encod(tokens)\n","      y = self.dropout(pos_emb)\n","      for _ in range(self.num_blocks):\n","        y =self.Tb(y)\n","      y = self.linear(y)\n","      y = self.softmax(y)\n","      m = torch.argmax(y,-1)\n","      pred_token = m[:,-1:]\n","      cond_text = torch.cat([cond_text,pred_token], dim=1)\n","    pred = cond_text\n","    # END OF YOUR CODE\n","    return pred"],"id":"fluid-raise","execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"removable-galaxy"},"source":["## Evaluation\n","\n","We will evaluate the model on accuracy of sentence completion. Please know that we choose this evaluation protocol for simplicity. There are better evaluation metrics that you should use in your own projects."],"id":"removable-galaxy"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9a19aebb8ef4a09bdad874b3160890c7","grade":false,"grade_id":"cell-dc0757fbee6aabe2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"electronic-balance","executionInfo":{"status":"ok","timestamp":1618620114419,"user_tz":240,"elapsed":7825,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["def eval_acc(model, data_loader, num_samples=5):\n","  \n","  with torch.no_grad():\n","    model.eval()\n","    \n","    cond_len = 4  # provide the first 4 tokens (including 'bos')\n","    pred_len = MAX_LEN + 2 - cond_len  # predict the rest\n","    \n","    total = 0\n","    correct = 0\n","    \n","    for batch, data in enumerate(data_loader):\n","      data = [d.cuda() for d in data]\n","      \n","      cond_text = data[0][:, :cond_len]\n","      pred = model.predict(cond_text, pred_len)\n","      \n","      if batch < num_samples:  # show some samples\n","        target_sentence = [target_vocab.index2word[token] for token in data[0][0].tolist()]\n","        pred_sentence = [target_vocab.index2word[token] for token in pred[0].tolist()]\n","        \n","        print(f'=== Sample {batch + 1} ===')\n","        print('target_sentence:', target_sentence)\n","        print('pred_sentence:  ', pred_sentence)\n","      \n","      target_len, sorted_indices = torch.sort(data[1], descending=True)\n","      target_text = data[0].index_select(0, sorted_indices)\n","      pred = pred.index_select(0, sorted_indices)\n","      target_len = target_len.cpu()\n","      \n","      target_text_packed = pack_padded_sequence(\n","        target_text[:, cond_len:], target_len - cond_len,\n","        batch_first=True, enforce_sorted=True)\n","      \n","      pred_packed = pack_padded_sequence(\n","        pred[:, cond_len:], target_len - cond_len,\n","        batch_first=True, enforce_sorted=True)\n","      \n","      total += target_text_packed.data.shape[0]\n","      correct += (pred_packed.data == target_text_packed.data).sum().item()\n","    \n","    acc = 100 * correct / total\n","  \n","  return acc"],"id":"electronic-balance","execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"false-kingston"},"source":["## Training\n","\n","A basic training script is provided below. You need to make modifications as appropriate. Please refer to the [paper](https://arxiv.org/pdf/1706.03762.pdf) for typical hyperparameter values and learning rate schedules.\n","\n","To get full credit, your model should **achieve at least 38% validation accuracy**."],"id":"false-kingston"},{"cell_type":"code","metadata":{"id":"crazy-front","executionInfo":{"status":"ok","timestamp":1618620114419,"user_tz":240,"elapsed":7813,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["def train(model, train_loader, val_loader, num_epochs, learning_rate):\n","  \n","  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","  \n","  loss_history = []\n","  for epoch in range(num_epochs):\n","    val_acc = eval_acc(model, val_loader, num_samples=0)\n","    \n","    model.train()\n","    for batch, data in enumerate(train_loader):\n","      data = [d.cuda() for d in data]\n","      \n","      optimizer.zero_grad()\n","      loss = model(*data)\n","      loss.backward()\n","      clip_grad_norm_(model.parameters(), 1)  # gradient clipping\n","      optimizer.step()\n","      \n","      with torch.no_grad():\n","        loss_history.append(loss.item())\n","        if batch == 0:\n","          print('Train Epoch: {:3} \\t Loss: {:F} \\t Val Acc: {:F}'.format(\n","            epoch, loss.item(), val_acc))\n","  \n","  return model, loss_history"],"id":"crazy-front","execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"canadian-fossil","executionInfo":{"status":"ok","timestamp":1618626471428,"user_tz":240,"elapsed":924101,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"3b0ddad3-cec7-49aa-efbb-491a075a6e37"},"source":["torch.manual_seed(0)\n","\n","batch_size = 100\n","num_epochs = 30\n","learning_rate = 0.0001\n","\n","num_blocks = 9\n","d_model = 512\n","num_heads = 8\n","d_ff = 4 * d_model\n","dropout = 0.1\n","\n","target_vocab, train_loader, val_loader = get_data_loader(target_text, batch_size)\n","model = Transformer(num_blocks, target_vocab.size, MAX_LEN + 2, d_model, num_heads, d_ff, dropout)\n","model = model.cuda()\n","\n","model, loss_history = train(model, train_loader, val_loader, num_epochs, learning_rate)"],"id":"canadian-fossil","execution_count":22,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch:   0 \t Loss: 8.217893 \t Val Acc: 0.044283\n","Train Epoch:   1 \t Loss: 3.412091 \t Val Acc: 23.567443\n","Train Epoch:   2 \t Loss: 3.285695 \t Val Acc: 22.748207\n","Train Epoch:   3 \t Loss: 3.331706 \t Val Acc: 23.494376\n","Train Epoch:   4 \t Loss: 3.112067 \t Val Acc: 23.700292\n","Train Epoch:   5 \t Loss: 3.218395 \t Val Acc: 23.675937\n","Train Epoch:   6 \t Loss: 2.975890 \t Val Acc: 23.295102\n","Train Epoch:   7 \t Loss: 3.070854 \t Val Acc: 23.534231\n","Train Epoch:   8 \t Loss: 3.006568 \t Val Acc: 23.399167\n","Train Epoch:   9 \t Loss: 3.013700 \t Val Acc: 24.484102\n","Train Epoch:  10 \t Loss: 2.938579 \t Val Acc: 23.374812\n","Train Epoch:  11 \t Loss: 2.952747 \t Val Acc: 22.380657\n","Train Epoch:  12 \t Loss: 2.936075 \t Val Acc: 23.731290\n","Train Epoch:  13 \t Loss: 2.905391 \t Val Acc: 24.098840\n","Train Epoch:  14 \t Loss: 2.876745 \t Val Acc: 24.284829\n","Train Epoch:  15 \t Loss: 2.809169 \t Val Acc: 23.950492\n","Train Epoch:  16 \t Loss: 2.880395 \t Val Acc: 23.144540\n","Train Epoch:  17 \t Loss: 2.756679 \t Val Acc: 23.337171\n","Train Epoch:  18 \t Loss: 2.748901 \t Val Acc: 23.600655\n","Train Epoch:  19 \t Loss: 2.816386 \t Val Acc: 22.768134\n","Train Epoch:  20 \t Loss: 2.794082 \t Val Acc: 23.636082\n","Train Epoch:  21 \t Loss: 2.733114 \t Val Acc: 24.067842\n","Train Epoch:  22 \t Loss: 2.746423 \t Val Acc: 22.909840\n","Train Epoch:  23 \t Loss: 2.774392 \t Val Acc: 23.780002\n","Train Epoch:  24 \t Loss: 2.719556 \t Val Acc: 23.467806\n","Train Epoch:  25 \t Loss: 2.667076 \t Val Acc: 24.134266\n","Train Epoch:  26 \t Loss: 2.670216 \t Val Acc: 23.879639\n","Train Epoch:  27 \t Loss: 2.676000 \t Val Acc: 24.349039\n","Train Epoch:  28 \t Loss: 2.696434 \t Val Acc: 23.939421\n","Train Epoch:  29 \t Loss: 2.647768 \t Val Acc: 23.173324\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"decreased-percentage","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1618626515977,"user_tz":240,"elapsed":499,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"f3ac7173-668f-42b5-e0bb-30e72e1d61e7"},"source":["plt.plot(loss_history, 'o')\n","plt.xlabel('Iteration number')\n","plt.ylabel('Loss value')\n","plt.show()"],"id":"decreased-percentage","execution_count":23,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"starting-plastic","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618626557446,"user_tz":240,"elapsed":38521,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"36212d7a-ad14-4765-dd9d-9541ceb9d284"},"source":["train_acc = eval_acc(model, train_loader)\n","print('Training Accuracy:', train_acc)"],"id":"starting-plastic","execution_count":24,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["=== Sample 1 ===\n","target_sentence: ['bos', 'she', 'presented', 'him', 'with', 'the', 'unk', '.', 'eos', 'pad']\n","pred_sentence:   ['bos', 'she', 'presented', 'him', 'to', 'the', 'unk', '.', 'eos', '.']\n","=== Sample 2 ===\n","target_sentence: ['bos', 'the', 'heater', 'doesn', 't', 'work', '.', 'eos', 'pad', 'pad']\n","pred_sentence:   ['bos', 'the', 'heater', 'doesn', 't', 'believe', 'you', '.', 'eos', '.']\n","=== Sample 3 ===\n","target_sentence: ['bos', 'this', 'room', 'is', 'not', 'very', 'large', '.', 'eos', 'pad']\n","pred_sentence:   ['bos', 'this', 'room', 'is', 'unk', '.', 'eos', '.', 'eos', '.']\n","=== Sample 4 ===\n","target_sentence: ['bos', 'i', 'took', 'part', 'in', 'the', 'unk', '.', 'eos', 'pad']\n","pred_sentence:   ['bos', 'i', 'took', 'part', 'in', 'the', 'unk', '.', 'eos', '.']\n","=== Sample 5 ===\n","target_sentence: ['bos', 'he', 's', 'acting', 'on', 'his', 'own', '.', 'eos', 'pad']\n","pred_sentence:   ['bos', 'he', 's', 'acting', 'up', '.', 'eos', '.', 'eos', '.']\n","Training Accuracy: 24.590582424595578\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"26c009e1905748bad2a3192323f36be8","grade":true,"grade_id":"cell-6bab750be42b380e","locked":true,"points":45,"schema_version":3,"solution":false,"task":false},"id":"fixed-ivory","colab":{"base_uri":"https://localhost:8080/","height":524},"executionInfo":{"status":"error","timestamp":1618626670192,"user_tz":240,"elapsed":9883,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"14e02c8d-32f3-41e7-b8ce-9dfde26977d8"},"source":["val_acc = eval_acc(model, val_loader)\n","print('Validation Accuracy:', val_acc)\n","assert val_acc >= 38"],"id":"fixed-ivory","execution_count":26,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["=== Sample 1 ===\n","target_sentence: ['bos', 'i', 'm', 'being', 'unk', '.', 'eos', 'pad', 'pad', 'pad']\n","pred_sentence:   ['bos', 'i', 'm', 'being', 'unk', '.', 'eos', '.', 'eos', '.']\n","=== Sample 2 ===\n","target_sentence: ['bos', 'he', 'has', 'already', 'said', 'yes', '.', 'eos', 'pad', 'pad']\n","pred_sentence:   ['bos', 'he', 'has', 'already', 'unk', '.', 'eos', '.', 'eos', '.']\n","=== Sample 3 ===\n","target_sentence: ['bos', 'germany', 'was', 'once', 'unk', 'with', 'italy', '.', 'eos', 'pad']\n","pred_sentence:   ['bos', 'germany', 'was', 'once', 'a', 'unk', '.', 'eos', '.', 'eos']\n","=== Sample 4 ===\n","target_sentence: ['bos', 'this', 'is', 'a', 'little', 'gift', 'for', 'you', '.', 'eos']\n","pred_sentence:   ['bos', 'this', 'is', 'a', 'unk', '.', 'eos', '.', 'eos', '.']\n","=== Sample 5 ===\n","target_sentence: ['bos', 'would', 'you', 'mind', 'if', 'i', 'smoke', '?', 'eos', 'pad']\n","pred_sentence:   ['bos', 'would', 'you', 'mind', 'if', 'i', 'm', 'busy', '.', 'eos']\n","Validation Accuracy: 23.45894960588079\n"],"name":"stdout"},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-0621829a464b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Validation Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m38\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAssertionError\u001b[0m: "]}]}]}