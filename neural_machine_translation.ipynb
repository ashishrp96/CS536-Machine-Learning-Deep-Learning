{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"neural_machine_translation.ipynb","provenance":[],"collapsed_sections":["medieval-horse"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"starting-wedding"},"source":["---\n","# Neural Machine Translation with Vanilla RNNs, LSTMs, and Attention\n","\n","In this assignment, you will implement Neural Machine Translation (NMT) models using:\n","1. Vanilla RNNs\n","2. LSTMs\n","3. LSTMs with attention\n","\n","As in previous assignments, you will see code blocks that look like this:\n","```python\n","###############################################################################\n","# TODO: Create a variable x with value 536.                                   #\n","###############################################################################\n","# Replace \"pass\" statement with your code\n","pass\n","# END OF YOUR CODE\n","```\n","\n","You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n","```python\n","###############################################################################\n","# TODO: Create a variable x with value 536.                                   #\n","###############################################################################\n","# Replace \"pass\" statement with your code\n","x = 536\n","# END OF YOUR CODE\n","```\n","\n","Also, please remember:\n","- Do not write or modify any code outside of code blocks unless otherwise stated.\n","- Do not delete any cells from the notebook. You may add new cells to perform scratch work, but delete them before submitting.\n","- Run all cells before submitting. You will only get credit for code that has been run.\n","- Submit your notebook as `netid.ipynb`, where `netid` is your actual netid.\n","- Your submission will be graded with PyTorch 1.7.0 and Python 3.6, which are the default versions in Google Colab."],"id":"starting-wedding"},{"cell_type":"code","metadata":{"id":"OsyEfrOWc1n8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617243232626,"user_tz":240,"elapsed":22552,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"7d6f51c7-b643-4e43-a836-8a4ed10f1c08"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"OsyEfrOWc1n8","execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9027852d08e9c4839fbade409c378d55","grade":false,"grade_id":"cell-635e96eb05746e6f","locked":true,"schema_version":3,"solution":false,"task":false},"id":"partial-filling"},"source":["from __future__ import print_function\n","from __future__ import division\n","\n","import math\n","import pickle\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","from collections import Counter\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.nn.utils.rnn import pack_padded_sequence, invert_permutation\n","from torch.nn.utils import clip_grad_norm_\n","\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)"],"id":"partial-filling","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"italic-compromise"},"source":["assert torch.cuda.is_available(), 'GPU unavailable'"],"id":"italic-compromise","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"endless-symbol"},"source":["## Data Preparation\n","\n","We will use the dataset from this PyTorch [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html), and train neural networks to perform French-to-English translation. A preprocessed version is provided in `data.pickle`. Please put the file in the same directory as this notebook. In Google Colab, you have the option to upload files.\n","\n","The preprocessed dataset contains 10K French and English sentence pairs. Each sentence consists of 9-16 tokens. We will use 8K sentence pairs for training, and the remaining 2K for validation."],"id":"endless-symbol"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"8e110f4af61cf1e20523e3970e17936d","grade":false,"grade_id":"cell-46316296fee2d7c2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"flexible-isaac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617243242959,"user_tz":240,"elapsed":1272,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"7dde8546-2523-4031-ab58-5eae5f85b472"},"source":["# Load data\n","with open('/content/drive/MyDrive/Machine Learning/Assignment 3/data.pickle', 'rb') as f:\n","  target_text, source_text = pickle.load(f)\n","print(f'There are {len(source_text)} fra-eng sentence pairs.')\n","\n","# Show a random sample\n","idx = random.randrange(len(source_text))\n","print('Here is a random sample:')\n","print('source sentence (fra):', source_text[idx])\n","print('target sentence (eng):', target_text[idx])\n","\n","# Split sentences into words\n","source_text = [sentence.split(' ') for sentence in source_text]\n","target_text = [sentence.split(' ') for sentence in target_text]\n","MAX_LEN = max(max([len(sentence) for sentence in source_text]),\n","              max([len(sentence) for sentence in target_text]))\n","print(f'The maximum sentence length is {MAX_LEN}.')"],"id":"flexible-isaac","execution_count":null,"outputs":[{"output_type":"stream","text":["There are 10000 fra-eng sentence pairs.\n","Here is a random sample:\n","source sentence (fra): tout le monde dans la piece poussa un soupir de soulagement .\n","target sentence (eng): everybody in the room let out a sigh of relief .\n","The maximum sentence length is 16.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"placed-semiconductor"},"source":["## Vocabulary\n","\n","NMT models usually learn an embedding for each token. For this, PyTorch provides [`nn.Embedding`](https://pytorch.org/docs/1.7.0/generated/torch.nn.Embedding.html#torch.nn.Embedding) that stores these embeddings as a lookup table. When given the index of a token, this module can retrieve the token's embedding.\n","\n","To make use of `nn.Embedding`, we need to map each token to an index. To convert model outputs back to readable tokens, we also need a reverse mapping that can retrieve the token given its index. The `Vocab` class below maintains exactly these two mappings. Note that to limit the vocabulary size and improve generalization, we do not store rare words in the vocabulary. They are replaced with the `'unk'` token."],"id":"placed-semiconductor"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"a478d297a477cdf7d79ec663f8d219ae","grade":false,"grade_id":"cell-f45af2b474dd22b9","locked":true,"schema_version":3,"solution":false,"task":false},"id":"endless-designation"},"source":["class Vocab:\n","  \n","  def __init__(self, text, min_freq=5):\n","    \n","    # Initialize the vocabulary with special tokens\n","    self.index2word = {\n","      0: 'pad',  # padding\n","      1: 'bos',  # begin of sentence\n","      2: 'eos',  # end of sentence\n","      3: 'unk',  # unknown\n","    }\n","    self.word2index = {v: k for k, v in self.index2word.items()}\n","    self.size = 4  # initial vocabulary size\n","    \n","    # Only add tokens that appear more than `min_freq` times in the training data\n","    tokens = [token for sentence in text for token in sentence]\n","    token_freq = Counter(tokens)\n","    tokens = [token for token in token_freq if token_freq[token] >= min_freq]\n","    self._build_vocab(tokens)\n","  \n","  \n","  def _build_vocab(self, tokens):\n","    \n","    for token in tokens:\n","      if token not in self.word2index:\n","        self.word2index[token] = self.size\n","        self.index2word[self.size] = token\n","        self.size += 1\n","  \n","  \n","  def __getitem__(self, tokens):\n","    \n","    if not isinstance(tokens, (list, tuple)):\n","      return self.word2index.get(tokens, self.word2index['unk'])\n","    else:\n","      return [self.__getitem__(token) for token in tokens]"],"id":"endless-designation","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"d65f4c05a6db0760822d7023c39dbd3c","grade":false,"grade_id":"cell-77998c10f676c8e2","locked":true,"schema_version":3,"solution":false,"task":false},"id":"declared-southwest"},"source":["def get_data_loader(source_text, target_text, batch_size):\n","  \"\"\"\n","  Build vocabulary and prepare data.\n","  \"\"\"\n","  # Use the first 8000 pairs as training data, and the rest for validation\n","  source_vocab = Vocab(source_text[:8000])\n","  target_vocab = Vocab(target_text[:8000])\n","  \n","  # Convert each word to its index in the vocabulary\n","  source_text = source_vocab[source_text]\n","  # Record the length of each sentence\n","  source_len = [len(sentence) for sentence in source_text]\n","  # Pad each sentence to `MAX_LEN`\n","  source_text = [sentence + [source_vocab['pad']] * (MAX_LEN - len(sentence))\n","                 for sentence in source_text]\n","  # Convert to PyTorch tensors\n","  source_text = torch.LongTensor(source_text)  # shape: (num_sentences, MAX_LEN)\n","  source_len = torch.LongTensor(source_len)  # shape: (num_sentences,)\n","  \n","  # Convert each word to its index in the vocabulary\n","  target_text = target_vocab[target_text]\n","  # Add 'bos' and 'eos' tokens\n","  target_text = [[target_vocab['bos']] + sentence + [target_vocab['eos']]\n","                 for sentence in target_text]\n","  # Record the length of each sentence\n","  target_len = [len(sentence) for sentence in target_text]\n","  # Pad each sentence to `MAX_LEN` (+2 for 'bos' and 'eos')\n","  target_text = [sentence + [target_vocab['pad']] * (MAX_LEN + 2 - len(sentence))\n","                 for sentence in target_text]\n","  # Convert to PyTorch tensors\n","  target_text = torch.LongTensor(target_text)  # shape: (num_sentences, MAX_LEN+2)\n","  target_len = torch.LongTensor(target_len)  # shape: (num_sentences,)\n","  \n","  train_set = TensorDataset(\n","    source_text[:8000], source_len[:8000],\n","    target_text[:8000], target_len[:8000])\n","  \n","  val_set = TensorDataset(\n","    source_text[8000:], source_len[8000:],\n","    target_text[8000:], target_len[8000:])\n","  \n","  loader_kwargs = {\n","    'batch_size': batch_size,\n","    'shuffle': True,\n","    'num_workers': 4,\n","    'pin_memory': True,\n","    'drop_last': True,\n","  }\n","  \n","  train_loader = DataLoader(train_set, **loader_kwargs)\n","  val_loader = DataLoader(val_set, **loader_kwargs)\n","  \n","  return source_vocab, target_vocab, train_loader, val_loader"],"id":"declared-southwest","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"international-indian","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617243252408,"user_tz":240,"elapsed":513,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"79e6294b-dac4-4e96-f057-6262c4658990"},"source":["source_vocab, target_vocab, train_loader, val_loader = get_data_loader(\n","  source_text, target_text, batch_size=40)\n","print('source_vocab size:', source_vocab.size)\n","print('target_vocab size:', target_vocab.size)"],"id":"international-indian","execution_count":null,"outputs":[{"output_type":"stream","text":["source_vocab size: 1462\n","target_vocab size: 1237\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"rental-burden"},"source":["## Sequence to Sequence Models for Neural Machine Translation\n","\n","A popular approach to neural machine translation is to use sequence to sequence (seq2seq) models, as shown below.\n","\n","<img src=\"https://raw.githubusercontent.com/dsgiitr/d2l-pytorch/master/img/seq2seq_predict.svg\" width=600>\n","\n","We first use the encoder to obtain an encoding of the source sentence. This encoding is then passed to the decoder which generates the translation one word at a time. Note that to start translation, we feed in the `'bos'` token. The generated word is then used as input for the next step. The translation is considered finished when the decoder outputs the `'eos'` token (i.e., all subsequent outputs are ignored).\n","\n","During training, we use teacher forcing, which means we feed in the ground truth words to the decoder instead of the generated words, as shown below.\n","\n","<img src=\"https://raw.githubusercontent.com/dsgiitr/d2l-pytorch/master/img/seq2seq.svg\" width=600>"],"id":"rental-burden"},{"cell_type":"markdown","metadata":{"id":"quality-reply"},"source":["## Sequence to Sequence with Vanilla RNNs\n","\n","Let us first implement the simplest seq2seq model, which uses vanilla RNNs for both the encoder and the decoder.\n","\n","- **Encoder**\n","\n","    The encoder RNN has its hidden state initialized to zero. At each step, the encoder takes in a word in the source sentence, and updates the hidden state. The hidden state at the last step is thus an encoding of all the words in the source sentence.\n","\n","- **Decoder**\n","\n","    The encoder hidden state at the last step is used to directly initialize the decoder hidden state, which is then updated using the decoder RNN. The encoder and decoder RNNs do not share parameters. A fully connected layer is used to predict the next word from the decoder hidden state at each step. Its output is an unnormalized probability distribution over the target vocabulary.\n","\n","<img src=\"https://raw.githubusercontent.com/dsgiitr/d2l-pytorch/master/img/seq2seq-details.svg\" width=450>"],"id":"quality-reply"},{"cell_type":"markdown","metadata":{"id":"incorporate-brunei"},"source":["Implement `EncoderRNN` and `DecoderRNN` below:"],"id":"incorporate-brunei"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"37598304964b9790f0ab07abbcf0ada5","grade":false,"grade_id":"cell-1bd4dbb9c8d51f05","locked":false,"schema_version":3,"solution":true,"task":false},"id":"sticky-telephone"},"source":["class EncoderRNN(nn.Module):\n","  \n","  def __init__(self, vocab_size, embedding_dim, hidden_size):\n","    super().__init__()\n","    \n","    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # word embeddings\n","    self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n","  \n","  \n","  def forward(self, source_text, source_len):\n","    # type: (Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    Inputs:\n","    - source_text: A LongTensor of shape (batch_size, source_seq_len)\n","      containing a mini-batch of source sentences padded to `source_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - source_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each source sentence (before being padded).\n","    \n","    Returns:\n","    - source_hT: A PyTorch tensor of shape (batch_size, hidden_size)\n","      containing the RNN hidden state at the end of each source sentence\n","      (i.e., `source_hT` encodes all tokens before the padding tokens).\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass.                                           #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    embedded = self.embedding(source_text)\n","    outputs, source_hT = self.rnn(embedded)\n","    source_hT=source_hT.squeeze(0)\n","    # END OF YOUR CODE\n","    return source_hT"],"id":"sticky-telephone","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"54239fde96363451a25b9bdafac434cf","grade":false,"grade_id":"cell-697a33eabe4ebc94","locked":false,"schema_version":3,"solution":true,"task":false},"id":"round-ancient"},"source":["class DecoderRNN(nn.Module):\n","  \n","  def __init__(self, vocab_size, embedding_dim, hidden_size):\n","    super().__init__()\n","    \n","    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # word embeddings\n","    self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n","    self.out = nn.Linear(hidden_size, vocab_size)\n","    self.softmax = nn.Softmax(dim=1)\n","  \n","  def forward(self, target_text, source_hT):\n","    # type: (Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    For each word in each target sentence, predict its next word.\n","    \n","    Inputs:\n","    - target_text: A LongTensor of shape (batch_size, target_seq_len)\n","      containing a mini-batch of target sentences padded to `target_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - source_hT: A PyTorch tensor of shape (batch_size, hidden_size)\n","      containing the RNN hidden state at the end of each source sentence\n","      (i.e., `source_hT` encodes all tokens before the padding tokens).\n","    \n","    Returns:\n","    - pred: A PyTorch tensor of shape (batch_size, target_seq_len, vocab_size)\n","      containing the next word predictions in the form of unnormalized scores\n","      for each possible word in the vocabulary.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass.                                           #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    \n","    embedded = self.embedding(target_text)\n","    source_hT= source_hT.unsqueeze(0)\n","    outputs, source_hT = self.rnn(embedded, source_hT)\n","    outputs=outputs.squeeze(1)\n","    pred=self.out(outputs)\n","    # END OF YOUR CODE\n","    return pred\n","  \n","  \n","  def predict(self, input_token, h_prev):\n","    # type: (Tensor, Tensor) -> Tuple[Tensor, Tensor]\n","    \"\"\"\n","    Single-step prediction: For each input word, predict its next word.\n","    \n","    Inputs:\n","    - input_token: A LongTensor of shape (batch_size, 1)\n","      containing a mini-batch of input tokens.\n","      Each token is represented by its index in the vocabulary.\n","    - h_prev: A PyTorch tensor of shape (1, batch_size, hidden_size)\n","      containing the RNN encoding of all previous input tokens.\n","    \n","    Returns:\n","    - pred_token: A LongTensor of shape (batch_size, 1)\n","      containing the next word predictions.\n","      Each token is represented by its index in the vocabulary.\n","    - h: A PyTorch tensor of shape (1, batch_size, hidden_size)\n","      containing the updated RNN encoding that consumes the current input tokens.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the prediction phase.                                       #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    embedded=self.embedding(input_token)\n","    out, source_hT=self.rnn(embedded, h_prev)\n","    out=out.squeeze(1)\n","    out=self.out(out)\n","    out= self.softmax(out)\n","    _, ind =torch.max(out, dim=1)\n","    pred_token = ind.unsqueeze(1)\n","    h=torch.squeeze(source_hT,1)\n","    # END OF YOUR CODE\n","    return pred_token, h"],"id":"round-ancient","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"deluxe-stretch"},"source":["We now combine `EncoderRNN` and `DecoderRNN` to make a seq2seq model:"],"id":"deluxe-stretch"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"822ec654076b2bab7180766ddb58dc6e","grade":false,"grade_id":"cell-03a724da8183ab80","locked":true,"schema_version":3,"solution":false,"task":false},"id":"becoming-elimination"},"source":["class NMT_RNN(nn.Module):\n","  \n","  def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_size, max_pred_len):\n","    super().__init__()\n","    \n","    self.enc = EncoderRNN(source_vocab_size, embedding_dim, hidden_size)\n","    self.dec = DecoderRNN(target_vocab_size, embedding_dim, hidden_size)\n","    self.max_pred_len = max_pred_len\n","  \n","  \n","  def forward(self, source_text, source_len, target_text, target_len):\n","    # type: (Tensor, Tensor, Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    Perform next word prediction and compute loss.\n","    \n","    Inputs:\n","    - source_text: A LongTensor of shape (batch_size, source_seq_len)\n","      containing a mini-batch of source sentences padded to `source_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - source_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each source sentence (before being padded).\n","    - target_text: A LongTensor of shape (batch_size, target_seq_len)\n","      containing a mini-batch of target sentences padded to `target_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - target_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each target sentence (before being padded).\n","    \n","    Returns:\n","    - loss: A PyTorch scalar containing the loss for the mini-batch.\n","    \"\"\"\n","    source_hT = self.enc(source_text, source_len)\n","    pred = self.dec(target_text, source_hT)\n","    \n","    # Note that `target_text` contains many 'pad' tokens.\n","    # They should be excluded when we compute the loss.\n","    # To remove these 'pad' tokens, we can use `pack_padded_sequence()`.\n","    # Before doing that, we need to first sort target sentences in descending length.\n","    target_len, sorted_indices = torch.sort(target_len, descending=True)\n","    target_text = target_text.index_select(0, sorted_indices)\n","    pred = pred.index_select(0, sorted_indices)\n","    target_len = target_len.cpu()\n","    \n","    # Use `pack_padded_sequence()` to remove 'pad' tokens\n","    target_text_packed = pack_padded_sequence(\n","      target_text[:, 1:], target_len - 1,  # notice the shift in `target_text`\n","      batch_first=True, enforce_sorted=True)\n","    \n","    pred_packed = pack_padded_sequence(\n","      pred, target_len - 1,\n","      batch_first=True, enforce_sorted=True)\n","    \n","    loss = F.cross_entropy(pred_packed.data, target_text_packed.data, ignore_index=0)\n","    return loss\n","  \n","  \n","  def predict(self, source_text, source_len):\n","    # type: (Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    Predict the target sentence for each source sentence.\n","    The predicted word at step t will be used as input for step t+1.\n","    This is different from what is done in the forward pass.\n","    \n","    Inputs:\n","    - source_text: A LongTensor of shape (batch_size, source_seq_len)\n","      containing a mini-batch of source sentences padded to `source_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - source_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each source sentence (before being padded).\n","    \n","    Returns:\n","    - pred: A LongTensor of shape (batch_size, max_pred_len)\n","      containing the predicted target sentences.\n","      Each token is represented by its index in the vocabulary.\n","    \"\"\"\n","    h = self.enc(source_text, source_len).unsqueeze(0)\n","    input_token = torch.ones_like(source_len).view(-1, 1)  # feed in 'bos' to start\n","    pred_list = []\n","    \n","    for t in range(self.max_pred_len):  # predict up to `max_pred_len` steps\n","      pred_token, h = self.dec.predict(input_token, h)\n","      pred_list.append(pred_token)\n","      input_token = pred_token  # use the predicted token as input for next step\n","    \n","    pred = torch.cat(pred_list, dim=1)\n","    return pred"],"id":"becoming-elimination","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"endangered-devices"},"source":["### Evaluation\n","\n","To evaluate a trained NMT model, we use it to perform translation on some source sentences. The translation is generated by picking the most probable word predicted by the decoder at each step. We then compare the model translation with the ground truth translation, and compute the accuracy of the predicted words.\n","\n","Please know that we choose the evaluation protocol here for simplicity. There are better decoding techniques (e.g., beam search) and evaluation metrics (e.g., BLEU scores) that you should use in your own projects."],"id":"endangered-devices"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"4bbed5e20c4316bd35f5b06c7d01bdca","grade":false,"grade_id":"cell-bd9f5b57afbdf5d8","locked":true,"schema_version":3,"solution":false,"task":false},"id":"transparent-malaysia"},"source":["def eval_acc(model, data_loader, num_samples=5):\n","  \n","  with torch.no_grad():\n","    model.eval()\n","    \n","    total = 0\n","    correct = 0\n","    \n","    for batch, data in enumerate(data_loader):\n","      data = [d.cuda() for d in data]\n","      \n","      pred = model.predict(data[0], data[1])\n","      \n","      if batch < num_samples:  # show some samples\n","        source_sentence = [source_vocab.index2word[token] for token in data[0][0].tolist()]\n","        target_sentence = [target_vocab.index2word[token] for token in data[2][0].tolist()]\n","        pred_sentence = [target_vocab.index2word[token] for token in pred[0].tolist()]\n","        \n","        print(f'=== Sample {batch + 1} ===')\n","        print('source_sentence:', source_sentence)\n","        print('target_sentence:', target_sentence)\n","        print('pred_sentence:  ', pred_sentence)\n","      \n","      target_len, sorted_indices = torch.sort(data[3], descending=True)\n","      target_text = data[2].index_select(0, sorted_indices)\n","      pred = pred.index_select(0, sorted_indices)\n","      target_len = target_len.cpu()\n","      \n","      target_text_packed = pack_padded_sequence(\n","        target_text[:, 1:], target_len - 1,  # notice the shift in `target_text`\n","        batch_first=True, enforce_sorted=True)\n","      \n","      pred_packed = pack_padded_sequence(\n","        pred, target_len - 1,\n","        batch_first=True, enforce_sorted=True)\n","      \n","      total += target_text_packed.data.shape[0]\n","      correct += (pred_packed.data == target_text_packed.data).sum().item()\n","    \n","    acc = 100 * correct / total\n","  \n","  return acc"],"id":"transparent-malaysia","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"considered-alexander"},"source":["### Training\n","\n","A basic training script is provided below. You can make modifications as appropriate.\n","\n","To get full credit, your model should **achieve at least 20% validation accuracy**."],"id":"considered-alexander"},{"cell_type":"code","metadata":{"id":"dental-massage"},"source":["def train(model, train_loader, val_loader, num_epochs, learning_rate, weight_decay):\n","  \n","  optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","  \n","  loss_history = []\n","  for epoch in range(num_epochs):\n","    val_acc = eval_acc(model, val_loader, num_samples=0)\n","    \n","    model.train()\n","    \n","    for batch, data in enumerate(train_loader):\n","      data = [d.cuda() for d in data]\n","      \n","      optimizer.zero_grad()\n","      loss = model(*data)\n","      loss.backward()\n","      clip_grad_norm_(model.parameters(), 1)  # gradient clipping\n","      optimizer.step()\n","      \n","      with torch.no_grad():\n","        loss_history.append(loss.item())\n","        if batch == 0:\n","          print('Train Epoch: {:3} \\t Loss: {:F} \\t Val Acc: {:F}'.format(\n","            epoch, loss.item(), val_acc))\n","  \n","  return model, loss_history"],"id":"dental-massage","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sporting-hobby","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617243397120,"user_tz":240,"elapsed":121613,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"cdfd406c-21be-41d6-f2bf-bafa4edf3017"},"source":["torch.manual_seed(0)\n","\n","batch_size = 40\n","num_epochs = 50\n","learning_rate = 0.0005  # TODO\n","weight_decay = 0.0005  # TODO\n","\n","embedding_dim = 256\n","hidden_size = 256\n","\n","source_vocab, target_vocab, train_loader, val_loader = get_data_loader(\n","  source_text, target_text, batch_size)\n","model = NMT_RNN(source_vocab.size, target_vocab.size, embedding_dim, hidden_size, MAX_LEN + 1)\n","model = model.cuda()\n","\n","model, loss_history = train(model, train_loader, val_loader, num_epochs, learning_rate, weight_decay)"],"id":"sporting-hobby","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["Train Epoch:   0 \t Loss: 7.161056 \t Val Acc: 0.117203\n","Train Epoch:   1 \t Loss: 3.873477 \t Val Acc: 11.386031\n","Train Epoch:   2 \t Loss: 3.584569 \t Val Acc: 12.614490\n","Train Epoch:   3 \t Loss: 3.367575 \t Val Acc: 15.188610\n","Train Epoch:   4 \t Loss: 3.340525 \t Val Acc: 13.908061\n","Train Epoch:   5 \t Loss: 3.026997 \t Val Acc: 17.285237\n","Train Epoch:   6 \t Loss: 2.982296 \t Val Acc: 18.049225\n","Train Epoch:   7 \t Loss: 2.950874 \t Val Acc: 17.615141\n","Train Epoch:   8 \t Loss: 2.873890 \t Val Acc: 19.690064\n","Train Epoch:   9 \t Loss: 2.854097 \t Val Acc: 18.162087\n","Train Epoch:  10 \t Loss: 2.710468 \t Val Acc: 19.490385\n","Train Epoch:  11 \t Loss: 2.749976 \t Val Acc: 18.613535\n","Train Epoch:  12 \t Loss: 2.886588 \t Val Acc: 19.312410\n","Train Epoch:  13 \t Loss: 2.709291 \t Val Acc: 19.815948\n","Train Epoch:  14 \t Loss: 2.689458 \t Val Acc: 19.377523\n","Train Epoch:  15 \t Loss: 2.721359 \t Val Acc: 20.597300\n","Train Epoch:  16 \t Loss: 2.837832 \t Val Acc: 20.562573\n","Train Epoch:  17 \t Loss: 2.523937 \t Val Acc: 20.098103\n","Train Epoch:  18 \t Loss: 2.765306 \t Val Acc: 20.414985\n","Train Epoch:  19 \t Loss: 2.842050 \t Val Acc: 21.869167\n","Train Epoch:  20 \t Loss: 2.735999 \t Val Acc: 21.521900\n","Train Epoch:  21 \t Loss: 2.825975 \t Val Acc: 21.634761\n","Train Epoch:  22 \t Loss: 2.682838 \t Val Acc: 20.588618\n","Train Epoch:  23 \t Loss: 2.699234 \t Val Acc: 22.012415\n","Train Epoch:  24 \t Loss: 2.754448 \t Val Acc: 21.856144\n","Train Epoch:  25 \t Loss: 2.786721 \t Val Acc: 22.181708\n","Train Epoch:  26 \t Loss: 2.673652 \t Val Acc: 20.940227\n","Train Epoch:  27 \t Loss: 2.500824 \t Val Acc: 21.908235\n","Train Epoch:  28 \t Loss: 2.735337 \t Val Acc: 22.021096\n","Train Epoch:  29 \t Loss: 2.624461 \t Val Acc: 21.278812\n","Train Epoch:  30 \t Loss: 2.660915 \t Val Acc: 21.087815\n","Train Epoch:  31 \t Loss: 2.921869 \t Val Acc: 22.129618\n","Train Epoch:  32 \t Loss: 2.742924 \t Val Acc: 22.086209\n","Train Epoch:  33 \t Loss: 2.318850 \t Val Acc: 22.207753\n","Train Epoch:  34 \t Loss: 2.581725 \t Val Acc: 22.845857\n","Train Epoch:  35 \t Loss: 2.750933 \t Val Acc: 22.746017\n","Train Epoch:  36 \t Loss: 2.692354 \t Val Acc: 23.101966\n","Train Epoch:  37 \t Loss: 2.639937 \t Val Acc: 23.800842\n","Train Epoch:  38 \t Loss: 2.591429 \t Val Acc: 22.168685\n","Train Epoch:  39 \t Loss: 2.739676 \t Val Acc: 22.715631\n","Train Epoch:  40 \t Loss: 2.528916 \t Val Acc: 22.568043\n","Train Epoch:  41 \t Loss: 2.594246 \t Val Acc: 22.524634\n","Train Epoch:  42 \t Loss: 2.774243 \t Val Acc: 22.737336\n","Train Epoch:  43 \t Loss: 2.729666 \t Val Acc: 22.411772\n","Train Epoch:  44 \t Loss: 2.730003 \t Val Acc: 22.294570\n","Train Epoch:  45 \t Loss: 2.681620 \t Val Acc: 23.236533\n","Train Epoch:  46 \t Loss: 2.666697 \t Val Acc: 22.906628\n","Train Epoch:  47 \t Loss: 2.701191 \t Val Acc: 23.631549\n","Train Epoch:  48 \t Loss: 2.602196 \t Val Acc: 23.258237\n","Train Epoch:  49 \t Loss: 2.464222 \t Val Acc: 23.219169\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"settled-victorian","colab":{"base_uri":"https://localhost:8080/","height":497},"executionInfo":{"status":"ok","timestamp":1617243398158,"user_tz":240,"elapsed":931,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"2e95f796-4452-48dc-c4f1-8651f9268741"},"source":["plt.plot(loss_history, 'o')\n","plt.xlabel('Iteration number')\n","plt.ylabel('Loss value')\n","plt.show()"],"id":"settled-victorian","execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 720x576 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"sized-month","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617243400003,"user_tz":240,"elapsed":2706,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"ee81cddb-0869-482b-91cb-2e75e5e637b6"},"source":["train_acc = eval_acc(model, train_loader)\n","print('Training Accuracy:', train_acc)"],"id":"sized-month","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["=== Sample 1 ===\n","source_sentence: ['j', 'espere', 'que', 'nous', 'pouvons', 'poser', 'cette', 'question', 'a', 'tom', '.', 'pad', 'pad', 'pad', 'pad', 'pad']\n","target_sentence: ['bos', 'i', 'wish', 'we', 'could', 'ask', 'tom', 'that', 'question', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","pred_sentence:   ['i', 'd', 'like', 'to', 'talk', 'to', 'the', 'unk', 'of', 'the', 'unk', 'unk', '.', 'eos', '.', 'eos', '.']\n","=== Sample 2 ===\n","source_sentence: ['je', 'n', 'ai', 'jamais', 'pense', 'que', 'nous', 'unk', 'comme', 'ca', '.', 'pad', 'pad', 'pad', 'pad', 'pad']\n","target_sentence: ['bos', 'i', 'never', 'thought', 'we', 'd', 'end', 'up', 'like', 'this', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","pred_sentence:   ['i', 'don', 't', 'have', 'time', 'to', 'unk', 'unk', '.', 'eos', 'eos', 'unk', '.', 'eos', 'eos', '?', 'eos']\n","=== Sample 3 ===\n","source_sentence: ['le', 'voleur', 'unk', 'd', 'unk', 'unk', 'pour', 'unk', 'les', 'unk', '.', 'pad', 'pad', 'pad', 'pad', 'pad']\n","target_sentence: ['bos', 'the', 'thief', 'had', 'special', 'unk', 'for', 'unk', 'unk', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","pred_sentence:   ['the', 'unk', 'unk', 'unk', 'unk', 'to', 'unk', 'unk', 'unk', '.', 'eos', '.', 'eos', '.', 'eos', 'it', '.']\n","=== Sample 4 ===\n","source_sentence: ['je', 'le', 'unk', 'secret', '.', 'ne', 'vous', 'en', 'faites', 'pas', '.', 'pad', 'pad', 'pad', 'pad', 'pad']\n","target_sentence: ['bos', 'i', 'll', 'keep', 'it', 'a', 'secret', '.', 'don', 't', 'worry', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n","pred_sentence:   ['i', 'm', 'unk', 'to', 'unk', 'unk', 'of', 'unk', '.', 'eos', '.', 'eos', '.', 'eos', '.', 'eos', 'do']\n","=== Sample 5 ===\n","source_sentence: ['il', 'lui', 'a', 'conseille', 'd', 'aller', 'voir', 'la', 'police', '.', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","target_sentence: ['bos', 'she', 'was', 'advised', 'by', 'him', 'to', 'go', 'to', 'the', 'police', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n","pred_sentence:   ['he', 'was', 'unk', 'to', 'the', 'unk', 'of', 'the', 'unk', '.', 'eos', 'eos', '.', 'eos', '.', 'eos', '.']\n","Training Accuracy: 23.97940976089801\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"c6616d9c122893f804ba97b18dc2f5c4","grade":true,"grade_id":"cell-ca29e82b362c4f0a","locked":true,"points":25,"schema_version":3,"solution":false,"task":false},"id":"acting-malta","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617243400683,"user_tz":240,"elapsed":3274,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"bf03b9db-8e49-412b-9ecf-4e7705325c2c"},"source":["val_acc = eval_acc(model, val_loader)\n","print('Validation Accuracy:', val_acc)\n","assert val_acc >= 20"],"id":"acting-malta","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"stream","text":["=== Sample 1 ===\n","source_sentence: ['trouver', 'une', 'unk', 'qui', 'unk', 'fut', 'un', 'unk', 'unk', '.', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","target_sentence: ['bos', 'unk', 'a', 'unk', 'that', 'worked', 'was', 'a', 'unk', 'of', 'unk', 'and', 'unk', '.', 'eos', 'pad', 'pad', 'pad']\n","pred_sentence:   ['a', 'unk', 'of', 'unk', 'is', 'unk', 'to', 'unk', '.', 'eos', '.', 'eos', '.', 'eos', 'it', '.', 'eos']\n","=== Sample 2 ===\n","source_sentence: ['j', 'ignorais', 'que', 'tu', 'avais', 'un', 'petit', 'ami', '.', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","target_sentence: ['bos', 'i', 'didn', 't', 'know', 'you', 'had', 'a', 'boyfriend', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","pred_sentence:   ['i', 'don', 't', 'know', 'what', 'you', 're', 'going', 'to', 'do', '.', 'eos', '.', 'eos', '.', 'eos', '.']\n","=== Sample 3 ===\n","source_sentence: ['on', 'dit', 'qu', 'il', 'n', 'est', 'rien', 'de', 'plus', 'unk', 'que', 'le', 'temps', '.', 'pad', 'pad']\n","target_sentence: ['bos', 'it', 's', 'said', 'that', 'nothing', 'is', 'more', 'unk', 'than', 'time', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad']\n","pred_sentence:   ['they', 'are', 'not', 'to', 'unk', 'the', 'unk', 'of', 'the', 'unk', '.', 'eos', 'eos', '.', 'eos', '.', 'eos']\n","=== Sample 4 ===\n","source_sentence: ['ce', 'n', 'etait', 'pas', 'moi', '.', 'c', 'etait', 'le', 'chat', '.', 'pad', 'pad', 'pad', 'pad', 'pad']\n","target_sentence: ['bos', 'it', 'wasn', 't', 'me', '.', 'it', 'was', 'the', 'cat', '.', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","pred_sentence:   ['this', 'is', 'not', 'a', 'unk', 'of', 'unk', 'unk', '.', 'eos', 'eos', '.', 'eos', '.', 'eos', 'do', 'it']\n","=== Sample 5 ===\n","source_sentence: ['c', 'est', 'la', 'pire', 'chose', 'que', 'tu', 'puisses', 'faire', '!', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","target_sentence: ['bos', 'that', 's', 'the', 'worst', 'thing', 'you', 'can', 'do', '!', 'eos', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad', 'pad']\n","pred_sentence:   ['that', 's', 'the', 'unk', 'i', 'want', 'to', 'do', '?', 'eos', 'eos', '?', 'eos', 'eos', '?', 'eos', '?']\n","Validation Accuracy: 23.653253461822285\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cleared-heavy"},"source":["## LSTMs\n","\n","Vanilla RNNs are known to have vanishing gradient issues, which have been mitigated by LSTMs. Hence, replacing the RNNs in seq2seq models with LSTMs will probably improve performance. In this section, we will implement LSTMs from scratch."],"id":"cleared-heavy"},{"cell_type":"markdown","metadata":{"id":"technological-knight"},"source":["Let us first implement the single-step LSTM update. We will be replicating the functionality of [`nn.LSTMCell`](https://pytorch.org/docs/1.7.0/generated/torch.nn.LSTMCell.html#torch.nn.LSTMCell). Please refer to the documentation for more details. Note that for efficiency, the weights to be multiplied by the input are combined into a single weight matrix `weight_ih`, the weights to be multiplied by the previous hidden state are combined into `weight_hh`, and the biases are combined into a single vector `bias`. You are NOT allowed to use `nn.LSTMCell` or `nn.LSTM` in your implementation. For weight initialization, you can try [`nn.init.xavier_uniform_()`](https://pytorch.org/docs/1.7.0/nn.init.html#torch.nn.init.xavier_uniform_) and [`nn.init.orthogonal_()`](https://pytorch.org/docs/1.7.0/nn.init.html#torch.nn.init.orthogonal_). As suggested in this [post](https://danijar.com/tips-for-training-recurrent-neural-networks/), you may also initialize the forget gate bias to 1 so that the LSTM remembers more information by default."],"id":"technological-knight"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"94f1f576703d8e63c9e63d421190cc1b","grade":false,"grade_id":"cell-1cd63486f030a804","locked":false,"schema_version":3,"solution":true,"task":false},"id":"blind-racing"},"source":["class LSTMCell(nn.Module):\n","  \n","  def __init__(self, input_size, hidden_size):\n","    super().__init__()\n","    \n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    \n","    # Learnable parameters\n","    self.weight_ih = nn.Parameter(torch.Tensor(4 * self.hidden_size, self.input_size))\n","    self.weight_hh = nn.Parameter(torch.Tensor(4 * self.hidden_size, self.hidden_size))\n","    self.bias = nn.Parameter(torch.Tensor(4 * self.hidden_size))\n","    \n","    self.init_params()\n","  \n","  \n","  def init_params(self):\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Properly initialize the learnable parameters.                         #\n","    # This function should not have a \"return\" statement.                   #\n","    # You may use built-in functions in `torch.nn.init`.                    #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","\n","    self.istart=0\n","    self.iend=self.istart+self.hidden_size\n","    self.gstart= self.hidden_size\n","    self.gend=self.gstart+self.hidden_size\n","    self.fstart= self.gend\n","    self.fend=self.fstart+self.hidden_size\n","    self.ostart= self.fend\n","    self.oend=self.ostart+self.hidden_size\n","\n","    nn.init.xavier_uniform(self.weight_ih)\n","    nn.init.xavier_uniform(self.weight_hh)\n","    nn.init.zeros_(self.bias.data)\n","    self.bias.data[self.fstart:self.fend]=1.0\n","\n","\n","    # END OF YOUR CODE\n","  \n","  \n","  def forward(self, input, state):\n","    # type: (Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tensor]\n","    \"\"\"\n","    Inputs:\n","    - input: A PyTorch tensor of shape (batch_size, input_size)\n","      containing a mini-batch of input features.\n","    - state: A tuple of hidden state and cell state from the previous step,\n","      both of which are PyTorch tensors of shape (batch_size, hidden_size).\n","      You may assume that `state` is not None.\n","    \n","    Returns:\n","    - hy: A PyTorch tensor of shape (batch_size, hidden_size)\n","      containing the updated hidden state at the current step.\n","    - cy: A PyTorch tensor of shape (batch_size, hidden_size)\n","      containing the updated cell state at the current step.\n","    \n","    For more details, check `torch.nn.LSTMCell`.\n","    \"\"\"\n","    hx, cx = state\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass.                                           #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","  \n","    W_ii= self.weight_ih[self.istart:self.iend, :]\n","    W_hi= self.weight_hh[self.istart:self.iend, :]\n","    b_ii= self.bias[self.istart:self.iend]\n","\n","    W_iix = torch.mm(input, W_ii.T)\n","    W_iih = torch.mm(hx, W_hi.T)\n","    i = torch.sigmoid(W_iix+W_iih+b_ii)\n","\n","    W_if= self.weight_ih[self.fstart:self.fend, :]\n","    W_hf= self.weight_hh[self.fstart:self.fend, :]\n","    b_if= self.bias[self.fstart:self.fend]\n","\n","    W_ifx = torch.mm(input, W_if.T)\n","    W_ifh = torch.mm(hx, W_hf.T)\n","    f = torch.sigmoid(W_ifx+W_ifh+b_if)\n","\n","    W_ig= self.weight_ih[self.gstart:self.gend, :]\n","    W_hg= self.weight_hh[self.gstart:self.gend, :]\n","    b_ig= self.bias[self.gstart:self.gend]\n","\n","    W_igx = torch.mm(input, W_ig.T)\n","    W_igh = torch.mm(hx, W_hg.T)\n","    g = torch.tanh(W_igx+W_igh+b_ig)\n","\n","    W_io= self.weight_ih[self.ostart:self.oend, :]\n","    W_ho= self.weight_hh[self.ostart:self.oend, :]\n","    b_io= self.bias[self.ostart:self.oend]\n","\n","    W_iox = torch.mm(input, W_io.T)\n","    W_ioh = torch.mm(hx, W_ho.T)\n","    o = torch.sigmoid(W_iox+W_ioh+b_io)\n","\n","    cy = f*cx+i*g\n","    hy= o*torch.tanh(cy)\n","\n","    # END OF YOUR CODE\n","    return hy, cy"],"id":"blind-racing","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"proper-monthly"},"source":["We will use the following helper functions to check your results:"],"id":"proper-monthly"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"5734c4764254450e0369ca7fde6fdb69","grade":false,"grade_id":"cell-d0dfcd5976817206","locked":true,"schema_version":3,"solution":false,"task":false},"id":"corporate-buyer"},"source":["def max_diff(actual, expected):\n","  return (actual - expected).abs().max().item()"],"id":"corporate-buyer","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"0292350d64a28c6e08c17e6b55f69a48","grade":false,"grade_id":"cell-7adf64f58371b339","locked":true,"schema_version":3,"solution":false,"task":false},"id":"simple-indianapolis"},"source":["def check_lstm_cell(batch_size, input_size, hidden_size):\n","  \n","  torch.manual_seed(0)\n","  torch_lstm_cell = nn.LSTMCell(input_size, hidden_size)\n","  lstm_cell = LSTMCell(input_size, hidden_size)\n","  \n","  lstm_cell.weight_ih.data = torch_lstm_cell.weight_ih.data\n","  lstm_cell.weight_hh.data = torch_lstm_cell.weight_hh.data\n","  lstm_cell.bias.data = torch_lstm_cell.bias_ih.data + torch_lstm_cell.bias_hh.data\n","  \n","  input = torch.empty(batch_size, input_size).normal_()\n","  state = torch.empty(batch_size, 2 * hidden_size).normal_().chunk(2, 1)\n","  \n","  (hy_expected, cy_expected) = torch_lstm_cell(input, state)\n","  (hy_actual, cy_actual) = lstm_cell(input, state)\n","  \n","  assert hy_actual.shape == hy_expected.shape, 'incorrect hy shape'\n","  assert cy_actual.shape == cy_expected.shape, 'incorrect cy shape'\n","  \n","  hy_diff = max_diff(hy_actual, hy_expected)\n","  cy_diff = max_diff(cy_actual, cy_expected)\n","  \n","  print('hy_diff = %e' % hy_diff)\n","  print('cy_diff = %e' % cy_diff)\n","  assert hy_diff < 1e-5, 'incorrect hy'\n","  assert cy_diff < 1e-5, 'incorrect cy'"],"id":"simple-indianapolis","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"659fdc7307c7b3d80ba56b806843cd9f","grade":true,"grade_id":"cell-3fca48443be87688","locked":true,"points":10,"schema_version":3,"solution":false,"task":false},"id":"metric-louisiana","colab":{"base_uri":"https://localhost:8080/","height":334},"executionInfo":{"status":"error","timestamp":1617245877629,"user_tz":240,"elapsed":409,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"866bd7e3-2a90-4e6b-8e3a-6199be7999ef"},"source":["check_lstm_cell(batch_size=4, input_size=128, hidden_size=64)"],"id":"metric-louisiana","execution_count":null,"outputs":[{"output_type":"stream","text":["hy_diff = 7.707835e-01\n","cy_diff = 2.392879e+00\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:36: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"],"name":"stderr"},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-914ae7a10bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_lstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-32-6f758d1c61a9>\u001b[0m in \u001b[0;36mcheck_lstm_cell\u001b[0;34m(batch_size, input_size, hidden_size)\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'hy_diff = %e'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mhy_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cy_diff = %e'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mcy_diff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mhy_diff\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'incorrect hy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mcy_diff\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'incorrect cy'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: incorrect hy"]}]},{"cell_type":"markdown","metadata":{"id":"taken-minority"},"source":["Now that we have implemented the single-step update, we can apply it iteratively to a sequence of inputs and obtain the hidden states at each step. Please implement the `LSTM` class below, which contains `LSTMCell` as a submodule. You are NOT allowed to use `nn.LSTMCell` or `nn.LSTM` in your implementation.\n","\n","You are encouraged (but not required) to develop an efficient implementation, in which the padding tokens are skipped and not fed to the `LSTMCell`. This can save computation by ~25% on our dataset, assuming the sentence length has a uniform distribution. Check this [tutorial](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning#decoder-1) for an illustration."],"id":"taken-minority"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"4276c5f5d3921cea1229592c8d4dbdc7","grade":false,"grade_id":"cell-10ec063c2936f81c","locked":false,"schema_version":3,"solution":true,"task":false},"id":"unavailable-funeral"},"source":["class LSTM(nn.Module):\n","  \n","  def __init__(self, input_size, hidden_size):\n","    super().__init__()\n","    \n","    self.input_size = input_size\n","    self.hidden_size = hidden_size\n","    \n","    self.lstm_cell = LSTMCell(self.input_size, self.hidden_size)\n","  \n","  \n","  def forward(self, input, input_len, init_state):\n","    # type: (Tensor, Tensor, Tuple[Tensor, Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\n","    \"\"\"\n","    Inputs:\n","    - input: A PyTorch tensor of shape (batch_size, max_seq_len, input_size)\n","      containing a mini-batch of sequences padded to `max_seq_len`.\n","    - input_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each sequence (before being padded).\n","    - init_state: A tuple of initial hidden state and cell state,\n","      both of which are PyTorch tensors of shape (batch_size, hidden_size).\n","      You may assume that `init_state` is not None.\n","    \n","    Returns:\n","    - hidden_states: A PyTorch tensor of shape (batch_size, max_seq_len, hidden_size)\n","      containing the hidden state `ht` at each step t.\n","    - hT: A PyTorch tensor of shape (batch_size, hidden_size)\n","      containing the hidden state at the end of each sequence\n","      (i.e., `hT` encodes all tokens before the padding tokens).\n","    - cT: A PyTorch tensor of shape (batch_size, hidden_size)\n","      containing the cell state at the end of each sequence\n","      (i.e., `cT` encodes all tokens before the padding tokens).\n","    \"\"\"\n","    h, c = init_state\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass.                                           #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    batch_size, max_seq_len, input_shape = input.shape\n","    source = input.transpose(0,1)\n","    curr_state = (h, c)\n","    output = []\n","    cells=[]\n","    for i in source:\n","        hT,cT= self.lstm_cell.forward(i,curr_state)\n","        curr_state = (hT,cT)\n","        output.append(hT)\n","        cells.append(cT)\n","   \n","    cell_states = torch.stack(cells).transpose(0,1)\n","    hidden_states = torch.stack(output).transpose(0,1)  \n","\n","    input_len = torch.sub(input_len, 1)\n","    input_len = input_len.unsqueeze(-1)\n","\n","    ih = input_len.repeat(1, hidden_states.shape[2])\n","    ih = ih.unsqueeze(1)\n","\n","    hT = torch.gather(hidden_states, 1, ih).squeeze(1)\n","    \n","    ic = input_len.repeat(1, cell_states.shape[2])\n","    ic = ic.unsqueeze(1)\n","    cT = torch.gather(cell_states, 1, ic).squeeze(1)\n","    # END OF YOUR CODE\n","    return hidden_states, (hT, cT)"],"id":"unavailable-funeral","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"drawn-support"},"source":["We will use the following helper function to check your results:"],"id":"drawn-support"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7da2900fc4e8c0fc8b3f367c98ec8571","grade":false,"grade_id":"cell-e4707b217d4cc37a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"micro-walter"},"source":["def check_lstm(batch_size, max_seq_len, input_size, hidden_size):\n","  \n","  torch.manual_seed(0)\n","  torch_lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n","  lstm = LSTM(input_size, hidden_size)\n","  \n","  lstm.lstm_cell.weight_ih.data = torch_lstm.weight_ih_l0.data\n","  lstm.lstm_cell.weight_hh.data = torch_lstm.weight_hh_l0.data\n","  lstm.lstm_cell.bias.data = torch_lstm.bias_ih_l0.data + torch_lstm.bias_hh_l0.data\n","  \n","  input = torch.empty(batch_size, max_seq_len, input_size).normal_()\n","  input_len = torch.randint(1, max_seq_len + 1, (batch_size,))\n","  init_state = torch.zeros(batch_size, 2 * hidden_size).chunk(2, 1)\n","  \n","  input_packed = pack_padded_sequence(\n","    input, input_len, batch_first=True, enforce_sorted=False)\n","  \n","  hidden_states_expected_packed, (hT_expected, cT_expected) = torch_lstm(input_packed)\n","  \n","  hidden_states_actual, (hT_actual, cT_actual) = lstm(input, input_len, init_state)\n","  \n","  assert hidden_states_actual.shape == (batch_size, max_seq_len, hidden_size), 'incorrect hidden_states shape'\n","  assert hT_actual.shape == (batch_size, hidden_size), 'incorrect hT shape'\n","  assert cT_actual.shape == (batch_size, hidden_size), 'incorrect cT shape'\n","  \n","  input_len = input_len.index_select(0, input_packed.sorted_indices)\n","  hidden_states_actual = hidden_states_actual.index_select(0, input_packed.sorted_indices)\n","  hidden_states_actual_packed = pack_padded_sequence(\n","    hidden_states_actual, input_len, batch_first=True, enforce_sorted=True)\n","  \n","  hidden_states_diff = max_diff(\n","    hidden_states_actual_packed.data,\n","    hidden_states_expected_packed.data)\n","  \n","  hT_diff = max_diff(hT_actual, hT_expected)\n","  cT_diff = max_diff(cT_actual, cT_expected)\n","  \n","  print('hidden_states_diff = %e' % hidden_states_diff)\n","  print('hT_diff = %e' % hT_diff)\n","  print('cT_diff = %e' % cT_diff)\n","  assert hidden_states_diff < 1e-5, 'incorrect hidden_states'\n","  assert hT_diff < 1e-5, 'incorrect hT'\n","  assert cT_diff < 1e-5, 'incorrect cT'"],"id":"micro-walter","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"7f2bde0c493abaf4b5f3813d98113526","grade":true,"grade_id":"cell-20cbebedf2365e3e","locked":true,"points":10,"schema_version":3,"solution":false,"task":false},"id":"medical-automation","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617245328183,"user_tz":240,"elapsed":496,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"52195fb7-467b-4a6f-9989-a982bd68810c"},"source":["check_lstm(batch_size=32, max_seq_len=8, input_size=128, hidden_size=64)"],"id":"medical-automation","execution_count":null,"outputs":[{"output_type":"stream","text":["hidden_states_diff = 2.682209e-07\n","hT_diff = 1.862645e-07\n","cT_diff = 4.172325e-07\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"revised-reservoir"},"source":["## Sequence to Sequence with LSTMs\n","\n","Now let us implement a seq2seq model with both the encoder and the decoder being LSTMs."],"id":"revised-reservoir"},{"cell_type":"markdown","metadata":{"id":"accepted-client"},"source":["Implement `EncoderLSTM` and `DecoderLSTM` below. You may use PyTorch's built-in LSTMs or what you have implemented above."],"id":"accepted-client"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"141b863966c523c8dd52e08d696ae6ce","grade":false,"grade_id":"cell-d91c3f29aeda2c0e","locked":false,"schema_version":3,"solution":true,"task":false},"id":"pharmaceutical-shift"},"source":["class EncoderLSTM(nn.Module):\n","  \n","  def __init__(self, vocab_size, embedding_dim, hidden_size):\n","    super().__init__()\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Set up the encoder components.                                        #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # word embeddings\n","    self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n","    self.out= nn.Linear(hidden_size, vocab_size)\n","    self.hidden_size= hidden_size\n","    # END OF YOUR CODE\n","  \n","  \n","  def forward(self, source_text, source_len):\n","    # type: (Tensor, Tensor) -> Tuple[Tensor, Tuple[Tensor, Tensor]]\n","    \"\"\"\n","    Inputs:\n","    - source_text: A LongTensor of shape (batch_size, source_seq_len)\n","      containing a mini-batch of source sentences padded to `source_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - source_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each source sentence (before being padded).\n","    \n","    Returns:\n","    - hidden_states: A PyTorch tensor of shape (batch_size, source_seq_len, hidden_size)\n","      containing the LSTM hidden state `ht` at each step t of the source sentences.\n","    - hT: A PyTorch tensor of shape (batch_size, hidden_size)\n","      containing the LSTM hidden state at the end of each source sentence\n","      (i.e., `hT` encodes all tokens before the padding tokens).\n","    - cT: A PyTorch tensor of shape (batch_size, hidden_size)\n","      containing the LSTM cell state at the end of each source sentence\n","      (i.e., `cT` encodes all tokens before the padding tokens).\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass.                                           #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    embedded = self.embedding(source_text)\n","    N = embedded.shape[0]\n","    \n","    hidden_state = source_text.new_zeros(N, self.hidden_size).float()\n","    cell_state = source_text.new_zeros(N, self.hidden_size).float()\n","    hidden_state= hidden_state.unsqueeze(0)\n","    cell_state = cell_state.unsqueeze(0)\n","    \n","    hidden_states, (hT, cT) = self.lstm(embedded, (hidden_state, cell_state))\n","    # END OF YOUR CODE\n","    return hidden_states, (hT, cT)"],"id":"pharmaceutical-shift","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"a05a9889acf740f2a193f17ee8a52f72","grade":false,"grade_id":"cell-b067dc1a04cd1871","locked":false,"schema_version":3,"solution":true,"task":false},"id":"prostate-southeast"},"source":["class DecoderLSTM(nn.Module):\n","  \n","  def __init__(self, vocab_size, embedding_dim, hidden_size):\n","    super().__init__()\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Set up the decoder components.                                        #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0) \n","    self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n","    self.out = nn.Linear(hidden_size, vocab_size)\n","    self.softmax = nn.Softmax(dim=1)\n","    self.hidden_size= hidden_size\n","    # END OF YOUR CODE\n","  \n","  \n","  def forward(self, target_text, target_len, source_last_state):\n","    # type: (Tensor, Tensor, Tuple[Tensor, Tensor]) -> Tensor\n","    \"\"\"\n","    Perform next word prediction and compute loss.\n","    \n","    Inputs:\n","    - target_text: A LongTensor of shape (batch_size, target_seq_len)\n","      containing a mini-batch of target sentences padded to `target_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - target_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each target sentence (before being padded).\n","    - source_last_state: A tuple of hidden state and cell state at the end of\n","      each source sentence, both of shape (batch_size, hidden_size).\n","    \n","    Returns:\n","    - loss: A PyTorch scalar containing the loss for the mini-batch.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass.                                           #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    embedded=self.embedding(target_text)\n","    hT, cT = source_last_state\n","    outputs=[]\n","    for token in embedded:\n","      output, (hT, cT) = self.lstm(token, (hT, cT))\n","      output = self.out(output)\n","      output = self.softmax(output)\n","      outputs.append(output.transpose(0,1))\n","    loss = torch.cat(outputs,dim=0).transpose(0,1)\n","    # END OF YOUR CODE\n","    \n","    return loss\n","  \n","  \n","  def predict(self, source_last_state, max_pred_len):\n","    # type: (Tuple[Tensor, Tensor], int) -> Tensor\n","    \"\"\"\n","    Predict target sentences given the LSTM encoding of source sentences.\n","    The predicted word at step t will be used as input for step t+1.\n","    This is different from what is done in the forward pass.\n","    \n","    Inputs:\n","    - source_last_state: A tuple of hidden state and cell state at the end of\n","      each source sentence, both of shape (batch_size, hidden_size).\n","    - max_pred_len: (int) number of prediction steps.\n","    \n","    Returns:\n","    - pred: A LongTensor of shape (batch_size, max_pred_len)\n","      containing the predicted target sentences.\n","      Each token is represented by its index in the vocabulary.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the prediction phase.                                       #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    # for w in max_pred_len:\n","    #   embedded= self.embedding(w)\n","    input_token = torch.ones(source_last_state[0].shape[0]).reshape(1,-1).long().cuda().view(-1, 1)\n","    # h_prev = h_prev.unsqueeze(0)\n","\n","    pred_list=[]\n","    for t in range(max_pred_len):  # predict up to `max_pred_len` steps\n","      embedded_tokens = self.embedding(input_token)\n","      embedded_tokens = torch.transpose(embedded_tokens, 0, 1)\n","      output, (h,c) = self.lstm(embedded_tokens,source_last_state)\n","      output = output.squeeze(1)\n","      output = self.out(output) \n","      output = self.softmax(output)\n","\n","      # Pick the index of token with max probability\n","      max_value, max_index = torch.max(output, dim=1)\n","      \n","      # Add dim 1 to the vector of max probable token indices\n","      pred_token = max_index.unsqueeze(1)\n","\n","      pred_list.append(pred_token)\n","      input_token = pred_token \n","    \n","    pred = torch.cat(pred_list, dim=1)\n","    \n","    # END OF YOUR CODE\n","    return pred"],"id":"prostate-southeast","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"1da46844a3100730d435f8a8ba385959","grade":false,"grade_id":"cell-11cb8bfb0133780e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"mental-pathology"},"source":["class NMT_LSTM(nn.Module):\n","  \n","  def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_size, max_pred_len):\n","    super().__init__()\n","    \n","    self.enc = EncoderLSTM(source_vocab_size, embedding_dim, hidden_size)\n","    self.dec = DecoderLSTM(target_vocab_size, embedding_dim, hidden_size)\n","    self.max_pred_len = max_pred_len\n","  \n","  \n","  def forward(self, source_text, source_len, target_text, target_len):\n","    # type: (Tensor, Tensor, Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    Perform next word prediction and compute loss.\n","    \n","    Inputs:\n","    - source_text: A LongTensor of shape (batch_size, source_seq_len)\n","      containing a mini-batch of source sentences padded to `source_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - source_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each source sentence (before being padded).\n","    - target_text: A LongTensor of shape (batch_size, target_seq_len)\n","      containing a mini-batch of target sentences padded to `target_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - target_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each target sentence (before being padded).\n","    \n","    Returns:\n","    - loss: A PyTorch scalar containing the loss for the mini-batch.\n","    \"\"\"\n","    _, source_last_state = self.enc(source_text, source_len)\n","    loss = self.dec(target_text, target_len, source_last_state)\n","    return loss\n","  \n","  \n","  def predict(self, source_text, source_len):\n","    # type: (Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    Predict the target sentence for each source sentence.\n","    The predicted word at step t will be used as input for step t+1.\n","    This is different from what is done in the forward pass.\n","    \n","    Inputs:\n","    - source_text: A LongTensor of shape (batch_size, source_seq_len)\n","      containing a mini-batch of source sentences padded to `source_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - source_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each source sentence (before being padded).\n","    \n","    Returns:\n","    - pred: A LongTensor of shape (batch_size, max_pred_len)\n","      containing the predicted target sentences.\n","      Each token is represented by its index in the vocabulary.\n","    \"\"\"\n","    _, source_last_state = self.enc(source_text, source_len)\n","    pred = self.dec.predict(source_last_state, self.max_pred_len)\n","    return pred"],"id":"mental-pathology","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"southeast-announcement"},"source":["### Training\n","\n","A basic training script is provided below. You can make modifications as appropriate.\n","\n","To get full credit, your model should **achieve at least 35% validation accuracy**."],"id":"southeast-announcement"},{"cell_type":"code","metadata":{"id":"legendary-alliance","colab":{"base_uri":"https://localhost:8080/","height":395},"executionInfo":{"status":"error","timestamp":1617243748898,"user_tz":240,"elapsed":895,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"d34b6846-437b-46ed-cd31-081e540c4fb3"},"source":["torch.manual_seed(0)\n","\n","batch_size = 40\n","num_epochs = 50\n","learning_rate = 0.0001  # TODO\n","weight_decay = 0.0001  # TODO\n","\n","embedding_dim = 256\n","hidden_size = 256\n","\n","source_vocab, target_vocab, train_loader, val_loader = get_data_loader(\n","  source_text, target_text, batch_size)\n","model = NMT_LSTM(source_vocab.size, target_vocab.size, embedding_dim, hidden_size, MAX_LEN + 1)\n","model = model.cuda()\n","\n","model, loss_history = train(model, train_loader, val_loader, num_epochs, learning_rate, weight_decay)"],"id":"legendary-alliance","execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-a5ddeeaa2b49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-12-7ced8352b560>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate, weight_decay)\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mloss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-8fd9dffcf8b5>\u001b[0m in \u001b[0;36meval_acc\u001b[0;34m(model, data_loader, num_samples)\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m       \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# show some samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-26-287c925c71db>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source_text, source_len)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \"\"\"\n\u001b[1;32m     55\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_last_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_last_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pred_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-28-bf359e4c5ee7>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, source_last_state, max_pred_len)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0membedded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m       \u001b[0membedded_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource_last_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    657\u001b[0m             \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 659\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    660\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    605\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         self.check_hidden_size(hidden[0], self.get_expected_hidden_size(input, batch_sizes),\n\u001b[0;32m--> 607\u001b[0;31m                                'Expected hidden[0] size {}, got {}')\n\u001b[0m\u001b[1;32m    608\u001b[0m         self.check_hidden_size(hidden[1], self.get_expected_cell_size(input, batch_sizes),\n\u001b[1;32m    609\u001b[0m                                'Expected hidden[1] size {}, got {}')\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mcheck_hidden_size\u001b[0;34m(self, hx, expected_hidden_size, msg)\u001b[0m\n\u001b[1;32m    221\u001b[0m                           msg: str = 'Expected hidden size {}, got {}') -> None:\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpected_hidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected hidden[0] size (1, 1, 256), got [1, 40, 256]"]}]},{"cell_type":"code","metadata":{"id":"comparable-longer"},"source":["plt.plot(loss_history, 'o')\n","plt.xlabel('Iteration number')\n","plt.ylabel('Loss value')\n","plt.show()"],"id":"comparable-longer","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dietary-croatia"},"source":["train_acc = eval_acc(model, train_loader)\n","print('Training Accuracy:', train_acc)"],"id":"dietary-croatia","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9c148b4e5bca46eb01cb639006100c81","grade":true,"grade_id":"cell-88776d0dab55c8f3","locked":true,"points":40,"schema_version":3,"solution":false,"task":false},"id":"flying-checklist"},"source":["val_acc = eval_acc(model, val_loader)\n","print('Validation Accuracy:', val_acc)\n","assert val_acc >= 35"],"id":"flying-checklist","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"final-vessel"},"source":["## Sequence to Sequence with Attention\n","\n","So far we have been compressing the entire source sentence into some fixed-length encoding vector, from which the decoder needs to extract relevant information to predict each target word. However, as suggested in this [paper](https://arxiv.org/pdf/1409.0473.pdf), such a fixed-length encoding can become a bottleneck when the sentence gets longer. A better approach would be to let the decoder have direct access to the encoder hidden states at all steps. This allows the decoder to focus on relevant words in the source sentence more easily when predicting a target word.\n","\n","<img src=\"https://guillaumegenthial.github.io/assets/img2latex/seq2seq_attention_mechanism_new.svg\" width=800>\n","\n","As depicted above, at each step $t$, the decoder uses its previous hidden state $h_{t-1}^\\text{dec}$ to compute an attention score $\\alpha_{t, i}$ for each word $i$ in the source sentence:\n","$$\n","\\alpha_{t, i} = f(h_{t-1}^\\text{dec}, h_i^\\text{enc}).\n","$$\n","Then, a context vector $c_t$ is computed by a weighted sum of the encoder hidden states:\n","$$\n","\\bar{\\alpha}_{t, :} = \\text{softmax}(\\alpha_{t, :}),\n","$$\n","$$\n","c_t = \\sum_i \\bar{\\alpha}_{t, i} h_i^\\text{enc}.\n","$$\n","This context vector is expected to capture the most relevant information for prediction at step $t$. Hence, it is concatenated with the original decoder input $x_t$ and fed to the decoder LSTM:\n","$$\n","h_t^\\text{dec} = \\text{LSTM}^\\text{dec}(h_{t-1}^\\text{dec}, \\text{concat}[x_t, c_t]).\n","$$\n","The predicted distribution $p_t$ for the next word is computed by a fully connected layer $g(\\cdot)$:\n","$$\n","p_t = g(\\text{concat}[h_t^\\text{dec}, x_t, c_t]).\n","$$"],"id":"final-vessel"},{"cell_type":"markdown","metadata":{"id":"artificial-paintball"},"source":["Let us first implement the attention mechanism. This includes the scoring function $f(\\cdot, \\cdot)$ and the $\\text{softmax}(\\cdot)$. Since the padding tokens do not contain any useful information, they should be masked out in the softmax. Implement the `masked_softmax()` function below that computes softmax after masking out the padding tokens:"],"id":"artificial-paintball"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"42d7b55fcbe845bd58e5ff525b58d425","grade":false,"grade_id":"cell-ba767eb8b78e8f86","locked":false,"schema_version":3,"solution":true,"task":false},"id":"retired-inspiration"},"source":["def masked_softmax(attention_scores, valid_len):\n","  \"\"\"\n","  Inputs:\n","  - attention_scores: A PyTorch tensor of shape (batch_size, max_seq_len)\n","    containing the unnormalized per-token attention scores for a mini-batch of sequences,\n","    each padded to `max_seq_len`. The scores are to be normalized\n","    across the non-padding tokens within each sequence.\n","  - valid_len: A LongTensor of shape (batch_size,)\n","    containing the number of non-padding tokens for each sequence.\n","  \n","  Returns:\n","  - attention_weights: A PyTorch tensor of shape (batch_size, max_seq_len)\n","    containing the normalized per-token attention weights. For each sequence,\n","    the sum of attention weights over non-padding tokens should be (approximately) 1,\n","    while the attention weights for padding tokens should be (approximately) 0.\n","  \"\"\"\n","  mask_value = -1e9\n","  #########################################################################\n","  # TODO:                                                                 #\n","  # Replace attention scores at padding tokens with `mask_value`,         #\n","  # then compute softmax.                                                 #\n","  #########################################################################\n","  # Replace \"pass\" statement with your code\n","  if valid_len is None:\n","    return nn.functional.softmax(attention_scores, dim=-1)\n","  else:\n","    shape = attention_scores.shape\n","    if valid_len.dim() == 1:\n","        valid_len = torch.repeat_interleave(valid_len, repeats=shape[1],\n","                                            dim=0)\n","    else:\n","        valid_len = valid_len.reshape(-1)\n","    attention_scores = attention_scores.reshape(-1, shape[-1])\n","    for count, row in enumerate(attention_scores):\n","        row[int(valid_len[count]):]=-1e6\n","    attention_weights = nn.functional.softmax(attention_scores.reshape(shape), dim=-1)    \n","  # END OF YOUR CODE\n","  return attention_weights"],"id":"retired-inspiration","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"passing-chrome"},"source":["We will use the following helper function to check your results:"],"id":"passing-chrome"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"bf0b8d4eda0e0ae7cffaa398a43e9e79","grade":false,"grade_id":"cell-e3bdcd8a834393ea","locked":true,"schema_version":3,"solution":false,"task":false},"id":"smart-smell"},"source":["def check_masked_softmax(batch_size, max_seq_len):\n","  \n","  torch.manual_seed(0)\n","  attention_scores = torch.empty(batch_size, max_seq_len).normal_()\n","  valid_len = torch.randint(1, max_seq_len + 1, (batch_size,))\n","  \n","  attention_weights = masked_softmax(attention_scores, valid_len)\n","  \n","  attention_weights_cumsum = attention_weights.cumsum(dim=-1)\n","  \n","  attention_weights_cumsum_all = attention_weights_cumsum[:, -1]\n","  attention_weights_cumsum_valid = attention_weights_cumsum[torch.arange(batch_size), valid_len - 1]\n","  \n","  assert (attention_weights >= 0).all()\n","  assert max_diff(attention_weights_cumsum_all, 1) < 1e-5\n","  assert max_diff(attention_weights_cumsum_valid, 1) < 1e-5\n","  print('Test passed')"],"id":"smart-smell","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"a3d4f19de4e4ed3af1db741143220732","grade":true,"grade_id":"cell-0e9a724c184ff799","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"id":"creative-shock","colab":{"base_uri":"https://localhost:8080/","height":266},"executionInfo":{"status":"error","timestamp":1616984292318,"user_tz":240,"elapsed":323,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"47902798-ce6a-4c05-f304-6c21edb94900"},"source":["check_masked_softmax(batch_size=64, max_seq_len=32)"],"id":"creative-shock","execution_count":null,"outputs":[{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-181-122814c9edf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_masked_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_seq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-180-d4124a344a9b>\u001b[0m in \u001b[0;36mcheck_masked_softmax\u001b[0;34m(batch_size, max_seq_len)\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_weights\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;32massert\u001b[0m \u001b[0mmax_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights_cumsum_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mmax_diff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_weights_cumsum_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Test passed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"white-consensus"},"source":["Next, we will implement the scoring function as an MLP with one hidden layer. Consider the general setting where we are given a set of key-value pairs $\\{(k_i, v_i)\\}$ and a query $q$. The attention scores can be computed as:\n","$$\n","\\alpha(q, k_i) = w^\\top \\text{tanh}(W_q q + W_k k_i + b),\n","$$\n","where $w$, $W_q$, $W_k$, and $b$ are learnable parameters, and the dimension of $w$ is the hidden size of the MLP.\n","\n","The attention output is a weighted sum of values:\n","$$\n","c = \\sum_i \\bar{\\alpha}(q, k_i) v_i,\n","$$\n","where $\\bar{\\alpha}(q, k_i)$ is the normalized attention weights."],"id":"white-consensus"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"d5ff12524b868ada7c6d51a2d8db6ec8","grade":false,"grade_id":"cell-2df3d66464cd400a","locked":false,"schema_version":3,"solution":true,"task":false},"id":"affected-reader"},"source":["class MLP_Attention(nn.Module):\n","  \n","  def __init__(self, query_dim, key_dim, hidden_size):\n","    super().__init__()\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Set up the module components.                                         #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    pass\n","    # END OF YOUR CODE\n","  \n","  \n","  def forward(self, query, keys, values, valid_len):\n","    # type: (Tensor, Tensor, Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    Inputs:\n","    - query: A PyTorch tensor of shape (batch_size, query_dim).\n","    - keys: A PyTorch tensor of shape (batch_size, max_seq_len, key_dim)\n","      containing the per-token keys for a mini-batch of sequences,\n","      each padded to `max_seq_len`.\n","    - values: A PyTorch tensor of shape (batch_size, max_seq_len, value_dim)\n","      containing the per-token values for a mini-batch of sequences,\n","      each padded to `max_seq_len`.\n","    - valid_len: A LongTensor of shape (batch_size,)\n","      containing the number of non-padding tokens for each sequence.\n","    \n","    Returns:\n","    - attention: A PyTorch tensor of shape (batch_size, value_dim)\n","      containing the weighted sum of values, where the weights are\n","      computed by MLP attention over the non-padding tokens.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass without explicit loops.                    #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    pass\n","    # END OF YOUR CODE\n","    return attention"],"id":"affected-reader","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"technical-sharp"},"source":["Now we can implement the seq2seq model with attention. Note that only the decoder needs to be changed. Implement `DecoderAttLSTM` below:"],"id":"technical-sharp"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"0f493a6f7df2597b50b872c51211e6f1","grade":false,"grade_id":"cell-b8f7b0ce55a520c0","locked":false,"schema_version":3,"solution":true,"task":false},"id":"cultural-exception"},"source":["class DecoderAttLSTM(nn.Module):\n","  \n","  def __init__(self, vocab_size, embedding_dim, hidden_size):\n","    super().__init__()\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Set up the decoder components.                                        #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    pass\n","    # END OF YOUR CODE\n","  \n","  \n","  def forward(self, target_text, target_len, source_last_state, source_hidden_states, source_len):\n","    # type: (Tensor, Tensor, Tuple[Tensor, Tensor], Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    Perform next word prediction and compute loss.\n","    \n","    Inputs:\n","    - target_text: A LongTensor of shape (batch_size, target_seq_len)\n","      containing a mini-batch of target sentences padded to `target_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - target_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each target sentence (before being padded).\n","    - source_last_state: A tuple of hidden state and cell state at the end of\n","      each source sentence, both of shape (batch_size, hidden_size).\n","    - source_hidden_states: A PyTorch tensor of shape (batch_size, source_seq_len, hidden_size)\n","      containing the LSTM hidden state `ht` at each step t of the source sentences.\n","    - source_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each source sentence (before being padded).\n","    \n","    Returns:\n","    - loss: A PyTorch scalar containing the loss for the mini-batch.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the forward pass.                                           #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    pass\n","    # END OF YOUR CODE\n","    return loss\n","  \n","  \n","  def predict(self, source_last_state, max_pred_len, source_hidden_states, source_len):\n","    # type: (Tuple[Tensor, Tensor], int, Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    Predict target sentences given the LSTM encoding of source sentences.\n","    The predicted word at step t will be used as input for step t+1.\n","    This is different from what is done in the forward pass.\n","    \n","    Inputs:\n","    - source_last_state: A tuple of hidden state and cell state at the end of\n","      each source sentence, both of shape (batch_size, hidden_size).\n","    - max_pred_len: (int) number of prediction steps.\n","    - source_hidden_states: A PyTorch tensor of shape (batch_size, source_seq_len, hidden_size)\n","      containing the LSTM hidden state `ht` at each step t of the source sentences.\n","    - source_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each source sentence (before being padded).\n","    \n","    Returns:\n","    - pred: A LongTensor of shape (batch_size, max_pred_len)\n","      containing the predicted target sentences.\n","      Each token is represented by its index in the vocabulary.\n","    \"\"\"\n","    #########################################################################\n","    # TODO:                                                                 #\n","    # Implement the prediction phase.                                       #\n","    #########################################################################\n","    # Replace \"pass\" statement with your code\n","    pass\n","    # END OF YOUR CODE\n","    return pred"],"id":"cultural-exception","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"020ecce0b89a0c946aba02928b576a1c","grade":false,"grade_id":"cell-0655e38c41673d2a","locked":true,"schema_version":3,"solution":false,"task":false},"id":"lined-clothing"},"source":["class NMT_AttLSTM(nn.Module):\n","  \n","  def __init__(self, source_vocab_size, target_vocab_size, embedding_dim, hidden_size, max_pred_len):\n","    super().__init__()\n","    \n","    self.enc = EncoderLSTM(source_vocab_size, embedding_dim, hidden_size)\n","    self.dec = DecoderAttLSTM(target_vocab_size, embedding_dim, hidden_size)\n","    self.max_pred_len = max_pred_len\n","  \n","  \n","  def forward(self, source_text, source_len, target_text, target_len):\n","    # type: (Tensor, Tensor, Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    Perform next word prediction and compute loss.\n","    \n","    Inputs:\n","    - source_text: A LongTensor of shape (batch_size, source_seq_len)\n","      containing a mini-batch of source sentences padded to `source_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - source_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each source sentence (before being padded).\n","    - target_text: A LongTensor of shape (batch_size, target_seq_len)\n","      containing a mini-batch of target sentences padded to `target_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - target_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each target sentence (before being padded).\n","    \n","    Returns:\n","    - loss: A PyTorch scalar containing the loss for the mini-batch.\n","    \"\"\"\n","    source_hidden_states, source_last_state = self.enc(source_text, source_len)\n","    loss = self.dec(target_text, target_len, source_last_state, source_hidden_states, source_len)\n","    return loss\n","  \n","  \n","  def predict(self, source_text, source_len):\n","    # type: (Tensor, Tensor) -> Tensor\n","    \"\"\"\n","    Predict the target sentence for each source sentence.\n","    The predicted word at step t will be used as input for step t+1.\n","    This is different from what is done in the forward pass.\n","    \n","    Inputs:\n","    - source_text: A LongTensor of shape (batch_size, source_seq_len)\n","      containing a mini-batch of source sentences padded to `source_seq_len`.\n","      Each token is represented by its index in the vocabulary.\n","    - source_len: A LongTensor of shape (batch_size,)\n","      containing the actual length of each source sentence (before being padded).\n","    \n","    Returns:\n","    - pred: A LongTensor of shape (batch_size, max_pred_len)\n","      containing the predicted target sentences.\n","      Each token is represented by its index in the vocabulary.\n","    \"\"\"\n","    source_hidden_states, source_last_state = self.enc(source_text, source_len)\n","    pred = self.dec.predict(source_last_state, self.max_pred_len, source_hidden_states, source_len)\n","    return pred"],"id":"lined-clothing","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"medieval-horse"},"source":["### Training\n","\n","A basic training script is provided below. You can make modifications as appropriate.\n","\n","To get full credit, your model should **achieve at least 44% validation accuracy**."],"id":"medieval-horse"},{"cell_type":"code","metadata":{"id":"artificial-general"},"source":["torch.manual_seed(0)\n","\n","batch_size = 40\n","num_epochs = 50\n","learning_rate = 0  # TODO\n","weight_decay = 0  # TODO\n","\n","embedding_dim = 256\n","hidden_size = 256\n","\n","source_vocab, target_vocab, train_loader, val_loader = get_data_loader(\n","  source_text, target_text, batch_size)\n","model = NMT_AttLSTM(source_vocab.size, target_vocab.size, embedding_dim, hidden_size, MAX_LEN + 1)\n","model = model.cuda()\n","\n","model, loss_history = train(model, train_loader, val_loader, num_epochs, learning_rate, weight_decay)"],"id":"artificial-general","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"confidential-saskatchewan"},"source":["plt.plot(loss_history, 'o')\n","plt.xlabel('Iteration number')\n","plt.ylabel('Loss value')\n","plt.show()"],"id":"confidential-saskatchewan","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"worthy-stewart"},"source":["train_acc = eval_acc(model, train_loader)\n","print('Training Accuracy:', train_acc)"],"id":"worthy-stewart","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"b64187c3a8ab06b945f8de1b44520d28","grade":true,"grade_id":"cell-16c7aadecc11343f","locked":true,"points":35,"schema_version":3,"solution":false,"task":false},"id":"responsible-gentleman"},"source":["val_acc = eval_acc(model, val_loader)\n","print('Validation Accuracy:', val_acc)\n","assert val_acc >= 44"],"id":"responsible-gentleman","execution_count":null,"outputs":[]}]}