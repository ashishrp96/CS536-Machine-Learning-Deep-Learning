{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.12"},"colab":{"name":"dqn.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"dangerous-charles"},"source":["---\n","# Deep Q-Learning\n","\n","In this assignment, you will implement DQN for deep reinforcement learning from high-dimensional observations.\n","\n","As in previous assignments, you will see code blocks that look like this:\n","```python\n","###############################################################################\n","# TODO: Create a variable x with value 536.                                   #\n","###############################################################################\n","# Replace \"pass\" statement with your code\n","pass\n","# END OF YOUR CODE\n","```\n","\n","You should replace the `pass` statement with your own code and leave the blocks intact, like this:\n","```python\n","###############################################################################\n","# TODO: Create a variable x with value 536.                                   #\n","###############################################################################\n","# Replace \"pass\" statement with your code\n","x = 536\n","# END OF YOUR CODE\n","```\n","\n","Also, please remember:\n","- Do not write or modify any code outside of code blocks unless otherwise stated.\n","- Do not delete any cells from the notebook. You may add new cells to perform scratch work, but delete them before submitting.\n","- Run all cells before submitting. You will only get credit for code that has been run.\n","- Submit your notebook as `netid.ipynb`, where `netid` is your actual netid.\n","- Your submission will be graded with PyTorch 1.7.0 and Python 3.6, which are the default versions in Google Colab."],"id":"dangerous-charles"},{"cell_type":"code","metadata":{"id":"initial-beginning","executionInfo":{"status":"ok","timestamp":1620884168582,"user_tz":240,"elapsed":4859,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["from __future__ import print_function\n","from __future__ import division\n","\n","import math\n","import random\n","import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.nn.utils import clip_grad_norm_\n","\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","plt.rcParams['figure.figsize'] = (10.0, 8.0)"],"id":"initial-beginning","execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"personal-claim","executionInfo":{"status":"ok","timestamp":1620884168588,"user_tz":240,"elapsed":4856,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["assert torch.cuda.is_available(), 'GPU unavailable'"],"id":"personal-claim","execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"endless-charge"},"source":["## Toy Environment\n","\n","We first introduce a toy environment that we can use to test our implementation quickly:\n","\n","- **4 states**: 0, 1, 2, 3. Each state corresponds to an observation of shape $84 \\times 84 \\times 4$.\n","- **5 actions**: 0, 1, 2, 3, 4. Action $0 \\le i \\le 3$ goes straight to state $i$. Action 4 makes the agent stay in the same state.\n","- **Rewards**: Going to state $i$ from states 0, 1, and 3 gives a reward $R(i)$, where $R(0)=0.1$, $R(1)=-0.2$, $R(2)=0$, and $R(3)=-0.1$. If we start in state 2, then the rewards defined above are multiplied by -20. The discount rate $\\gamma=1$.\n","- **Episode length**: One episode lasts 5 timesteps (for a total of 5 actions) and always starts in state 0 (no rewards at the initial state).\n","\n","For example, one episode may have the following actions: 1, 2, 4, 3, 0. This would result in the following states: 0 (initial), 1, 2, 2, 3, 0. And the following rewards: -0.2, 0, 0, 2, 0.1."],"id":"endless-charge"},{"cell_type":"markdown","metadata":{"id":"anticipated-playback"},"source":["What is the maximum sum of rewards that can be achieved in a single episode? Assign your answer to the variable `max_reward`. Also, provide an `action_sequence` of length 5 that achieves the maximum sum of rewards."],"id":"anticipated-playback"},{"cell_type":"code","metadata":{"id":"white-elephant","executionInfo":{"status":"ok","timestamp":1620884168589,"user_tz":240,"elapsed":4837,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["#########################################################################\n","# TODO:                                                                 #\n","# Set `max_reward` to the maximum sum of rewards that can be achieved   #\n","# in a single episode in the above toy environment.                     #\n","#########################################################################\n","max_reward = 8.1"],"id":"white-elephant","execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"08ad0bd0bf30a83e0fde8008cbe6445f","grade":true,"grade_id":"cell-55ad8e55c37d2683","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"id":"measured-matter","executionInfo":{"status":"ok","timestamp":1620884168590,"user_tz":240,"elapsed":4820,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["# Hidden test cell for `max_reward`. Do not delete."],"id":"measured-matter","execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"refined-oliver","executionInfo":{"status":"ok","timestamp":1620884168591,"user_tz":240,"elapsed":4816,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["#########################################################################\n","# TODO:                                                                 #\n","# Set `action_sequence` to a sequence of 5 actions that achieves        #\n","# the maximum sum of rewards in the above toy environment.              #\n","#########################################################################\n","action_sequence = [2,1,2,1,0]\n","\n","assert len(action_sequence) == 5"],"id":"refined-oliver","execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"15ab347d133c5129053e3c516af7fe62","grade":true,"grade_id":"cell-9639a0902a5a3934","locked":true,"points":5,"schema_version":3,"solution":false,"task":false},"id":"minute-jackson","executionInfo":{"status":"ok","timestamp":1620884168592,"user_tz":240,"elapsed":4813,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["# Hidden test cell for `action_sequence`. Do not delete."],"id":"minute-jackson","execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"minute-creek"},"source":["Below is an implementation of the above toy environment. It has the same interface as [OpenAI Gym](https://gym.openai.com/), so hopefully what you implement in this assignment can also be used for the various environments available in OpenAI Gym."],"id":"minute-creek"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"0aa5ed3c269ee08e6651a5f2015b3d31","grade":false,"grade_id":"cell-1c5dd5a1d227650e","locked":true,"schema_version":3,"solution":false,"task":false},"id":"administrative-richards","executionInfo":{"status":"ok","timestamp":1620884168593,"user_tz":240,"elapsed":4811,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class ObservationSpace():\n","\n","    def __init__(self, shape=(84, 84, 4)):\n","        self.shape = shape\n","        self.state_0 = np.random.randint(0, 30, shape, dtype=np.uint8)\n","        self.state_1 = np.random.randint(60, 90, shape, dtype=np.uint8)\n","        self.state_2 = np.random.randint(120, 150, shape, dtype=np.uint8)\n","        self.state_3 = np.random.randint(180, 210, shape, dtype=np.uint8)\n","        self.states = [self.state_0, self.state_1, self.state_2, self.state_3]\n","\n","\n","class ActionSpace():\n","\n","    def __init__(self, n=5):\n","        self.n = n\n","\n","    def seed(self, seed):\n","        # no-op\n","        self.seed=0\n","\n","    def sample(self):\n","        return np.random.randint(0, self.n)\n","\n","\n","class ToyEnv():\n","\n","    def __init__(self, shape=(84, 84, 4)):\n","        self.observation_space = ObservationSpace(shape)\n","        self.action_space = ActionSpace(5)\n","        self.rewards = [0.1, -0.2, 0.0, -0.1]\n","        self.cur_state = 0\n","        self.num_steps = 0\n","        self.was_in_state_2 = False\n","\n","    def seed(self, seed):\n","        # no-op\n","        self.seed=0\n","\n","    def reset(self):\n","        self.cur_state = 0\n","        self.num_steps = 0\n","        self.was_in_state_2 = False\n","        return self.observation_space.states[self.cur_state]\n","\n","    def step(self, action):\n","        assert action in range(5), 'invalid action'\n","        self.num_steps += 1\n","        if action < 4:\n","            self.cur_state = action\n","        reward = self.rewards[self.cur_state]\n","        if self.was_in_state_2:\n","            reward *= -20\n","        if self.cur_state == 2:\n","            self.was_in_state_2 = True\n","        else:\n","            self.was_in_state_2 = False\n","        return self.observation_space.states[self.cur_state], reward, self.num_steps >= 5, {}\n","\n","    def render(self):\n","        print(self.cur_state)"],"id":"administrative-richards","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tired-semester"},"source":["## Q-Network\n","\n","The Q-network takes in the observation corresponding to a state $s$, and outputs the action-value $Q(s, a)$ for each valid action $a$. The input is a FloatTensor of shape $(\\text{batch_size}, 84, 84, \\text{in_channels})$, and the output is a FloatTensor of shape $(\\text{batch_size}, \\text{num_actions})$. You may use the architecture described in the [paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)."],"id":"tired-semester"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"9c6898ed2bb411a47e3fb618cc1d688c","grade":false,"grade_id":"cell-dd142a03652d5e0c","locked":false,"schema_version":3,"solution":true,"task":false},"id":"utility-christmas","executionInfo":{"status":"ok","timestamp":1620884168594,"user_tz":240,"elapsed":4809,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class ReshapeImage(nn.Module):\n","    \n","    def __init__(self):\n","        super().__init__()\n","    \n","    def forward(self, x):\n","        if len(x.shape) < 4:\n","            x = x[None, ...]\n","        x = x.permute(0, 3, 1, 2).contiguous()\n","        return x\n","\n","\n","def create_q_network(in_channels, num_actions):\n","    \n","    return nn.Sequential(\n","        ReshapeImage(),\n","        #########################################################################\n","        # TODO:                                                                 #\n","        # Implement the Q-network.                                              #\n","        #########################################################################\n","        # Replace \"pass\" statement with your code\n","        nn.Conv2d(in_channels=in_channels, out_channels= 32 , kernel_size=3),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","        nn.ReLU(),\n","        nn.Conv2d(in_channels=32, out_channels= 64 , kernel_size=2),\n","        nn.MaxPool2d(kernel_size=2, stride=2),\n","        nn.ReLU(),\n","        nn.Conv2d(in_channels=64, out_channels= 20 , kernel_size=3),\n","        nn.ReLU(),\n","        nn.Flatten(start_dim=1), \n","        nn.Linear(6480, 100),\n","        nn.ReLU(),\n","        nn.Linear(100, 20),\n","        nn.ReLU(),\n","        nn.Linear(20,num_actions)\n","        \n","\n","        # END OF YOUR CODE\n","    )"],"id":"utility-christmas","execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"heavy-bracket"},"source":["## $\\epsilon$-Greedy Behavior Policy\n","\n","Q-learning is off-policy, which means the transitions are collected by a behavior policy that is different from the target policy we are trying to optimize. The behavior policy used in DQN is $\\epsilon$-greedy, with $\\epsilon$ annealed linearly from 1.0 to 0.1. Implement `LinearSchedule` below, which linearly anneals the `value` from `value_begin` to `value_end` over `nsteps`."],"id":"heavy-bracket"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"2b64b1d9524f0172ca7205ce19fc1b2e","grade":false,"grade_id":"cell-3cb7ca2fe1d625d6","locked":false,"schema_version":3,"solution":true,"task":false},"id":"junior-league","executionInfo":{"status":"ok","timestamp":1620884168595,"user_tz":240,"elapsed":4806,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class LinearSchedule():\n","    \n","    def __init__(self, value_begin, value_end, nsteps):\n","        self.value_begin = value_begin\n","        self.value_end = value_end\n","        self.nsteps = nsteps\n","        \n","        # current value\n","        self.value = value_begin\n","    \n","    def update(self, t):\n","        #########################################################################\n","        # TODO:                                                                 #\n","        # Modify self.value such that it is a linear interpolation from         #\n","        # self.value_begin to self.value_end as t goes from 0 to self.nsteps.   #\n","        # For t > self.nsteps, self.value = self.value_end.                     #\n","        #########################################################################\n","        # Replace \"pass\" statement with your code\n","        \n","        factor = min(float(t)/self.nsteps, 1.0)\n","        self.value = max(self.value_end, self.value_begin + factor * (self.value_end - self.value_begin))\n","        # END OF YOUR CODE"],"id":"junior-league","execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"according-logan"},"source":["The `BehaviorPolicy` can now be implemented as a subclass of `LinearSchedule`."],"id":"according-logan"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"9d33bcc11621e88e60b35e22dde5a8e2","grade":false,"grade_id":"cell-6d7127b9c02201ab","locked":true,"schema_version":3,"solution":false,"task":false},"id":"latin-sally","executionInfo":{"status":"ok","timestamp":1620884168596,"user_tz":240,"elapsed":4798,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class BehaviorPolicy(LinearSchedule):\n","    \n","    def __init__(self, env, value_begin, value_end, nsteps):\n","        super().__init__(value_begin, value_end, nsteps)\n","        self.env = env\n","    \n","    def get_action(self, best_action):\n","        \"\"\"\n","        With probability self.value, returns a random action; otherwise returns best_action.\n","        \"\"\"\n","        if np.random.random() < self.value:\n","            return self.env.action_space.sample()\n","        else:\n","            return best_action"],"id":"latin-sally","execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"residential-coffee"},"source":["## Experience Replay\n","\n","DQN uses experience replay to stabilize training. It stores the transitions into a replay buffer, and samples mini-batches from the buffer to apply Q-learning updates. An implementation of `ReplayBuffer` is provided below. You need to implement the `sample` function that samples a mini-batch of transitions. You may want to read through the code to understand what it is doing."],"id":"residential-coffee"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"7e5397368a33712f93d48cea1ca7efa7","grade":false,"grade_id":"cell-f0c86c619afaaa7e","locked":false,"schema_version":3,"solution":true,"task":false},"id":"advance-computer","executionInfo":{"status":"ok","timestamp":1620884169034,"user_tz":240,"elapsed":5231,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class ReplayBuffer():\n","    \n","    def __init__(self, size, frame_history_len):\n","        \"\"\"\n","        An implementation of the replay buffer.\n","        \n","        Parameters\n","        ----------\n","        size: int\n","            Max number of frames to store in the buffer. When the buffer is full,\n","            the oldest frames are dropped first.\n","        frame_history_len: int\n","            Number of consecutive frames to use for each observation.\n","        \"\"\"\n","        self.size = size\n","        self.frame_history_len = frame_history_len\n","        \n","        self.next_idx = 0\n","        self.num_frames_in_buffer = 0\n","        \n","        self.obs    = None\n","        self.action = None\n","        self.reward = None\n","        self.done   = None\n","    \n","    def store_frame(self, frame):\n","        \"\"\"\n","        Stores a single frame into the buffer at the next available index, overwriting\n","        old frames if necessary.\n","        \n","        Parameters\n","        ----------\n","        frame: np.array of shape (img_h, img_w, img_c) and dtype np.uint8\n","            The frame to be stored.\n","        \"\"\"\n","        if self.obs is None:\n","            self.obs    = np.empty([self.size] + list(frame.shape), dtype=np.uint8)\n","            self.action = np.empty([self.size],                     dtype=np.int32)\n","            self.reward = np.empty([self.size],                     dtype=np.float32)\n","            self.done   = np.empty([self.size],                     dtype=np.bool)\n","        \n","        idx = self.next_idx\n","        self.obs[idx] = frame\n","        \n","        self.next_idx = (idx + 1) % self.size\n","        self.num_frames_in_buffer = min(self.size, self.num_frames_in_buffer + 1)\n","    \n","    def store_effect(self, action, reward, done):\n","        \"\"\"\n","        Stores the effects of the `action`.\n","        \n","        Parameters\n","        ----------\n","        action: int\n","            Action taken upon last observation.\n","        reward: float\n","            Reward received from taking the action.\n","        done: bool\n","            True if the episode terminates after taking the action.\n","        \"\"\"\n","        idx = (self.next_idx - 1) % self.size\n","        self.action[idx] = action\n","        self.reward[idx] = reward\n","        self.done[idx]   = done\n","    \n","    def get_observation(self):\n","        \"\"\"\n","        Returns the most recent `frame_history_len` frames, to be used as\n","        input to the Q-network.\n","        \n","        Returns\n","        -------\n","        observation: np.array of shape (img_h, img_w, frame_history_len * img_c)\n","            and dtype np.uint8\n","        \"\"\"\n","        assert self.num_frames_in_buffer > 0\n","        return self._get_frame_history((self.next_idx - 1) % self.size)\n","    \n","    def _get_frame_history(self, end_idx):\n","        end_idx = end_idx + 1  # make `end_idx` exclusive\n","        start_idx = end_idx - self.frame_history_len\n","        \n","        if start_idx < 0 and self.num_frames_in_buffer < self.size:\n","            # not enough frames in buffer\n","            start_idx = 0\n","        \n","        # avoid mixing different episodes\n","        for idx in range(start_idx, end_idx - 1):\n","            if self.done[idx % self.size]:\n","                start_idx = idx + 1\n","        \n","        num_missing_frames = self.frame_history_len - (end_idx - start_idx)\n","        if start_idx < 0 or num_missing_frames > 0:\n","            pad = [np.zeros_like(self.obs[0]) for _ in range(num_missing_frames)]\n","            frames = pad + [self.obs[idx % self.size] for idx in range(start_idx, end_idx)]\n","            return np.concatenate(frames, 2)\n","        else:\n","            img_h, img_w = self.obs.shape[1], self.obs.shape[2]\n","            return self.obs[start_idx : end_idx].transpose(1, 2, 0, 3).reshape(img_h, img_w, -1)\n","    \n","    def _can_sample(self, batch_size):\n","        \"\"\"\n","        Returns True if `batch_size` different transitions can be sampled from the buffer.\n","        \"\"\"\n","        if self.num_frames_in_buffer < self.size:\n","            return batch_size + 1 <= self.num_frames_in_buffer\n","        else:\n","            return batch_size + self.frame_history_len <= self.size\n","    \n","    def sample(self, batch_size):\n","        \"\"\"\n","        Samples `batch_size` different transitions.\n","        \n","        The i-th sampled transition is the following:\n","        \n","        When observing `obs_batch[i]`, the agent took action `act_batch[i]`,\n","        after which reward `rew_batch[i]` was received and subsequent\n","        observation `next_obs_batch[i]` was obtained, unless the episode\n","        was done which is represented by `done_mask[i]`. That is,\n","        done_mask[i] = 1.0 if the episode terminates after taking the action.\n","        \n","        Parameters\n","        ----------\n","        batch_size: int\n","            Number of transitions to sample.\n","        \n","        Returns\n","        -------\n","        obs_batch: np.array of shape (batch_size, img_h, img_w, frame_history_len * img_c)\n","            and dtype np.uint8\n","        act_batch: np.array of shape (batch_size,) and dtype np.int32\n","        rew_batch: np.array of shape (batch_size,) and dtype np.float32\n","        next_obs_batch: np.array of shape (batch_size, img_h, img_w, frame_history_len * img_c)\n","            and dtype np.uint8\n","        done_mask: np.array of shape (batch_size,) and dtype np.float32\n","        \"\"\"\n","        assert self._can_sample(batch_size)\n","        #########################################################################\n","        # TODO:                                                                 #\n","        # Sample a batch of different transitions.                              #\n","        #########################################################################\n","        \n","        indices = []\n","        for _ in range(batch_size):\n","          div_idx = random.randint(0, self.num_frames_in_buffer - 2)\n","          if div_idx not in indices:\n","            indices.append(div_idx)\n","        obs_batch      = np.concatenate([self._get_frame_history(idx)[np.newaxis, :] for idx in indices], 0)\n","        act_batch      = self.action[indices]\n","        rew_batch      = self.reward[indices]\n","        next_obs_batch = np.concatenate([self._get_frame_history(idx + 1)[np.newaxis, :] for idx in indices], 0)\n","        done_mask      = np.array([self.done[idx] for idx in indices], dtype=np.float32)\n","\n","        return obs_batch, act_batch, rew_batch, next_obs_batch, done_mask"],"id":"advance-computer","execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"worldwide-finder"},"source":["## DQN Training\n","\n","Implement Algorithm 1 in the [paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). Make sure to use `with torch.no_grad()` or `detach()` to block unwanted gradient flow."],"id":"worldwide-finder"},{"cell_type":"code","metadata":{"deletable":false,"nbgrader":{"cell_type":"code","checksum":"04d0c2694777ef80a9e774a049b26987","grade":false,"grade_id":"cell-27b7ef5711fe962c","locked":false,"schema_version":3,"solution":true,"task":false},"id":"interesting-afternoon","executionInfo":{"status":"ok","timestamp":1620884169035,"user_tz":240,"elapsed":5228,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["class DQN:\n","    \n","    def __init__(self, env, config):\n","        self.env = env\n","        self.config = config\n","        \n","        self.replay_buffer = ReplayBuffer(config['buffer_size'], config['frame_history_len'])\n","        self.behavior = BehaviorPolicy(env, config['eps_begin'], config['eps_end'], config['eps_nsteps'])\n","        \n","        self.q_net = None\n","        self.q_net_target = None\n","        self.optimizer = None\n","        #########################################################################\n","        # TODO:                                                                 #\n","        # Initialize the Q-network `self.q_net`, target Q-network               #\n","        # `self.q_net_target`, and optimizer.                                   #\n","        #########################################################################\n","        # Replace \"pass\" statement with your code\n","        self.q_net = create_q_network(env.observation_space.shape[2]*config['frame_history_len'], env.action_space.n).to(config['device'])\n","        self.q_net_target = create_q_network(env.observation_space.shape[2]*config['frame_history_len'], env.action_space.n).to(config['device'])\n","        # self.q_net_target.load_state_dict(self.q_net.state_dict())\n","        self.optimizer = optim.RMSprop(self.q_net.parameters(), lr=config['lr'])\n","        # END OF YOUR CODE\n","    \n","    def _update_target_params(self):\n","        \"\"\"\n","        Updates `self.q_net_target` parameters by copying parameters from `self.q_net`.\n","        \"\"\"\n","        with torch.no_grad():\n","            self.q_net_target.load_state_dict(self.q_net.state_dict())\n","    \n","    def train(self):\n","        t = 0  # total number of actions taken\n","        rewards = []  # to store the cumulative reward for each episode\n","        while t < self.config['train_nsteps']:\n","            gamma = 1.\n","            total_reward = 0.\n","            \n","            # start a new episode\n","            state = self.env.reset()\n","            self.replay_buffer.store_frame(state)\n","            \n","            # while episode does not terminate\n","            while True:\n","                # get `frame_history_len` most recent frames\n","                # as an input observation to the Q-network\n","                q_input = self.replay_buffer.get_observation()\n","                q_input = torch.from_numpy(q_input).to(self.config['device']).float()\n","                \n","                action = None\n","                #########################################################################\n","                # TODO:                                                                 #\n","                # Use the behavior policy to select action.                             #\n","                # You may directly use `q_input` as \\phi(s_t) in the paper.             #\n","                #########################################################################\n","                # Replace \"pass\" statement with your code\n","                out = self.q_net(q_input)\n","                _, actions= torch.max(out, dim=1)\n","                self.behavior.update(t)\n","                action = self.behavior.get_action(actions.item())\n","                # END OF YOUR CODE\n","                \n","                # execute action and store transition\n","                new_state, reward, done, info = self.env.step(action)\n","                self.replay_buffer.store_effect(action, reward, done)\n","                if not done:\n","                    state = new_state\n","                    self.replay_buffer.store_frame(state)\n","                \n","                if t >= self.config['learning_start']:\n","                    #########################################################################\n","                    # TODO:                                                                 #\n","                    # Sample a mini-batch of transitions from the replay buffer,            #\n","                    # and perform a gradient descent step on `self.q_net`.                  #\n","                    #########################################################################\n","                    # Replace \"pass\" statement with your code\n","                    obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = self.replay_buffer.sample(self.config['batch_size'])\n","                    obs_batch = torch.tensor(obs_batch).to(config['device']).float()\n","                    act_batch = torch.tensor(act_batch).to(config['device']).long()\n","                    rew_batch = torch.tensor(rew_batch).to(config['device']).float()\n","                    next_obs_batch = torch.tensor(next_obs_batch).to(config['device']).float()\n","                    done_mask_n = torch.tensor(1-done_mask).to(config['device']).float()\n","\n","                    out = self.q_net(obs_batch).gather(1,act_batch.unsqueeze(1))\n","                    out_target = torch.max(self.q_net_target(next_obs_batch), dim=1)\n","                    out_target = rew_batch + (gamma*done_mask_n*out_target[0])\n","                    loss = torch.sum((out_target - out.squeeze())**2)\n","\n","                    self.optimizer.zero_grad()\n","                    loss.backward()\n","                    self.optimizer.step()\n","                    # END OF YOUR CODE\n","                    \n","                    # periodically update the target network\n","                    if (t + 1 - self.config['learning_start']) % self.config['target_update_freq'] == 0:\n","                        self._update_target_params()\n","                \n","                # cumulative reward\n","                total_reward += gamma * reward\n","                gamma *= self.config['gamma']\n","                \n","                t += 1\n","                if done or t >= self.config['train_nsteps']:\n","                    break\n","            \n","            # end of episode\n","            if done:\n","                rewards.append(total_reward)\n","                print(f'step: {t} \\t epsilon: {self.behavior.value:.4f} \\t cumulative reward: {total_reward: .1f}')\n","        \n","        return rewards"],"id":"interesting-afternoon","execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"diagnostic-breed","executionInfo":{"status":"ok","timestamp":1620884169036,"user_tz":240,"elapsed":5220,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["def run(config):\n","    \n","    random.seed(config['seed'])\n","    np.random.seed(config['seed'])\n","    torch.manual_seed(config['seed'])\n","    \n","    env = config['env']()\n","    env.seed(config['seed'])\n","    env.action_space.seed(config['seed'])\n","    \n","    dqn = DQN(env, config)\n","    \n","    rewards = dqn.train()\n","    return dqn, rewards"],"id":"diagnostic-breed","execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"resistant-chemical","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620884700049,"user_tz":240,"elapsed":536132,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"3f12b28e-f1c4-4de9-ac85-5da027876cfc"},"source":["config = {\n","    'seed': 0,\n","    'device': 'cuda',\n","    \n","    'env': ToyEnv,\n","    'gamma': 1.0,\n","    'frame_history_len':4 ,\n","    \n","    'buffer_size': 1000000,\n","    'batch_size': 32,\n","    \n","    'train_nsteps': 10000,\n","    'learning_start': 500,\n","    'target_update_freq': 500,\n","    \n","    'eps_begin': 1.0,\n","    'eps_end': 0.1,\n","    'eps_nsteps': 9900,\n","    \n","    'lr': 0.00025,\n","    'grad_clip_value': 0.5,\n","    \n","    'num_episodes_eval': 1,\n","}\n","\n","dqn, rewards = run(config)"],"id":"resistant-chemical","execution_count":14,"outputs":[{"output_type":"stream","text":["step: 5 \t epsilon: 0.9996 \t cumulative reward: -0.2\n","step: 10 \t epsilon: 0.9992 \t cumulative reward: -0.7\n","step: 15 \t epsilon: 0.9987 \t cumulative reward:  4.1\n","step: 20 \t epsilon: 0.9983 \t cumulative reward: -0.4\n","step: 25 \t epsilon: 0.9978 \t cumulative reward: -0.5\n","step: 30 \t epsilon: 0.9974 \t cumulative reward: -0.7\n","step: 35 \t epsilon: 0.9969 \t cumulative reward:  0.0\n","step: 40 \t epsilon: 0.9965 \t cumulative reward: -0.4\n","step: 45 \t epsilon: 0.9960 \t cumulative reward: -0.1\n","step: 50 \t epsilon: 0.9955 \t cumulative reward: -2.3\n","step: 55 \t epsilon: 0.9951 \t cumulative reward:  0.5\n","step: 60 \t epsilon: 0.9946 \t cumulative reward: -2.1\n","step: 65 \t epsilon: 0.9942 \t cumulative reward: -2.2\n","step: 70 \t epsilon: 0.9937 \t cumulative reward:  2.2\n","step: 75 \t epsilon: 0.9933 \t cumulative reward: -2.2\n","step: 80 \t epsilon: 0.9928 \t cumulative reward: -1.8\n","step: 85 \t epsilon: 0.9924 \t cumulative reward: -2.5\n","step: 90 \t epsilon: 0.9919 \t cumulative reward:  0.0\n","step: 95 \t epsilon: 0.9915 \t cumulative reward: -0.1\n","step: 100 \t epsilon: 0.9910 \t cumulative reward: -0.3\n","step: 105 \t epsilon: 0.9905 \t cumulative reward:  0.0\n","step: 110 \t epsilon: 0.9901 \t cumulative reward:  0.2\n","step: 115 \t epsilon: 0.9896 \t cumulative reward:  0.0\n","step: 120 \t epsilon: 0.9892 \t cumulative reward:  2.2\n","step: 125 \t epsilon: 0.9887 \t cumulative reward:  3.7\n","step: 130 \t epsilon: 0.9883 \t cumulative reward: -0.4\n","step: 135 \t epsilon: 0.9878 \t cumulative reward:  5.8\n","step: 140 \t epsilon: 0.9874 \t cumulative reward:  0.2\n","step: 145 \t epsilon: 0.9869 \t cumulative reward:  0.1\n","step: 150 \t epsilon: 0.9865 \t cumulative reward: -1.9\n","step: 155 \t epsilon: 0.9860 \t cumulative reward:  3.7\n","step: 160 \t epsilon: 0.9855 \t cumulative reward: -1.7\n","step: 165 \t epsilon: 0.9851 \t cumulative reward: -0.3\n","step: 170 \t epsilon: 0.9846 \t cumulative reward: -1.9\n","step: 175 \t epsilon: 0.9842 \t cumulative reward:  0.2\n","step: 180 \t epsilon: 0.9837 \t cumulative reward: -0.4\n","step: 185 \t epsilon: 0.9833 \t cumulative reward:  7.8\n","step: 190 \t epsilon: 0.9828 \t cumulative reward:  2.0\n","step: 195 \t epsilon: 0.9824 \t cumulative reward:  0.1\n","step: 200 \t epsilon: 0.9819 \t cumulative reward:  5.9\n","step: 205 \t epsilon: 0.9815 \t cumulative reward:  2.0\n","step: 210 \t epsilon: 0.9810 \t cumulative reward:  0.0\n","step: 215 \t epsilon: 0.9805 \t cumulative reward:  5.8\n","step: 220 \t epsilon: 0.9801 \t cumulative reward:  0.2\n","step: 225 \t epsilon: 0.9796 \t cumulative reward: -1.0\n","step: 230 \t epsilon: 0.9792 \t cumulative reward:  2.0\n","step: 235 \t epsilon: 0.9787 \t cumulative reward: -0.6\n","step: 240 \t epsilon: 0.9783 \t cumulative reward:  3.5\n","step: 245 \t epsilon: 0.9778 \t cumulative reward:  0.0\n","step: 250 \t epsilon: 0.9774 \t cumulative reward:  3.5\n","step: 255 \t epsilon: 0.9769 \t cumulative reward:  0.2\n","step: 260 \t epsilon: 0.9765 \t cumulative reward: -0.5\n","step: 265 \t epsilon: 0.9760 \t cumulative reward:  0.2\n","step: 270 \t epsilon: 0.9755 \t cumulative reward: -0.3\n","step: 275 \t epsilon: 0.9751 \t cumulative reward:  4.2\n","step: 280 \t epsilon: 0.9746 \t cumulative reward: -0.4\n","step: 285 \t epsilon: 0.9742 \t cumulative reward: -0.2\n","step: 290 \t epsilon: 0.9737 \t cumulative reward: -2.5\n","step: 295 \t epsilon: 0.9733 \t cumulative reward:  4.0\n","step: 300 \t epsilon: 0.9728 \t cumulative reward:  0.2\n","step: 305 \t epsilon: 0.9724 \t cumulative reward: -0.9\n","step: 310 \t epsilon: 0.9719 \t cumulative reward:  3.6\n","step: 315 \t epsilon: 0.9715 \t cumulative reward: -0.5\n","step: 320 \t epsilon: 0.9710 \t cumulative reward:  3.5\n","step: 325 \t epsilon: 0.9705 \t cumulative reward: -2.3\n","step: 330 \t epsilon: 0.9701 \t cumulative reward: -0.6\n","step: 335 \t epsilon: 0.9696 \t cumulative reward: -0.5\n","step: 340 \t epsilon: 0.9692 \t cumulative reward:  3.9\n","step: 345 \t epsilon: 0.9687 \t cumulative reward: -0.5\n","step: 350 \t epsilon: 0.9683 \t cumulative reward: -0.5\n","step: 355 \t epsilon: 0.9678 \t cumulative reward:  1.9\n","step: 360 \t epsilon: 0.9674 \t cumulative reward:  0.0\n","step: 365 \t epsilon: 0.9669 \t cumulative reward: -2.1\n","step: 370 \t epsilon: 0.9665 \t cumulative reward: -0.3\n","step: 375 \t epsilon: 0.9660 \t cumulative reward: -0.5\n","step: 380 \t epsilon: 0.9655 \t cumulative reward: -1.8\n","step: 385 \t epsilon: 0.9651 \t cumulative reward: -2.0\n","step: 390 \t epsilon: 0.9646 \t cumulative reward: -0.2\n","step: 395 \t epsilon: 0.9642 \t cumulative reward:  0.3\n","step: 400 \t epsilon: 0.9637 \t cumulative reward: -0.7\n","step: 405 \t epsilon: 0.9633 \t cumulative reward: -0.4\n","step: 410 \t epsilon: 0.9628 \t cumulative reward: -0.9\n","step: 415 \t epsilon: 0.9624 \t cumulative reward: -0.3\n","step: 420 \t epsilon: 0.9619 \t cumulative reward:  0.0\n","step: 425 \t epsilon: 0.9615 \t cumulative reward:  0.1\n","step: 430 \t epsilon: 0.9610 \t cumulative reward: -0.5\n","step: 435 \t epsilon: 0.9605 \t cumulative reward:  1.8\n","step: 440 \t epsilon: 0.9601 \t cumulative reward:  0.2\n","step: 445 \t epsilon: 0.9596 \t cumulative reward: -0.3\n","step: 450 \t epsilon: 0.9592 \t cumulative reward:  0.3\n","step: 455 \t epsilon: 0.9587 \t cumulative reward: -0.3\n","step: 460 \t epsilon: 0.9583 \t cumulative reward:  3.4\n","step: 465 \t epsilon: 0.9578 \t cumulative reward:  0.3\n","step: 470 \t epsilon: 0.9574 \t cumulative reward:  3.8\n","step: 475 \t epsilon: 0.9569 \t cumulative reward:  4.0\n","step: 480 \t epsilon: 0.9565 \t cumulative reward: -0.2\n","step: 485 \t epsilon: 0.9560 \t cumulative reward: -0.7\n","step: 490 \t epsilon: 0.9555 \t cumulative reward:  4.1\n","step: 495 \t epsilon: 0.9551 \t cumulative reward: -0.5\n","step: 500 \t epsilon: 0.9546 \t cumulative reward: -2.4\n","step: 505 \t epsilon: 0.9542 \t cumulative reward:  4.3\n","step: 510 \t epsilon: 0.9537 \t cumulative reward: -0.5\n","step: 515 \t epsilon: 0.9533 \t cumulative reward: -0.5\n","step: 520 \t epsilon: 0.9528 \t cumulative reward:  0.1\n","step: 525 \t epsilon: 0.9524 \t cumulative reward:  4.0\n","step: 530 \t epsilon: 0.9519 \t cumulative reward:  0.5\n","step: 535 \t epsilon: 0.9515 \t cumulative reward: -0.9\n","step: 540 \t epsilon: 0.9510 \t cumulative reward:  4.0\n","step: 545 \t epsilon: 0.9505 \t cumulative reward: -1.7\n","step: 550 \t epsilon: 0.9501 \t cumulative reward: -0.0\n","step: 555 \t epsilon: 0.9496 \t cumulative reward: -2.0\n","step: 560 \t epsilon: 0.9492 \t cumulative reward:  3.8\n","step: 565 \t epsilon: 0.9487 \t cumulative reward: -0.5\n","step: 570 \t epsilon: 0.9483 \t cumulative reward: -0.5\n","step: 575 \t epsilon: 0.9478 \t cumulative reward: -0.6\n","step: 580 \t epsilon: 0.9474 \t cumulative reward:  2.0\n","step: 585 \t epsilon: 0.9469 \t cumulative reward:  0.5\n","step: 590 \t epsilon: 0.9465 \t cumulative reward: -2.3\n","step: 595 \t epsilon: 0.9460 \t cumulative reward: -0.6\n","step: 600 \t epsilon: 0.9455 \t cumulative reward: -0.2\n","step: 605 \t epsilon: 0.9451 \t cumulative reward: -0.7\n","step: 610 \t epsilon: 0.9446 \t cumulative reward: -0.7\n","step: 615 \t epsilon: 0.9442 \t cumulative reward:  3.7\n","step: 620 \t epsilon: 0.9437 \t cumulative reward:  2.1\n","step: 625 \t epsilon: 0.9433 \t cumulative reward:  2.1\n","step: 630 \t epsilon: 0.9428 \t cumulative reward:  4.2\n","step: 635 \t epsilon: 0.9424 \t cumulative reward: -0.4\n","step: 640 \t epsilon: 0.9419 \t cumulative reward:  0.1\n","step: 645 \t epsilon: 0.9415 \t cumulative reward:  3.8\n","step: 650 \t epsilon: 0.9410 \t cumulative reward:  2.1\n","step: 655 \t epsilon: 0.9405 \t cumulative reward:  0.0\n","step: 660 \t epsilon: 0.9401 \t cumulative reward: -0.3\n","step: 665 \t epsilon: 0.9396 \t cumulative reward:  0.3\n","step: 670 \t epsilon: 0.9392 \t cumulative reward:  1.7\n","step: 675 \t epsilon: 0.9387 \t cumulative reward:  2.2\n","step: 680 \t epsilon: 0.9383 \t cumulative reward: -0.2\n","step: 685 \t epsilon: 0.9378 \t cumulative reward: -0.5\n","step: 690 \t epsilon: 0.9374 \t cumulative reward: -1.9\n","step: 695 \t epsilon: 0.9369 \t cumulative reward: -2.6\n","step: 700 \t epsilon: 0.9365 \t cumulative reward: -2.2\n","step: 705 \t epsilon: 0.9360 \t cumulative reward: -0.9\n","step: 710 \t epsilon: 0.9355 \t cumulative reward:  1.8\n","step: 715 \t epsilon: 0.9351 \t cumulative reward: -0.1\n","step: 720 \t epsilon: 0.9346 \t cumulative reward:  0.0\n","step: 725 \t epsilon: 0.9342 \t cumulative reward: -0.6\n","step: 730 \t epsilon: 0.9337 \t cumulative reward: -1.9\n","step: 735 \t epsilon: 0.9333 \t cumulative reward:  3.7\n","step: 740 \t epsilon: 0.9328 \t cumulative reward: -0.4\n","step: 745 \t epsilon: 0.9324 \t cumulative reward: -0.7\n","step: 750 \t epsilon: 0.9319 \t cumulative reward:  2.2\n","step: 755 \t epsilon: 0.9315 \t cumulative reward:  0.1\n","step: 760 \t epsilon: 0.9310 \t cumulative reward:  3.8\n","step: 765 \t epsilon: 0.9305 \t cumulative reward:  0.1\n","step: 770 \t epsilon: 0.9301 \t cumulative reward: -1.9\n","step: 775 \t epsilon: 0.9296 \t cumulative reward:  1.6\n","step: 780 \t epsilon: 0.9292 \t cumulative reward: -0.7\n","step: 785 \t epsilon: 0.9287 \t cumulative reward: -2.3\n","step: 790 \t epsilon: 0.9283 \t cumulative reward: -0.2\n","step: 795 \t epsilon: 0.9278 \t cumulative reward:  0.5\n","step: 800 \t epsilon: 0.9274 \t cumulative reward: -0.6\n","step: 805 \t epsilon: 0.9269 \t cumulative reward: -1.0\n","step: 810 \t epsilon: 0.9265 \t cumulative reward: -1.8\n","step: 815 \t epsilon: 0.9260 \t cumulative reward: -2.2\n","step: 820 \t epsilon: 0.9255 \t cumulative reward: -0.7\n","step: 825 \t epsilon: 0.9251 \t cumulative reward:  3.8\n","step: 830 \t epsilon: 0.9246 \t cumulative reward: -0.3\n","step: 835 \t epsilon: 0.9242 \t cumulative reward:  3.6\n","step: 840 \t epsilon: 0.9237 \t cumulative reward:  0.2\n","step: 845 \t epsilon: 0.9233 \t cumulative reward: -0.7\n","step: 850 \t epsilon: 0.9228 \t cumulative reward: -2.1\n","step: 855 \t epsilon: 0.9224 \t cumulative reward:  3.5\n","step: 860 \t epsilon: 0.9219 \t cumulative reward: -0.4\n","step: 865 \t epsilon: 0.9215 \t cumulative reward:  1.7\n","step: 870 \t epsilon: 0.9210 \t cumulative reward: -2.3\n","step: 875 \t epsilon: 0.9205 \t cumulative reward: -0.1\n","step: 880 \t epsilon: 0.9201 \t cumulative reward: -0.7\n","step: 885 \t epsilon: 0.9196 \t cumulative reward: -0.3\n","step: 890 \t epsilon: 0.9192 \t cumulative reward:  1.9\n","step: 895 \t epsilon: 0.9187 \t cumulative reward: -0.5\n","step: 900 \t epsilon: 0.9183 \t cumulative reward:  1.9\n","step: 905 \t epsilon: 0.9178 \t cumulative reward: -2.0\n","step: 910 \t epsilon: 0.9174 \t cumulative reward: -1.7\n","step: 915 \t epsilon: 0.9169 \t cumulative reward: -2.2\n","step: 920 \t epsilon: 0.9165 \t cumulative reward: -0.7\n","step: 925 \t epsilon: 0.9160 \t cumulative reward: -0.3\n","step: 930 \t epsilon: 0.9155 \t cumulative reward: -0.3\n","step: 935 \t epsilon: 0.9151 \t cumulative reward: -0.2\n","step: 940 \t epsilon: 0.9146 \t cumulative reward: -2.3\n","step: 945 \t epsilon: 0.9142 \t cumulative reward:  0.1\n","step: 950 \t epsilon: 0.9137 \t cumulative reward: -2.3\n","step: 955 \t epsilon: 0.9133 \t cumulative reward: -0.5\n","step: 960 \t epsilon: 0.9128 \t cumulative reward:  2.1\n","step: 965 \t epsilon: 0.9124 \t cumulative reward: -0.6\n","step: 970 \t epsilon: 0.9119 \t cumulative reward: -1.8\n","step: 975 \t epsilon: 0.9115 \t cumulative reward:  5.9\n","step: 980 \t epsilon: 0.9110 \t cumulative reward:  3.4\n","step: 985 \t epsilon: 0.9105 \t cumulative reward:  1.8\n","step: 990 \t epsilon: 0.9101 \t cumulative reward:  1.9\n","step: 995 \t epsilon: 0.9096 \t cumulative reward:  2.0\n","step: 1000 \t epsilon: 0.9092 \t cumulative reward: -0.2\n","step: 1005 \t epsilon: 0.9087 \t cumulative reward: -0.2\n","step: 1010 \t epsilon: 0.9083 \t cumulative reward:  0.1\n","step: 1015 \t epsilon: 0.9078 \t cumulative reward:  7.8\n","step: 1020 \t epsilon: 0.9074 \t cumulative reward:  1.5\n","step: 1025 \t epsilon: 0.9069 \t cumulative reward:  0.1\n","step: 1030 \t epsilon: 0.9065 \t cumulative reward: -0.3\n","step: 1035 \t epsilon: 0.9060 \t cumulative reward: -0.2\n","step: 1040 \t epsilon: 0.9055 \t cumulative reward:  6.0\n","step: 1045 \t epsilon: 0.9051 \t cumulative reward:  1.8\n","step: 1050 \t epsilon: 0.9046 \t cumulative reward: -0.1\n","step: 1055 \t epsilon: 0.9042 \t cumulative reward: -0.3\n","step: 1060 \t epsilon: 0.9037 \t cumulative reward:  1.9\n","step: 1065 \t epsilon: 0.9033 \t cumulative reward: -0.1\n","step: 1070 \t epsilon: 0.9028 \t cumulative reward:  4.1\n","step: 1075 \t epsilon: 0.9024 \t cumulative reward:  0.0\n","step: 1080 \t epsilon: 0.9019 \t cumulative reward:  4.0\n","step: 1085 \t epsilon: 0.9015 \t cumulative reward:  3.6\n","step: 1090 \t epsilon: 0.9010 \t cumulative reward: -0.6\n","step: 1095 \t epsilon: 0.9005 \t cumulative reward:  0.0\n","step: 1100 \t epsilon: 0.9001 \t cumulative reward:  1.7\n","step: 1105 \t epsilon: 0.8996 \t cumulative reward:  1.9\n","step: 1110 \t epsilon: 0.8992 \t cumulative reward: -2.4\n","step: 1115 \t epsilon: 0.8987 \t cumulative reward: -0.3\n","step: 1120 \t epsilon: 0.8983 \t cumulative reward:  5.9\n","step: 1125 \t epsilon: 0.8978 \t cumulative reward:  3.8\n","step: 1130 \t epsilon: 0.8974 \t cumulative reward:  1.8\n","step: 1135 \t epsilon: 0.8969 \t cumulative reward: -2.3\n","step: 1140 \t epsilon: 0.8965 \t cumulative reward:  0.1\n","step: 1145 \t epsilon: 0.8960 \t cumulative reward:  2.1\n","step: 1150 \t epsilon: 0.8955 \t cumulative reward:  3.7\n","step: 1155 \t epsilon: 0.8951 \t cumulative reward:  3.6\n","step: 1160 \t epsilon: 0.8946 \t cumulative reward:  3.9\n","step: 1165 \t epsilon: 0.8942 \t cumulative reward:  2.0\n","step: 1170 \t epsilon: 0.8937 \t cumulative reward: -0.6\n","step: 1175 \t epsilon: 0.8933 \t cumulative reward:  2.0\n","step: 1180 \t epsilon: 0.8928 \t cumulative reward: -0.1\n","step: 1185 \t epsilon: 0.8924 \t cumulative reward:  0.0\n","step: 1190 \t epsilon: 0.8919 \t cumulative reward:  3.7\n","step: 1195 \t epsilon: 0.8915 \t cumulative reward:  3.8\n","step: 1200 \t epsilon: 0.8910 \t cumulative reward: -0.9\n","step: 1205 \t epsilon: 0.8905 \t cumulative reward:  0.3\n","step: 1210 \t epsilon: 0.8901 \t cumulative reward:  2.0\n","step: 1215 \t epsilon: 0.8896 \t cumulative reward:  0.1\n","step: 1220 \t epsilon: 0.8892 \t cumulative reward:  0.3\n","step: 1225 \t epsilon: 0.8887 \t cumulative reward:  2.0\n","step: 1230 \t epsilon: 0.8883 \t cumulative reward:  1.4\n","step: 1235 \t epsilon: 0.8878 \t cumulative reward: -0.8\n","step: 1240 \t epsilon: 0.8874 \t cumulative reward:  1.9\n","step: 1245 \t epsilon: 0.8869 \t cumulative reward: -0.3\n","step: 1250 \t epsilon: 0.8865 \t cumulative reward: -0.6\n","step: 1255 \t epsilon: 0.8860 \t cumulative reward: -2.0\n","step: 1260 \t epsilon: 0.8855 \t cumulative reward:  3.6\n","step: 1265 \t epsilon: 0.8851 \t cumulative reward:  0.5\n","step: 1270 \t epsilon: 0.8846 \t cumulative reward: -0.7\n","step: 1275 \t epsilon: 0.8842 \t cumulative reward: -0.3\n","step: 1280 \t epsilon: 0.8837 \t cumulative reward:  4.0\n","step: 1285 \t epsilon: 0.8833 \t cumulative reward: -0.7\n","step: 1290 \t epsilon: 0.8828 \t cumulative reward:  3.9\n","step: 1295 \t epsilon: 0.8824 \t cumulative reward: -1.9\n","step: 1300 \t epsilon: 0.8819 \t cumulative reward: -2.0\n","step: 1305 \t epsilon: 0.8815 \t cumulative reward: -0.4\n","step: 1310 \t epsilon: 0.8810 \t cumulative reward:  4.0\n","step: 1315 \t epsilon: 0.8805 \t cumulative reward:  2.1\n","step: 1320 \t epsilon: 0.8801 \t cumulative reward:  4.1\n","step: 1325 \t epsilon: 0.8796 \t cumulative reward:  0.1\n","step: 1330 \t epsilon: 0.8792 \t cumulative reward: -0.3\n","step: 1335 \t epsilon: 0.8787 \t cumulative reward:  3.5\n","step: 1340 \t epsilon: 0.8783 \t cumulative reward: -0.5\n","step: 1345 \t epsilon: 0.8778 \t cumulative reward:  3.7\n","step: 1350 \t epsilon: 0.8774 \t cumulative reward: -0.2\n","step: 1355 \t epsilon: 0.8769 \t cumulative reward:  1.9\n","step: 1360 \t epsilon: 0.8765 \t cumulative reward:  1.8\n","step: 1365 \t epsilon: 0.8760 \t cumulative reward:  4.1\n","step: 1370 \t epsilon: 0.8755 \t cumulative reward: -0.1\n","step: 1375 \t epsilon: 0.8751 \t cumulative reward: -0.1\n","step: 1380 \t epsilon: 0.8746 \t cumulative reward: -0.3\n","step: 1385 \t epsilon: 0.8742 \t cumulative reward: -0.8\n","step: 1390 \t epsilon: 0.8737 \t cumulative reward: -0.2\n","step: 1395 \t epsilon: 0.8733 \t cumulative reward:  0.0\n","step: 1400 \t epsilon: 0.8728 \t cumulative reward: -2.4\n","step: 1405 \t epsilon: 0.8724 \t cumulative reward:  3.8\n","step: 1410 \t epsilon: 0.8719 \t cumulative reward:  7.8\n","step: 1415 \t epsilon: 0.8715 \t cumulative reward:  4.0\n","step: 1420 \t epsilon: 0.8710 \t cumulative reward:  1.5\n","step: 1425 \t epsilon: 0.8705 \t cumulative reward: -2.5\n","step: 1430 \t epsilon: 0.8701 \t cumulative reward: -1.8\n","step: 1435 \t epsilon: 0.8696 \t cumulative reward:  2.0\n","step: 1440 \t epsilon: 0.8692 \t cumulative reward: -0.8\n","step: 1445 \t epsilon: 0.8687 \t cumulative reward: -2.1\n","step: 1450 \t epsilon: 0.8683 \t cumulative reward: -2.4\n","step: 1455 \t epsilon: 0.8678 \t cumulative reward:  1.6\n","step: 1460 \t epsilon: 0.8674 \t cumulative reward: -0.7\n","step: 1465 \t epsilon: 0.8669 \t cumulative reward: -2.3\n","step: 1470 \t epsilon: 0.8665 \t cumulative reward:  1.9\n","step: 1475 \t epsilon: 0.8660 \t cumulative reward: -2.1\n","step: 1480 \t epsilon: 0.8655 \t cumulative reward:  0.1\n","step: 1485 \t epsilon: 0.8651 \t cumulative reward:  8.0\n","step: 1490 \t epsilon: 0.8646 \t cumulative reward: -0.2\n","step: 1495 \t epsilon: 0.8642 \t cumulative reward: -0.3\n","step: 1500 \t epsilon: 0.8637 \t cumulative reward: -0.2\n","step: 1505 \t epsilon: 0.8633 \t cumulative reward:  0.2\n","step: 1510 \t epsilon: 0.8628 \t cumulative reward: -0.2\n","step: 1515 \t epsilon: 0.8624 \t cumulative reward: -0.2\n","step: 1520 \t epsilon: 0.8619 \t cumulative reward:  3.7\n","step: 1525 \t epsilon: 0.8615 \t cumulative reward:  0.0\n","step: 1530 \t epsilon: 0.8610 \t cumulative reward:  3.6\n","step: 1535 \t epsilon: 0.8605 \t cumulative reward:  3.8\n","step: 1540 \t epsilon: 0.8601 \t cumulative reward:  4.2\n","step: 1545 \t epsilon: 0.8596 \t cumulative reward:  3.8\n","step: 1550 \t epsilon: 0.8592 \t cumulative reward:  2.3\n","step: 1555 \t epsilon: 0.8587 \t cumulative reward:  3.9\n","step: 1560 \t epsilon: 0.8583 \t cumulative reward: -0.2\n","step: 1565 \t epsilon: 0.8578 \t cumulative reward:  3.7\n","step: 1570 \t epsilon: 0.8574 \t cumulative reward: -0.6\n","step: 1575 \t epsilon: 0.8569 \t cumulative reward: -2.0\n","step: 1580 \t epsilon: 0.8565 \t cumulative reward:  4.0\n","step: 1585 \t epsilon: 0.8560 \t cumulative reward:  0.1\n","step: 1590 \t epsilon: 0.8555 \t cumulative reward:  3.5\n","step: 1595 \t epsilon: 0.8551 \t cumulative reward:  8.1\n","step: 1600 \t epsilon: 0.8546 \t cumulative reward: -0.3\n","step: 1605 \t epsilon: 0.8542 \t cumulative reward: -0.8\n","step: 1610 \t epsilon: 0.8537 \t cumulative reward:  1.9\n","step: 1615 \t epsilon: 0.8533 \t cumulative reward:  1.6\n","step: 1620 \t epsilon: 0.8528 \t cumulative reward:  3.8\n","step: 1625 \t epsilon: 0.8524 \t cumulative reward:  1.7\n","step: 1630 \t epsilon: 0.8519 \t cumulative reward:  2.0\n","step: 1635 \t epsilon: 0.8515 \t cumulative reward:  1.9\n","step: 1640 \t epsilon: 0.8510 \t cumulative reward:  1.9\n","step: 1645 \t epsilon: 0.8505 \t cumulative reward:  3.4\n","step: 1650 \t epsilon: 0.8501 \t cumulative reward: -0.1\n","step: 1655 \t epsilon: 0.8496 \t cumulative reward: -0.1\n","step: 1660 \t epsilon: 0.8492 \t cumulative reward:  1.8\n","step: 1665 \t epsilon: 0.8487 \t cumulative reward: -0.2\n","step: 1670 \t epsilon: 0.8483 \t cumulative reward:  1.9\n","step: 1675 \t epsilon: 0.8478 \t cumulative reward:  0.1\n","step: 1680 \t epsilon: 0.8474 \t cumulative reward:  3.7\n","step: 1685 \t epsilon: 0.8469 \t cumulative reward:  3.6\n","step: 1690 \t epsilon: 0.8465 \t cumulative reward:  3.8\n","step: 1695 \t epsilon: 0.8460 \t cumulative reward: -0.4\n","step: 1700 \t epsilon: 0.8455 \t cumulative reward:  4.2\n","step: 1705 \t epsilon: 0.8451 \t cumulative reward:  4.3\n","step: 1710 \t epsilon: 0.8446 \t cumulative reward:  0.3\n","step: 1715 \t epsilon: 0.8442 \t cumulative reward: -0.2\n","step: 1720 \t epsilon: 0.8437 \t cumulative reward:  1.7\n","step: 1725 \t epsilon: 0.8433 \t cumulative reward:  1.8\n","step: 1730 \t epsilon: 0.8428 \t cumulative reward:  3.7\n","step: 1735 \t epsilon: 0.8424 \t cumulative reward: -0.4\n","step: 1740 \t epsilon: 0.8419 \t cumulative reward:  4.2\n","step: 1745 \t epsilon: 0.8415 \t cumulative reward:  1.9\n","step: 1750 \t epsilon: 0.8410 \t cumulative reward:  3.9\n","step: 1755 \t epsilon: 0.8405 \t cumulative reward:  4.0\n","step: 1760 \t epsilon: 0.8401 \t cumulative reward: -0.5\n","step: 1765 \t epsilon: 0.8396 \t cumulative reward: -2.0\n","step: 1770 \t epsilon: 0.8392 \t cumulative reward:  3.9\n","step: 1775 \t epsilon: 0.8387 \t cumulative reward:  4.1\n","step: 1780 \t epsilon: 0.8383 \t cumulative reward:  2.1\n","step: 1785 \t epsilon: 0.8378 \t cumulative reward:  3.9\n","step: 1790 \t epsilon: 0.8374 \t cumulative reward: -0.5\n","step: 1795 \t epsilon: 0.8369 \t cumulative reward:  1.8\n","step: 1800 \t epsilon: 0.8365 \t cumulative reward: -2.1\n","step: 1805 \t epsilon: 0.8360 \t cumulative reward:  1.9\n","step: 1810 \t epsilon: 0.8355 \t cumulative reward: -0.3\n","step: 1815 \t epsilon: 0.8351 \t cumulative reward: -0.1\n","step: 1820 \t epsilon: 0.8346 \t cumulative reward: -0.1\n","step: 1825 \t epsilon: 0.8342 \t cumulative reward: -0.3\n","step: 1830 \t epsilon: 0.8337 \t cumulative reward: -2.2\n","step: 1835 \t epsilon: 0.8333 \t cumulative reward: -0.1\n","step: 1840 \t epsilon: 0.8328 \t cumulative reward: -2.1\n","step: 1845 \t epsilon: 0.8324 \t cumulative reward: -0.2\n","step: 1850 \t epsilon: 0.8319 \t cumulative reward:  4.1\n","step: 1855 \t epsilon: 0.8315 \t cumulative reward:  2.1\n","step: 1860 \t epsilon: 0.8310 \t cumulative reward: -1.8\n","step: 1865 \t epsilon: 0.8305 \t cumulative reward:  1.8\n","step: 1870 \t epsilon: 0.8301 \t cumulative reward:  0.3\n","step: 1875 \t epsilon: 0.8296 \t cumulative reward: -0.2\n","step: 1880 \t epsilon: 0.8292 \t cumulative reward:  3.5\n","step: 1885 \t epsilon: 0.8287 \t cumulative reward:  4.2\n","step: 1890 \t epsilon: 0.8283 \t cumulative reward: -0.7\n","step: 1895 \t epsilon: 0.8278 \t cumulative reward: -0.4\n","step: 1900 \t epsilon: 0.8274 \t cumulative reward:  3.9\n","step: 1905 \t epsilon: 0.8269 \t cumulative reward:  3.9\n","step: 1910 \t epsilon: 0.8265 \t cumulative reward:  1.9\n","step: 1915 \t epsilon: 0.8260 \t cumulative reward:  0.1\n","step: 1920 \t epsilon: 0.8255 \t cumulative reward:  4.2\n","step: 1925 \t epsilon: 0.8251 \t cumulative reward:  3.9\n","step: 1930 \t epsilon: 0.8246 \t cumulative reward:  6.0\n","step: 1935 \t epsilon: 0.8242 \t cumulative reward: -0.8\n","step: 1940 \t epsilon: 0.8237 \t cumulative reward:  3.4\n","step: 1945 \t epsilon: 0.8233 \t cumulative reward:  0.3\n","step: 1950 \t epsilon: 0.8228 \t cumulative reward: -1.7\n","step: 1955 \t epsilon: 0.8224 \t cumulative reward:  0.3\n","step: 1960 \t epsilon: 0.8219 \t cumulative reward:  1.5\n","step: 1965 \t epsilon: 0.8215 \t cumulative reward:  0.0\n","step: 1970 \t epsilon: 0.8210 \t cumulative reward: -0.6\n","step: 1975 \t epsilon: 0.8205 \t cumulative reward: -0.4\n","step: 1980 \t epsilon: 0.8201 \t cumulative reward:  8.1\n","step: 1985 \t epsilon: 0.8196 \t cumulative reward: -0.4\n","step: 1990 \t epsilon: 0.8192 \t cumulative reward: -0.2\n","step: 1995 \t epsilon: 0.8187 \t cumulative reward: -0.3\n","step: 2000 \t epsilon: 0.8183 \t cumulative reward: -0.6\n","step: 2005 \t epsilon: 0.8178 \t cumulative reward:  3.8\n","step: 2010 \t epsilon: 0.8174 \t cumulative reward:  3.9\n","step: 2015 \t epsilon: 0.8169 \t cumulative reward:  4.1\n","step: 2020 \t epsilon: 0.8165 \t cumulative reward:  3.9\n","step: 2025 \t epsilon: 0.8160 \t cumulative reward: -2.5\n","step: 2030 \t epsilon: 0.8155 \t cumulative reward:  3.8\n","step: 2035 \t epsilon: 0.8151 \t cumulative reward:  2.2\n","step: 2040 \t epsilon: 0.8146 \t cumulative reward:  3.9\n","step: 2045 \t epsilon: 0.8142 \t cumulative reward:  5.8\n","step: 2050 \t epsilon: 0.8137 \t cumulative reward:  3.6\n","step: 2055 \t epsilon: 0.8133 \t cumulative reward:  3.6\n","step: 2060 \t epsilon: 0.8128 \t cumulative reward: -0.6\n","step: 2065 \t epsilon: 0.8124 \t cumulative reward:  0.4\n","step: 2070 \t epsilon: 0.8119 \t cumulative reward:  3.6\n","step: 2075 \t epsilon: 0.8115 \t cumulative reward:  1.5\n","step: 2080 \t epsilon: 0.8110 \t cumulative reward:  1.7\n","step: 2085 \t epsilon: 0.8105 \t cumulative reward:  2.1\n","step: 2090 \t epsilon: 0.8101 \t cumulative reward: -2.1\n","step: 2095 \t epsilon: 0.8096 \t cumulative reward:  3.7\n","step: 2100 \t epsilon: 0.8092 \t cumulative reward: -0.6\n","step: 2105 \t epsilon: 0.8087 \t cumulative reward:  3.8\n","step: 2110 \t epsilon: 0.8083 \t cumulative reward:  8.0\n","step: 2115 \t epsilon: 0.8078 \t cumulative reward:  3.6\n","step: 2120 \t epsilon: 0.8074 \t cumulative reward:  0.0\n","step: 2125 \t epsilon: 0.8069 \t cumulative reward: -0.6\n","step: 2130 \t epsilon: 0.8065 \t cumulative reward:  4.1\n","step: 2135 \t epsilon: 0.8060 \t cumulative reward: -0.1\n","step: 2140 \t epsilon: 0.8055 \t cumulative reward: -2.2\n","step: 2145 \t epsilon: 0.8051 \t cumulative reward: -1.8\n","step: 2150 \t epsilon: 0.8046 \t cumulative reward:  1.9\n","step: 2155 \t epsilon: 0.8042 \t cumulative reward:  0.1\n","step: 2160 \t epsilon: 0.8037 \t cumulative reward:  1.7\n","step: 2165 \t epsilon: 0.8033 \t cumulative reward:  4.0\n","step: 2170 \t epsilon: 0.8028 \t cumulative reward:  8.0\n","step: 2175 \t epsilon: 0.8024 \t cumulative reward:  1.9\n","step: 2180 \t epsilon: 0.8019 \t cumulative reward: -2.6\n","step: 2185 \t epsilon: 0.8015 \t cumulative reward: -0.4\n","step: 2190 \t epsilon: 0.8010 \t cumulative reward:  1.6\n","step: 2195 \t epsilon: 0.8005 \t cumulative reward:  0.1\n","step: 2200 \t epsilon: 0.8001 \t cumulative reward:  0.1\n","step: 2205 \t epsilon: 0.7996 \t cumulative reward:  2.0\n","step: 2210 \t epsilon: 0.7992 \t cumulative reward:  2.0\n","step: 2215 \t epsilon: 0.7987 \t cumulative reward: -0.7\n","step: 2220 \t epsilon: 0.7983 \t cumulative reward:  0.0\n","step: 2225 \t epsilon: 0.7978 \t cumulative reward:  3.5\n","step: 2230 \t epsilon: 0.7974 \t cumulative reward:  4.1\n","step: 2235 \t epsilon: 0.7969 \t cumulative reward:  3.9\n","step: 2240 \t epsilon: 0.7965 \t cumulative reward:  3.7\n","step: 2245 \t epsilon: 0.7960 \t cumulative reward:  0.1\n","step: 2250 \t epsilon: 0.7955 \t cumulative reward:  2.1\n","step: 2255 \t epsilon: 0.7951 \t cumulative reward:  2.1\n","step: 2260 \t epsilon: 0.7946 \t cumulative reward: -0.2\n","step: 2265 \t epsilon: 0.7942 \t cumulative reward:  8.0\n","step: 2270 \t epsilon: 0.7937 \t cumulative reward:  1.8\n","step: 2275 \t epsilon: 0.7933 \t cumulative reward: -3.9\n","step: 2280 \t epsilon: 0.7928 \t cumulative reward:  0.0\n","step: 2285 \t epsilon: 0.7924 \t cumulative reward:  3.9\n","step: 2290 \t epsilon: 0.7919 \t cumulative reward: -1.7\n","step: 2295 \t epsilon: 0.7915 \t cumulative reward:  3.5\n","step: 2300 \t epsilon: 0.7910 \t cumulative reward:  2.3\n","step: 2305 \t epsilon: 0.7905 \t cumulative reward:  0.1\n","step: 2310 \t epsilon: 0.7901 \t cumulative reward:  1.5\n","step: 2315 \t epsilon: 0.7896 \t cumulative reward:  0.1\n","step: 2320 \t epsilon: 0.7892 \t cumulative reward:  2.3\n","step: 2325 \t epsilon: 0.7887 \t cumulative reward: -1.8\n","step: 2330 \t epsilon: 0.7883 \t cumulative reward:  4.0\n","step: 2335 \t epsilon: 0.7878 \t cumulative reward:  4.0\n","step: 2340 \t epsilon: 0.7874 \t cumulative reward: -1.9\n","step: 2345 \t epsilon: 0.7869 \t cumulative reward: -1.8\n","step: 2350 \t epsilon: 0.7865 \t cumulative reward:  3.9\n","step: 2355 \t epsilon: 0.7860 \t cumulative reward:  3.4\n","step: 2360 \t epsilon: 0.7855 \t cumulative reward:  7.8\n","step: 2365 \t epsilon: 0.7851 \t cumulative reward:  0.1\n","step: 2370 \t epsilon: 0.7846 \t cumulative reward: -0.3\n","step: 2375 \t epsilon: 0.7842 \t cumulative reward:  3.7\n","step: 2380 \t epsilon: 0.7837 \t cumulative reward: -2.0\n","step: 2385 \t epsilon: 0.7833 \t cumulative reward: -1.9\n","step: 2390 \t epsilon: 0.7828 \t cumulative reward:  1.8\n","step: 2395 \t epsilon: 0.7824 \t cumulative reward: -0.2\n","step: 2400 \t epsilon: 0.7819 \t cumulative reward: -2.3\n","step: 2405 \t epsilon: 0.7815 \t cumulative reward:  3.7\n","step: 2410 \t epsilon: 0.7810 \t cumulative reward: -0.6\n","step: 2415 \t epsilon: 0.7805 \t cumulative reward:  8.0\n","step: 2420 \t epsilon: 0.7801 \t cumulative reward:  3.8\n","step: 2425 \t epsilon: 0.7796 \t cumulative reward: -2.4\n","step: 2430 \t epsilon: 0.7792 \t cumulative reward:  2.1\n","step: 2435 \t epsilon: 0.7787 \t cumulative reward:  3.7\n","step: 2440 \t epsilon: 0.7783 \t cumulative reward:  1.9\n","step: 2445 \t epsilon: 0.7778 \t cumulative reward: -2.2\n","step: 2450 \t epsilon: 0.7774 \t cumulative reward:  4.1\n","step: 2455 \t epsilon: 0.7769 \t cumulative reward:  3.4\n","step: 2460 \t epsilon: 0.7765 \t cumulative reward:  2.0\n","step: 2465 \t epsilon: 0.7760 \t cumulative reward:  0.0\n","step: 2470 \t epsilon: 0.7755 \t cumulative reward: -1.8\n","step: 2475 \t epsilon: 0.7751 \t cumulative reward:  8.0\n","step: 2480 \t epsilon: 0.7746 \t cumulative reward: -1.9\n","step: 2485 \t epsilon: 0.7742 \t cumulative reward:  3.6\n","step: 2490 \t epsilon: 0.7737 \t cumulative reward:  1.8\n","step: 2495 \t epsilon: 0.7733 \t cumulative reward:  4.0\n","step: 2500 \t epsilon: 0.7728 \t cumulative reward:  2.0\n","step: 2505 \t epsilon: 0.7724 \t cumulative reward:  3.8\n","step: 2510 \t epsilon: 0.7719 \t cumulative reward: -2.1\n","step: 2515 \t epsilon: 0.7715 \t cumulative reward: -0.7\n","step: 2520 \t epsilon: 0.7710 \t cumulative reward:  4.2\n","step: 2525 \t epsilon: 0.7705 \t cumulative reward: -2.3\n","step: 2530 \t epsilon: 0.7701 \t cumulative reward:  6.1\n","step: 2535 \t epsilon: 0.7696 \t cumulative reward:  3.9\n","step: 2540 \t epsilon: 0.7692 \t cumulative reward:  1.9\n","step: 2545 \t epsilon: 0.7687 \t cumulative reward: -0.3\n","step: 2550 \t epsilon: 0.7683 \t cumulative reward:  4.0\n","step: 2555 \t epsilon: 0.7678 \t cumulative reward: -1.9\n","step: 2560 \t epsilon: 0.7674 \t cumulative reward: -2.1\n","step: 2565 \t epsilon: 0.7669 \t cumulative reward:  2.1\n","step: 2570 \t epsilon: 0.7665 \t cumulative reward:  3.7\n","step: 2575 \t epsilon: 0.7660 \t cumulative reward:  3.8\n","step: 2580 \t epsilon: 0.7655 \t cumulative reward:  5.9\n","step: 2585 \t epsilon: 0.7651 \t cumulative reward:  3.8\n","step: 2590 \t epsilon: 0.7646 \t cumulative reward:  3.5\n","step: 2595 \t epsilon: 0.7642 \t cumulative reward:  4.3\n","step: 2600 \t epsilon: 0.7637 \t cumulative reward: -2.1\n","step: 2605 \t epsilon: 0.7633 \t cumulative reward:  6.1\n","step: 2610 \t epsilon: 0.7628 \t cumulative reward:  4.0\n","step: 2615 \t epsilon: 0.7624 \t cumulative reward:  3.8\n","step: 2620 \t epsilon: 0.7619 \t cumulative reward: -2.2\n","step: 2625 \t epsilon: 0.7615 \t cumulative reward: -1.9\n","step: 2630 \t epsilon: 0.7610 \t cumulative reward:  4.2\n","step: 2635 \t epsilon: 0.7605 \t cumulative reward:  0.1\n","step: 2640 \t epsilon: 0.7601 \t cumulative reward:  3.8\n","step: 2645 \t epsilon: 0.7596 \t cumulative reward:  1.9\n","step: 2650 \t epsilon: 0.7592 \t cumulative reward:  4.2\n","step: 2655 \t epsilon: 0.7587 \t cumulative reward:  3.9\n","step: 2660 \t epsilon: 0.7583 \t cumulative reward:  0.0\n","step: 2665 \t epsilon: 0.7578 \t cumulative reward: -0.3\n","step: 2670 \t epsilon: 0.7574 \t cumulative reward:  7.9\n","step: 2675 \t epsilon: 0.7569 \t cumulative reward:  3.8\n","step: 2680 \t epsilon: 0.7565 \t cumulative reward:  3.9\n","step: 2685 \t epsilon: 0.7560 \t cumulative reward:  4.1\n","step: 2690 \t epsilon: 0.7555 \t cumulative reward:  3.8\n","step: 2695 \t epsilon: 0.7551 \t cumulative reward: -0.1\n","step: 2700 \t epsilon: 0.7546 \t cumulative reward:  8.1\n","step: 2705 \t epsilon: 0.7542 \t cumulative reward:  0.1\n","step: 2710 \t epsilon: 0.7537 \t cumulative reward:  2.0\n","step: 2715 \t epsilon: 0.7533 \t cumulative reward:  4.0\n","step: 2720 \t epsilon: 0.7528 \t cumulative reward: -0.5\n","step: 2725 \t epsilon: 0.7524 \t cumulative reward: -1.7\n","step: 2730 \t epsilon: 0.7519 \t cumulative reward:  6.1\n","step: 2735 \t epsilon: 0.7515 \t cumulative reward: -1.7\n","step: 2740 \t epsilon: 0.7510 \t cumulative reward:  1.6\n","step: 2745 \t epsilon: 0.7505 \t cumulative reward:  4.3\n","step: 2750 \t epsilon: 0.7501 \t cumulative reward:  6.0\n","step: 2755 \t epsilon: 0.7496 \t cumulative reward:  3.9\n","step: 2760 \t epsilon: 0.7492 \t cumulative reward: -2.2\n","step: 2765 \t epsilon: 0.7487 \t cumulative reward: -1.7\n","step: 2770 \t epsilon: 0.7483 \t cumulative reward:  4.1\n","step: 2775 \t epsilon: 0.7478 \t cumulative reward:  3.4\n","step: 2780 \t epsilon: 0.7474 \t cumulative reward:  3.8\n","step: 2785 \t epsilon: 0.7469 \t cumulative reward:  3.7\n","step: 2790 \t epsilon: 0.7465 \t cumulative reward:  4.1\n","step: 2795 \t epsilon: 0.7460 \t cumulative reward:  2.1\n","step: 2800 \t epsilon: 0.7455 \t cumulative reward:  4.0\n","step: 2805 \t epsilon: 0.7451 \t cumulative reward:  0.1\n","step: 2810 \t epsilon: 0.7446 \t cumulative reward:  3.9\n","step: 2815 \t epsilon: 0.7442 \t cumulative reward:  8.0\n","step: 2820 \t epsilon: 0.7437 \t cumulative reward:  0.3\n","step: 2825 \t epsilon: 0.7433 \t cumulative reward: -3.9\n","step: 2830 \t epsilon: 0.7428 \t cumulative reward:  4.0\n","step: 2835 \t epsilon: 0.7424 \t cumulative reward: -0.1\n","step: 2840 \t epsilon: 0.7419 \t cumulative reward: -2.3\n","step: 2845 \t epsilon: 0.7415 \t cumulative reward:  4.0\n","step: 2850 \t epsilon: 0.7410 \t cumulative reward:  0.2\n","step: 2855 \t epsilon: 0.7405 \t cumulative reward:  3.8\n","step: 2860 \t epsilon: 0.7401 \t cumulative reward: -0.1\n","step: 2865 \t epsilon: 0.7396 \t cumulative reward:  5.9\n","step: 2870 \t epsilon: 0.7392 \t cumulative reward:  4.0\n","step: 2875 \t epsilon: 0.7387 \t cumulative reward:  3.9\n","step: 2880 \t epsilon: 0.7383 \t cumulative reward:  0.1\n","step: 2885 \t epsilon: 0.7378 \t cumulative reward: -0.5\n","step: 2890 \t epsilon: 0.7374 \t cumulative reward:  3.9\n","step: 2895 \t epsilon: 0.7369 \t cumulative reward: -0.2\n","step: 2900 \t epsilon: 0.7365 \t cumulative reward:  5.8\n","step: 2905 \t epsilon: 0.7360 \t cumulative reward: -1.9\n","step: 2910 \t epsilon: 0.7355 \t cumulative reward:  3.4\n","step: 2915 \t epsilon: 0.7351 \t cumulative reward:  4.3\n","step: 2920 \t epsilon: 0.7346 \t cumulative reward:  1.9\n","step: 2925 \t epsilon: 0.7342 \t cumulative reward:  3.9\n","step: 2930 \t epsilon: 0.7337 \t cumulative reward:  1.9\n","step: 2935 \t epsilon: 0.7333 \t cumulative reward:  2.1\n","step: 2940 \t epsilon: 0.7328 \t cumulative reward:  0.0\n","step: 2945 \t epsilon: 0.7324 \t cumulative reward:  4.1\n","step: 2950 \t epsilon: 0.7319 \t cumulative reward: -0.1\n","step: 2955 \t epsilon: 0.7315 \t cumulative reward:  4.3\n","step: 2960 \t epsilon: 0.7310 \t cumulative reward: -0.7\n","step: 2965 \t epsilon: 0.7305 \t cumulative reward:  3.8\n","step: 2970 \t epsilon: 0.7301 \t cumulative reward: -1.0\n","step: 2975 \t epsilon: 0.7296 \t cumulative reward: -1.7\n","step: 2980 \t epsilon: 0.7292 \t cumulative reward: -0.7\n","step: 2985 \t epsilon: 0.7287 \t cumulative reward:  3.9\n","step: 2990 \t epsilon: 0.7283 \t cumulative reward: -2.2\n","step: 2995 \t epsilon: 0.7278 \t cumulative reward:  2.1\n","step: 3000 \t epsilon: 0.7274 \t cumulative reward:  4.1\n","step: 3005 \t epsilon: 0.7269 \t cumulative reward:  4.0\n","step: 3010 \t epsilon: 0.7265 \t cumulative reward:  3.8\n","step: 3015 \t epsilon: 0.7260 \t cumulative reward:  1.6\n","step: 3020 \t epsilon: 0.7255 \t cumulative reward:  7.8\n","step: 3025 \t epsilon: 0.7251 \t cumulative reward:  1.7\n","step: 3030 \t epsilon: 0.7246 \t cumulative reward:  0.3\n","step: 3035 \t epsilon: 0.7242 \t cumulative reward:  0.1\n","step: 3040 \t epsilon: 0.7237 \t cumulative reward:  7.9\n","step: 3045 \t epsilon: 0.7233 \t cumulative reward: -2.0\n","step: 3050 \t epsilon: 0.7228 \t cumulative reward:  0.1\n","step: 3055 \t epsilon: 0.7224 \t cumulative reward:  8.0\n","step: 3060 \t epsilon: 0.7219 \t cumulative reward:  3.8\n","step: 3065 \t epsilon: 0.7215 \t cumulative reward:  1.7\n","step: 3070 \t epsilon: 0.7210 \t cumulative reward:  4.1\n","step: 3075 \t epsilon: 0.7205 \t cumulative reward:  3.6\n","step: 3080 \t epsilon: 0.7201 \t cumulative reward:  3.7\n","step: 3085 \t epsilon: 0.7196 \t cumulative reward: -2.0\n","step: 3090 \t epsilon: 0.7192 \t cumulative reward:  1.8\n","step: 3095 \t epsilon: 0.7187 \t cumulative reward:  2.0\n","step: 3100 \t epsilon: 0.7183 \t cumulative reward:  1.8\n","step: 3105 \t epsilon: 0.7178 \t cumulative reward:  3.8\n","step: 3110 \t epsilon: 0.7174 \t cumulative reward:  8.0\n","step: 3115 \t epsilon: 0.7169 \t cumulative reward:  7.9\n","step: 3120 \t epsilon: 0.7165 \t cumulative reward:  2.0\n","step: 3125 \t epsilon: 0.7160 \t cumulative reward:  3.7\n","step: 3130 \t epsilon: 0.7155 \t cumulative reward:  4.0\n","step: 3135 \t epsilon: 0.7151 \t cumulative reward:  2.1\n","step: 3140 \t epsilon: 0.7146 \t cumulative reward:  3.9\n","step: 3145 \t epsilon: 0.7142 \t cumulative reward:  1.9\n","step: 3150 \t epsilon: 0.7137 \t cumulative reward:  3.9\n","step: 3155 \t epsilon: 0.7133 \t cumulative reward:  1.9\n","step: 3160 \t epsilon: 0.7128 \t cumulative reward:  7.8\n","step: 3165 \t epsilon: 0.7124 \t cumulative reward:  3.8\n","step: 3170 \t epsilon: 0.7119 \t cumulative reward:  3.7\n","step: 3175 \t epsilon: 0.7115 \t cumulative reward:  6.0\n","step: 3180 \t epsilon: 0.7110 \t cumulative reward:  3.8\n","step: 3185 \t epsilon: 0.7105 \t cumulative reward:  0.2\n","step: 3190 \t epsilon: 0.7101 \t cumulative reward:  6.0\n","step: 3195 \t epsilon: 0.7096 \t cumulative reward:  3.8\n","step: 3200 \t epsilon: 0.7092 \t cumulative reward:  4.1\n","step: 3205 \t epsilon: 0.7087 \t cumulative reward:  3.5\n","step: 3210 \t epsilon: 0.7083 \t cumulative reward: -1.9\n","step: 3215 \t epsilon: 0.7078 \t cumulative reward:  6.1\n","step: 3220 \t epsilon: 0.7074 \t cumulative reward:  3.9\n","step: 3225 \t epsilon: 0.7069 \t cumulative reward:  2.1\n","step: 3230 \t epsilon: 0.7065 \t cumulative reward: -0.4\n","step: 3235 \t epsilon: 0.7060 \t cumulative reward: -0.8\n","step: 3240 \t epsilon: 0.7055 \t cumulative reward:  4.0\n","step: 3245 \t epsilon: 0.7051 \t cumulative reward: -2.0\n","step: 3250 \t epsilon: 0.7046 \t cumulative reward:  0.1\n","step: 3255 \t epsilon: 0.7042 \t cumulative reward: -2.3\n","step: 3260 \t epsilon: 0.7037 \t cumulative reward:  5.9\n","step: 3265 \t epsilon: 0.7033 \t cumulative reward:  3.7\n","step: 3270 \t epsilon: 0.7028 \t cumulative reward:  3.6\n","step: 3275 \t epsilon: 0.7024 \t cumulative reward: -1.9\n","step: 3280 \t epsilon: 0.7019 \t cumulative reward: -2.2\n","step: 3285 \t epsilon: 0.7015 \t cumulative reward:  8.1\n","step: 3290 \t epsilon: 0.7010 \t cumulative reward:  3.8\n","step: 3295 \t epsilon: 0.7005 \t cumulative reward:  3.9\n","step: 3300 \t epsilon: 0.7001 \t cumulative reward:  4.1\n","step: 3305 \t epsilon: 0.6996 \t cumulative reward:  3.8\n","step: 3310 \t epsilon: 0.6992 \t cumulative reward: -0.2\n","step: 3315 \t epsilon: 0.6987 \t cumulative reward:  1.9\n","step: 3320 \t epsilon: 0.6983 \t cumulative reward:  1.7\n","step: 3325 \t epsilon: 0.6978 \t cumulative reward:  6.0\n","step: 3330 \t epsilon: 0.6974 \t cumulative reward:  4.2\n","step: 3335 \t epsilon: 0.6969 \t cumulative reward:  3.6\n","step: 3340 \t epsilon: 0.6965 \t cumulative reward:  8.0\n","step: 3345 \t epsilon: 0.6960 \t cumulative reward:  3.7\n","step: 3350 \t epsilon: 0.6955 \t cumulative reward: -0.3\n","step: 3355 \t epsilon: 0.6951 \t cumulative reward:  4.0\n","step: 3360 \t epsilon: 0.6946 \t cumulative reward:  4.0\n","step: 3365 \t epsilon: 0.6942 \t cumulative reward:  1.8\n","step: 3370 \t epsilon: 0.6937 \t cumulative reward:  8.0\n","step: 3375 \t epsilon: 0.6933 \t cumulative reward:  3.7\n","step: 3380 \t epsilon: 0.6928 \t cumulative reward:  6.1\n","step: 3385 \t epsilon: 0.6924 \t cumulative reward:  6.0\n","step: 3390 \t epsilon: 0.6919 \t cumulative reward:  6.1\n","step: 3395 \t epsilon: 0.6915 \t cumulative reward: -2.1\n","step: 3400 \t epsilon: 0.6910 \t cumulative reward:  2.0\n","step: 3405 \t epsilon: 0.6905 \t cumulative reward: -1.9\n","step: 3410 \t epsilon: 0.6901 \t cumulative reward:  3.8\n","step: 3415 \t epsilon: 0.6896 \t cumulative reward: -0.2\n","step: 3420 \t epsilon: 0.6892 \t cumulative reward:  1.8\n","step: 3425 \t epsilon: 0.6887 \t cumulative reward: -0.9\n","step: 3430 \t epsilon: 0.6883 \t cumulative reward: -2.5\n","step: 3435 \t epsilon: 0.6878 \t cumulative reward:  1.8\n","step: 3440 \t epsilon: 0.6874 \t cumulative reward:  1.8\n","step: 3445 \t epsilon: 0.6869 \t cumulative reward: -1.9\n","step: 3450 \t epsilon: 0.6865 \t cumulative reward: -4.0\n","step: 3455 \t epsilon: 0.6860 \t cumulative reward:  4.3\n","step: 3460 \t epsilon: 0.6855 \t cumulative reward:  4.1\n","step: 3465 \t epsilon: 0.6851 \t cumulative reward:  1.8\n","step: 3470 \t epsilon: 0.6846 \t cumulative reward: -0.9\n","step: 3475 \t epsilon: 0.6842 \t cumulative reward:  3.5\n","step: 3480 \t epsilon: 0.6837 \t cumulative reward:  3.9\n","step: 3485 \t epsilon: 0.6833 \t cumulative reward:  6.0\n","step: 3490 \t epsilon: 0.6828 \t cumulative reward: -3.9\n","step: 3495 \t epsilon: 0.6824 \t cumulative reward: -2.3\n","step: 3500 \t epsilon: 0.6819 \t cumulative reward: -0.4\n","step: 3505 \t epsilon: 0.6815 \t cumulative reward:  3.8\n","step: 3510 \t epsilon: 0.6810 \t cumulative reward: -0.2\n","step: 3515 \t epsilon: 0.6805 \t cumulative reward:  3.7\n","step: 3520 \t epsilon: 0.6801 \t cumulative reward:  3.4\n","step: 3525 \t epsilon: 0.6796 \t cumulative reward: -0.4\n","step: 3530 \t epsilon: 0.6792 \t cumulative reward:  4.0\n","step: 3535 \t epsilon: 0.6787 \t cumulative reward:  3.9\n","step: 3540 \t epsilon: 0.6783 \t cumulative reward:  7.8\n","step: 3545 \t epsilon: 0.6778 \t cumulative reward:  3.8\n","step: 3550 \t epsilon: 0.6774 \t cumulative reward:  2.0\n","step: 3555 \t epsilon: 0.6769 \t cumulative reward:  3.9\n","step: 3560 \t epsilon: 0.6765 \t cumulative reward: -1.9\n","step: 3565 \t epsilon: 0.6760 \t cumulative reward:  4.1\n","step: 3570 \t epsilon: 0.6755 \t cumulative reward:  4.1\n","step: 3575 \t epsilon: 0.6751 \t cumulative reward: -0.3\n","step: 3580 \t epsilon: 0.6746 \t cumulative reward: -2.2\n","step: 3585 \t epsilon: 0.6742 \t cumulative reward:  3.7\n","step: 3590 \t epsilon: 0.6737 \t cumulative reward:  3.8\n","step: 3595 \t epsilon: 0.6733 \t cumulative reward:  3.4\n","step: 3600 \t epsilon: 0.6728 \t cumulative reward:  4.2\n","step: 3605 \t epsilon: 0.6724 \t cumulative reward:  4.1\n","step: 3610 \t epsilon: 0.6719 \t cumulative reward:  0.0\n","step: 3615 \t epsilon: 0.6715 \t cumulative reward:  3.9\n","step: 3620 \t epsilon: 0.6710 \t cumulative reward:  3.6\n","step: 3625 \t epsilon: 0.6705 \t cumulative reward:  3.7\n","step: 3630 \t epsilon: 0.6701 \t cumulative reward:  1.6\n","step: 3635 \t epsilon: 0.6696 \t cumulative reward: -0.7\n","step: 3640 \t epsilon: 0.6692 \t cumulative reward: -2.2\n","step: 3645 \t epsilon: 0.6687 \t cumulative reward:  2.1\n","step: 3650 \t epsilon: 0.6683 \t cumulative reward:  3.9\n","step: 3655 \t epsilon: 0.6678 \t cumulative reward:  8.0\n","step: 3660 \t epsilon: 0.6674 \t cumulative reward:  4.1\n","step: 3665 \t epsilon: 0.6669 \t cumulative reward:  7.8\n","step: 3670 \t epsilon: 0.6665 \t cumulative reward:  4.3\n","step: 3675 \t epsilon: 0.6660 \t cumulative reward: -2.0\n","step: 3680 \t epsilon: 0.6655 \t cumulative reward:  3.6\n","step: 3685 \t epsilon: 0.6651 \t cumulative reward:  2.1\n","step: 3690 \t epsilon: 0.6646 \t cumulative reward:  8.1\n","step: 3695 \t epsilon: 0.6642 \t cumulative reward: -1.7\n","step: 3700 \t epsilon: 0.6637 \t cumulative reward: -2.0\n","step: 3705 \t epsilon: 0.6633 \t cumulative reward:  5.8\n","step: 3710 \t epsilon: 0.6628 \t cumulative reward: -1.9\n","step: 3715 \t epsilon: 0.6624 \t cumulative reward: -2.3\n","step: 3720 \t epsilon: 0.6619 \t cumulative reward:  3.8\n","step: 3725 \t epsilon: 0.6615 \t cumulative reward:  3.9\n","step: 3730 \t epsilon: 0.6610 \t cumulative reward:  3.6\n","step: 3735 \t epsilon: 0.6605 \t cumulative reward:  1.9\n","step: 3740 \t epsilon: 0.6601 \t cumulative reward:  4.2\n","step: 3745 \t epsilon: 0.6596 \t cumulative reward:  4.2\n","step: 3750 \t epsilon: 0.6592 \t cumulative reward: -2.0\n","step: 3755 \t epsilon: 0.6587 \t cumulative reward:  8.0\n","step: 3760 \t epsilon: 0.6583 \t cumulative reward:  3.9\n","step: 3765 \t epsilon: 0.6578 \t cumulative reward:  8.0\n","step: 3770 \t epsilon: 0.6574 \t cumulative reward:  8.0\n","step: 3775 \t epsilon: 0.6569 \t cumulative reward:  1.7\n","step: 3780 \t epsilon: 0.6565 \t cumulative reward: -0.5\n","step: 3785 \t epsilon: 0.6560 \t cumulative reward:  3.7\n","step: 3790 \t epsilon: 0.6555 \t cumulative reward: -0.6\n","step: 3795 \t epsilon: 0.6551 \t cumulative reward:  3.7\n","step: 3800 \t epsilon: 0.6546 \t cumulative reward:  3.8\n","step: 3805 \t epsilon: 0.6542 \t cumulative reward: -2.1\n","step: 3810 \t epsilon: 0.6537 \t cumulative reward: -1.8\n","step: 3815 \t epsilon: 0.6533 \t cumulative reward: -2.2\n","step: 3820 \t epsilon: 0.6528 \t cumulative reward:  4.1\n","step: 3825 \t epsilon: 0.6524 \t cumulative reward: -0.3\n","step: 3830 \t epsilon: 0.6519 \t cumulative reward:  3.8\n","step: 3835 \t epsilon: 0.6515 \t cumulative reward:  3.6\n","step: 3840 \t epsilon: 0.6510 \t cumulative reward:  3.9\n","step: 3845 \t epsilon: 0.6505 \t cumulative reward:  5.8\n","step: 3850 \t epsilon: 0.6501 \t cumulative reward:  3.7\n","step: 3855 \t epsilon: 0.6496 \t cumulative reward:  3.5\n","step: 3860 \t epsilon: 0.6492 \t cumulative reward:  4.1\n","step: 3865 \t epsilon: 0.6487 \t cumulative reward:  2.1\n","step: 3870 \t epsilon: 0.6483 \t cumulative reward:  2.3\n","step: 3875 \t epsilon: 0.6478 \t cumulative reward:  8.0\n","step: 3880 \t epsilon: 0.6474 \t cumulative reward:  4.0\n","step: 3885 \t epsilon: 0.6469 \t cumulative reward:  3.5\n","step: 3890 \t epsilon: 0.6465 \t cumulative reward:  3.9\n","step: 3895 \t epsilon: 0.6460 \t cumulative reward:  5.8\n","step: 3900 \t epsilon: 0.6455 \t cumulative reward:  2.1\n","step: 3905 \t epsilon: 0.6451 \t cumulative reward:  6.0\n","step: 3910 \t epsilon: 0.6446 \t cumulative reward: -2.1\n","step: 3915 \t epsilon: 0.6442 \t cumulative reward: -1.9\n","step: 3920 \t epsilon: 0.6437 \t cumulative reward:  4.1\n","step: 3925 \t epsilon: 0.6433 \t cumulative reward:  3.8\n","step: 3930 \t epsilon: 0.6428 \t cumulative reward:  4.1\n","step: 3935 \t epsilon: 0.6424 \t cumulative reward:  4.0\n","step: 3940 \t epsilon: 0.6419 \t cumulative reward:  6.0\n","step: 3945 \t epsilon: 0.6415 \t cumulative reward:  2.1\n","step: 3950 \t epsilon: 0.6410 \t cumulative reward:  4.0\n","step: 3955 \t epsilon: 0.6405 \t cumulative reward: -2.0\n","step: 3960 \t epsilon: 0.6401 \t cumulative reward:  3.6\n","step: 3965 \t epsilon: 0.6396 \t cumulative reward:  3.9\n","step: 3970 \t epsilon: 0.6392 \t cumulative reward:  4.1\n","step: 3975 \t epsilon: 0.6387 \t cumulative reward:  7.8\n","step: 3980 \t epsilon: 0.6383 \t cumulative reward:  6.0\n","step: 3985 \t epsilon: 0.6378 \t cumulative reward:  3.9\n","step: 3990 \t epsilon: 0.6374 \t cumulative reward:  3.9\n","step: 3995 \t epsilon: 0.6369 \t cumulative reward:  3.6\n","step: 4000 \t epsilon: 0.6365 \t cumulative reward:  2.1\n","step: 4005 \t epsilon: 0.6360 \t cumulative reward: -2.1\n","step: 4010 \t epsilon: 0.6355 \t cumulative reward:  4.3\n","step: 4015 \t epsilon: 0.6351 \t cumulative reward:  0.4\n","step: 4020 \t epsilon: 0.6346 \t cumulative reward:  3.8\n","step: 4025 \t epsilon: 0.6342 \t cumulative reward: -3.9\n","step: 4030 \t epsilon: 0.6337 \t cumulative reward: -0.7\n","step: 4035 \t epsilon: 0.6333 \t cumulative reward:  3.7\n","step: 4040 \t epsilon: 0.6328 \t cumulative reward:  0.3\n","step: 4045 \t epsilon: 0.6324 \t cumulative reward:  5.9\n","step: 4050 \t epsilon: 0.6319 \t cumulative reward: -3.9\n","step: 4055 \t epsilon: 0.6315 \t cumulative reward:  3.5\n","step: 4060 \t epsilon: 0.6310 \t cumulative reward:  3.7\n","step: 4065 \t epsilon: 0.6305 \t cumulative reward:  7.8\n","step: 4070 \t epsilon: 0.6301 \t cumulative reward:  8.1\n","step: 4075 \t epsilon: 0.6296 \t cumulative reward:  2.0\n","step: 4080 \t epsilon: 0.6292 \t cumulative reward:  3.8\n","step: 4085 \t epsilon: 0.6287 \t cumulative reward:  7.8\n","step: 4090 \t epsilon: 0.6283 \t cumulative reward:  7.8\n","step: 4095 \t epsilon: 0.6278 \t cumulative reward:  8.1\n","step: 4100 \t epsilon: 0.6274 \t cumulative reward:  3.7\n","step: 4105 \t epsilon: 0.6269 \t cumulative reward: -0.6\n","step: 4110 \t epsilon: 0.6265 \t cumulative reward:  3.7\n","step: 4115 \t epsilon: 0.6260 \t cumulative reward:  4.1\n","step: 4120 \t epsilon: 0.6255 \t cumulative reward:  4.0\n","step: 4125 \t epsilon: 0.6251 \t cumulative reward:  5.8\n","step: 4130 \t epsilon: 0.6246 \t cumulative reward:  4.1\n","step: 4135 \t epsilon: 0.6242 \t cumulative reward: -0.5\n","step: 4140 \t epsilon: 0.6237 \t cumulative reward:  3.9\n","step: 4145 \t epsilon: 0.6233 \t cumulative reward:  4.0\n","step: 4150 \t epsilon: 0.6228 \t cumulative reward: -1.8\n","step: 4155 \t epsilon: 0.6224 \t cumulative reward:  4.2\n","step: 4160 \t epsilon: 0.6219 \t cumulative reward: -0.4\n","step: 4165 \t epsilon: 0.6215 \t cumulative reward:  8.0\n","step: 4170 \t epsilon: 0.6210 \t cumulative reward:  2.0\n","step: 4175 \t epsilon: 0.6205 \t cumulative reward:  3.9\n","step: 4180 \t epsilon: 0.6201 \t cumulative reward:  4.0\n","step: 4185 \t epsilon: 0.6196 \t cumulative reward:  8.0\n","step: 4190 \t epsilon: 0.6192 \t cumulative reward:  8.0\n","step: 4195 \t epsilon: 0.6187 \t cumulative reward:  4.0\n","step: 4200 \t epsilon: 0.6183 \t cumulative reward: -4.1\n","step: 4205 \t epsilon: 0.6178 \t cumulative reward:  4.0\n","step: 4210 \t epsilon: 0.6174 \t cumulative reward:  8.0\n","step: 4215 \t epsilon: 0.6169 \t cumulative reward:  0.0\n","step: 4220 \t epsilon: 0.6165 \t cumulative reward:  3.8\n","step: 4225 \t epsilon: 0.6160 \t cumulative reward:  3.5\n","step: 4230 \t epsilon: 0.6155 \t cumulative reward:  3.7\n","step: 4235 \t epsilon: 0.6151 \t cumulative reward:  4.3\n","step: 4240 \t epsilon: 0.6146 \t cumulative reward:  0.5\n","step: 4245 \t epsilon: 0.6142 \t cumulative reward:  8.0\n","step: 4250 \t epsilon: 0.6137 \t cumulative reward: -0.5\n","step: 4255 \t epsilon: 0.6133 \t cumulative reward: -2.2\n","step: 4260 \t epsilon: 0.6128 \t cumulative reward:  4.1\n","step: 4265 \t epsilon: 0.6124 \t cumulative reward:  3.8\n","step: 4270 \t epsilon: 0.6119 \t cumulative reward:  2.1\n","step: 4275 \t epsilon: 0.6115 \t cumulative reward:  8.0\n","step: 4280 \t epsilon: 0.6110 \t cumulative reward:  4.1\n","step: 4285 \t epsilon: 0.6105 \t cumulative reward:  2.0\n","step: 4290 \t epsilon: 0.6101 \t cumulative reward: -0.7\n","step: 4295 \t epsilon: 0.6096 \t cumulative reward:  2.0\n","step: 4300 \t epsilon: 0.6092 \t cumulative reward: -1.9\n","step: 4305 \t epsilon: 0.6087 \t cumulative reward:  7.9\n","step: 4310 \t epsilon: 0.6083 \t cumulative reward:  1.9\n","step: 4315 \t epsilon: 0.6078 \t cumulative reward:  3.9\n","step: 4320 \t epsilon: 0.6074 \t cumulative reward:  1.8\n","step: 4325 \t epsilon: 0.6069 \t cumulative reward:  0.3\n","step: 4330 \t epsilon: 0.6065 \t cumulative reward:  0.1\n","step: 4335 \t epsilon: 0.6060 \t cumulative reward:  6.0\n","step: 4340 \t epsilon: 0.6055 \t cumulative reward:  1.8\n","step: 4345 \t epsilon: 0.6051 \t cumulative reward:  5.9\n","step: 4350 \t epsilon: 0.6046 \t cumulative reward:  4.3\n","step: 4355 \t epsilon: 0.6042 \t cumulative reward:  3.4\n","step: 4360 \t epsilon: 0.6037 \t cumulative reward:  0.1\n","step: 4365 \t epsilon: 0.6033 \t cumulative reward:  2.1\n","step: 4370 \t epsilon: 0.6028 \t cumulative reward: -0.5\n","step: 4375 \t epsilon: 0.6024 \t cumulative reward:  4.0\n","step: 4380 \t epsilon: 0.6019 \t cumulative reward:  2.1\n","step: 4385 \t epsilon: 0.6015 \t cumulative reward: -2.0\n","step: 4390 \t epsilon: 0.6010 \t cumulative reward:  3.9\n","step: 4395 \t epsilon: 0.6005 \t cumulative reward:  4.1\n","step: 4400 \t epsilon: 0.6001 \t cumulative reward: -2.1\n","step: 4405 \t epsilon: 0.5996 \t cumulative reward:  7.8\n","step: 4410 \t epsilon: 0.5992 \t cumulative reward:  6.0\n","step: 4415 \t epsilon: 0.5987 \t cumulative reward:  3.4\n","step: 4420 \t epsilon: 0.5983 \t cumulative reward:  4.0\n","step: 4425 \t epsilon: 0.5978 \t cumulative reward:  0.3\n","step: 4430 \t epsilon: 0.5974 \t cumulative reward:  0.2\n","step: 4435 \t epsilon: 0.5969 \t cumulative reward:  8.1\n","step: 4440 \t epsilon: 0.5965 \t cumulative reward:  8.0\n","step: 4445 \t epsilon: 0.5960 \t cumulative reward:  6.0\n","step: 4450 \t epsilon: 0.5955 \t cumulative reward:  0.4\n","step: 4455 \t epsilon: 0.5951 \t cumulative reward:  3.9\n","step: 4460 \t epsilon: 0.5946 \t cumulative reward:  2.0\n","step: 4465 \t epsilon: 0.5942 \t cumulative reward: -0.1\n","step: 4470 \t epsilon: 0.5937 \t cumulative reward:  5.8\n","step: 4475 \t epsilon: 0.5933 \t cumulative reward:  4.1\n","step: 4480 \t epsilon: 0.5928 \t cumulative reward:  0.0\n","step: 4485 \t epsilon: 0.5924 \t cumulative reward:  7.8\n","step: 4490 \t epsilon: 0.5919 \t cumulative reward:  1.8\n","step: 4495 \t epsilon: 0.5915 \t cumulative reward: -3.9\n","step: 4500 \t epsilon: 0.5910 \t cumulative reward: -1.9\n","step: 4505 \t epsilon: 0.5905 \t cumulative reward:  3.8\n","step: 4510 \t epsilon: 0.5901 \t cumulative reward:  8.0\n","step: 4515 \t epsilon: 0.5896 \t cumulative reward:  3.7\n","step: 4520 \t epsilon: 0.5892 \t cumulative reward:  4.0\n","step: 4525 \t epsilon: 0.5887 \t cumulative reward:  6.0\n","step: 4530 \t epsilon: 0.5883 \t cumulative reward:  4.2\n","step: 4535 \t epsilon: 0.5878 \t cumulative reward:  4.0\n","step: 4540 \t epsilon: 0.5874 \t cumulative reward:  3.7\n","step: 4545 \t epsilon: 0.5869 \t cumulative reward:  0.1\n","step: 4550 \t epsilon: 0.5865 \t cumulative reward: -0.1\n","step: 4555 \t epsilon: 0.5860 \t cumulative reward:  7.9\n","step: 4560 \t epsilon: 0.5855 \t cumulative reward:  4.1\n","step: 4565 \t epsilon: 0.5851 \t cumulative reward:  4.2\n","step: 4570 \t epsilon: 0.5846 \t cumulative reward: -0.2\n","step: 4575 \t epsilon: 0.5842 \t cumulative reward:  7.8\n","step: 4580 \t epsilon: 0.5837 \t cumulative reward:  3.6\n","step: 4585 \t epsilon: 0.5833 \t cumulative reward:  1.8\n","step: 4590 \t epsilon: 0.5828 \t cumulative reward:  3.6\n","step: 4595 \t epsilon: 0.5824 \t cumulative reward:  7.9\n","step: 4600 \t epsilon: 0.5819 \t cumulative reward:  6.0\n","step: 4605 \t epsilon: 0.5815 \t cumulative reward:  7.8\n","step: 4610 \t epsilon: 0.5810 \t cumulative reward:  4.1\n","step: 4615 \t epsilon: 0.5805 \t cumulative reward:  3.8\n","step: 4620 \t epsilon: 0.5801 \t cumulative reward:  1.7\n","step: 4625 \t epsilon: 0.5796 \t cumulative reward: -1.9\n","step: 4630 \t epsilon: 0.5792 \t cumulative reward:  3.4\n","step: 4635 \t epsilon: 0.5787 \t cumulative reward:  6.0\n","step: 4640 \t epsilon: 0.5783 \t cumulative reward: -0.3\n","step: 4645 \t epsilon: 0.5778 \t cumulative reward:  4.2\n","step: 4650 \t epsilon: 0.5774 \t cumulative reward:  4.0\n","step: 4655 \t epsilon: 0.5769 \t cumulative reward:  8.1\n","step: 4660 \t epsilon: 0.5765 \t cumulative reward:  1.8\n","step: 4665 \t epsilon: 0.5760 \t cumulative reward:  4.0\n","step: 4670 \t epsilon: 0.5755 \t cumulative reward:  5.9\n","step: 4675 \t epsilon: 0.5751 \t cumulative reward:  0.0\n","step: 4680 \t epsilon: 0.5746 \t cumulative reward:  2.1\n","step: 4685 \t epsilon: 0.5742 \t cumulative reward:  4.1\n","step: 4690 \t epsilon: 0.5737 \t cumulative reward:  0.1\n","step: 4695 \t epsilon: 0.5733 \t cumulative reward:  1.7\n","step: 4700 \t epsilon: 0.5728 \t cumulative reward:  8.1\n","step: 4705 \t epsilon: 0.5724 \t cumulative reward:  4.2\n","step: 4710 \t epsilon: 0.5719 \t cumulative reward: -0.1\n","step: 4715 \t epsilon: 0.5715 \t cumulative reward:  8.1\n","step: 4720 \t epsilon: 0.5710 \t cumulative reward:  0.0\n","step: 4725 \t epsilon: 0.5705 \t cumulative reward:  3.6\n","step: 4730 \t epsilon: 0.5701 \t cumulative reward:  1.7\n","step: 4735 \t epsilon: 0.5696 \t cumulative reward:  8.0\n","step: 4740 \t epsilon: 0.5692 \t cumulative reward:  4.1\n","step: 4745 \t epsilon: 0.5687 \t cumulative reward:  4.2\n","step: 4750 \t epsilon: 0.5683 \t cumulative reward:  3.8\n","step: 4755 \t epsilon: 0.5678 \t cumulative reward:  3.4\n","step: 4760 \t epsilon: 0.5674 \t cumulative reward:  2.0\n","step: 4765 \t epsilon: 0.5669 \t cumulative reward:  2.1\n","step: 4770 \t epsilon: 0.5665 \t cumulative reward: -0.2\n","step: 4775 \t epsilon: 0.5660 \t cumulative reward: -0.3\n","step: 4780 \t epsilon: 0.5655 \t cumulative reward:  4.1\n","step: 4785 \t epsilon: 0.5651 \t cumulative reward: -2.3\n","step: 4790 \t epsilon: 0.5646 \t cumulative reward:  1.9\n","step: 4795 \t epsilon: 0.5642 \t cumulative reward:  7.8\n","step: 4800 \t epsilon: 0.5637 \t cumulative reward:  4.1\n","step: 4805 \t epsilon: 0.5633 \t cumulative reward:  0.2\n","step: 4810 \t epsilon: 0.5628 \t cumulative reward:  4.3\n","step: 4815 \t epsilon: 0.5624 \t cumulative reward:  4.0\n","step: 4820 \t epsilon: 0.5619 \t cumulative reward:  3.9\n","step: 4825 \t epsilon: 0.5615 \t cumulative reward:  2.2\n","step: 4830 \t epsilon: 0.5610 \t cumulative reward: -4.0\n","step: 4835 \t epsilon: 0.5605 \t cumulative reward:  4.2\n","step: 4840 \t epsilon: 0.5601 \t cumulative reward:  0.3\n","step: 4845 \t epsilon: 0.5596 \t cumulative reward:  4.0\n","step: 4850 \t epsilon: 0.5592 \t cumulative reward:  2.1\n","step: 4855 \t epsilon: 0.5587 \t cumulative reward:  3.8\n","step: 4860 \t epsilon: 0.5583 \t cumulative reward: -2.3\n","step: 4865 \t epsilon: 0.5578 \t cumulative reward:  7.8\n","step: 4870 \t epsilon: 0.5574 \t cumulative reward:  6.1\n","step: 4875 \t epsilon: 0.5569 \t cumulative reward:  8.1\n","step: 4880 \t epsilon: 0.5565 \t cumulative reward:  0.0\n","step: 4885 \t epsilon: 0.5560 \t cumulative reward:  4.2\n","step: 4890 \t epsilon: 0.5555 \t cumulative reward:  3.8\n","step: 4895 \t epsilon: 0.5551 \t cumulative reward:  0.4\n","step: 4900 \t epsilon: 0.5546 \t cumulative reward:  3.9\n","step: 4905 \t epsilon: 0.5542 \t cumulative reward:  0.1\n","step: 4910 \t epsilon: 0.5537 \t cumulative reward:  1.8\n","step: 4915 \t epsilon: 0.5533 \t cumulative reward:  8.0\n","step: 4920 \t epsilon: 0.5528 \t cumulative reward:  4.2\n","step: 4925 \t epsilon: 0.5524 \t cumulative reward: -2.1\n","step: 4930 \t epsilon: 0.5519 \t cumulative reward:  7.8\n","step: 4935 \t epsilon: 0.5515 \t cumulative reward:  8.0\n","step: 4940 \t epsilon: 0.5510 \t cumulative reward:  3.9\n","step: 4945 \t epsilon: 0.5505 \t cumulative reward:  3.8\n","step: 4950 \t epsilon: 0.5501 \t cumulative reward:  4.2\n","step: 4955 \t epsilon: 0.5496 \t cumulative reward:  5.8\n","step: 4960 \t epsilon: 0.5492 \t cumulative reward:  0.0\n","step: 4965 \t epsilon: 0.5487 \t cumulative reward:  4.3\n","step: 4970 \t epsilon: 0.5483 \t cumulative reward:  3.9\n","step: 4975 \t epsilon: 0.5478 \t cumulative reward: -1.9\n","step: 4980 \t epsilon: 0.5474 \t cumulative reward: -3.9\n","step: 4985 \t epsilon: 0.5469 \t cumulative reward:  0.1\n","step: 4990 \t epsilon: 0.5465 \t cumulative reward:  4.2\n","step: 4995 \t epsilon: 0.5460 \t cumulative reward: -2.0\n","step: 5000 \t epsilon: 0.5455 \t cumulative reward:  2.1\n","step: 5005 \t epsilon: 0.5451 \t cumulative reward:  5.9\n","step: 5010 \t epsilon: 0.5446 \t cumulative reward:  3.9\n","step: 5015 \t epsilon: 0.5442 \t cumulative reward:  3.9\n","step: 5020 \t epsilon: 0.5437 \t cumulative reward: -2.2\n","step: 5025 \t epsilon: 0.5433 \t cumulative reward:  6.1\n","step: 5030 \t epsilon: 0.5428 \t cumulative reward:  8.0\n","step: 5035 \t epsilon: 0.5424 \t cumulative reward: -0.5\n","step: 5040 \t epsilon: 0.5419 \t cumulative reward:  5.9\n","step: 5045 \t epsilon: 0.5415 \t cumulative reward:  8.1\n","step: 5050 \t epsilon: 0.5410 \t cumulative reward:  1.7\n","step: 5055 \t epsilon: 0.5405 \t cumulative reward:  2.1\n","step: 5060 \t epsilon: 0.5401 \t cumulative reward:  4.0\n","step: 5065 \t epsilon: 0.5396 \t cumulative reward:  7.9\n","step: 5070 \t epsilon: 0.5392 \t cumulative reward:  3.6\n","step: 5075 \t epsilon: 0.5387 \t cumulative reward:  2.0\n","step: 5080 \t epsilon: 0.5383 \t cumulative reward:  3.5\n","step: 5085 \t epsilon: 0.5378 \t cumulative reward:  6.0\n","step: 5090 \t epsilon: 0.5374 \t cumulative reward: -1.9\n","step: 5095 \t epsilon: 0.5369 \t cumulative reward:  6.0\n","step: 5100 \t epsilon: 0.5365 \t cumulative reward: -2.4\n","step: 5105 \t epsilon: 0.5360 \t cumulative reward:  6.1\n","step: 5110 \t epsilon: 0.5355 \t cumulative reward:  3.7\n","step: 5115 \t epsilon: 0.5351 \t cumulative reward:  3.8\n","step: 5120 \t epsilon: 0.5346 \t cumulative reward: -1.8\n","step: 5125 \t epsilon: 0.5342 \t cumulative reward:  4.2\n","step: 5130 \t epsilon: 0.5337 \t cumulative reward: -1.9\n","step: 5135 \t epsilon: 0.5333 \t cumulative reward:  4.0\n","step: 5140 \t epsilon: 0.5328 \t cumulative reward:  5.9\n","step: 5145 \t epsilon: 0.5324 \t cumulative reward:  4.0\n","step: 5150 \t epsilon: 0.5319 \t cumulative reward:  3.9\n","step: 5155 \t epsilon: 0.5315 \t cumulative reward:  8.1\n","step: 5160 \t epsilon: 0.5310 \t cumulative reward:  7.9\n","step: 5165 \t epsilon: 0.5305 \t cumulative reward:  3.4\n","step: 5170 \t epsilon: 0.5301 \t cumulative reward: -1.9\n","step: 5175 \t epsilon: 0.5296 \t cumulative reward:  4.0\n","step: 5180 \t epsilon: 0.5292 \t cumulative reward: -1.7\n","step: 5185 \t epsilon: 0.5287 \t cumulative reward:  1.9\n","step: 5190 \t epsilon: 0.5283 \t cumulative reward: -0.1\n","step: 5195 \t epsilon: 0.5278 \t cumulative reward:  3.9\n","step: 5200 \t epsilon: 0.5274 \t cumulative reward:  2.0\n","step: 5205 \t epsilon: 0.5269 \t cumulative reward:  4.1\n","step: 5210 \t epsilon: 0.5265 \t cumulative reward:  3.8\n","step: 5215 \t epsilon: 0.5260 \t cumulative reward:  7.8\n","step: 5220 \t epsilon: 0.5255 \t cumulative reward:  7.8\n","step: 5225 \t epsilon: 0.5251 \t cumulative reward:  8.0\n","step: 5230 \t epsilon: 0.5246 \t cumulative reward:  4.0\n","step: 5235 \t epsilon: 0.5242 \t cumulative reward:  2.1\n","step: 5240 \t epsilon: 0.5237 \t cumulative reward:  3.8\n","step: 5245 \t epsilon: 0.5233 \t cumulative reward:  3.7\n","step: 5250 \t epsilon: 0.5228 \t cumulative reward:  2.0\n","step: 5255 \t epsilon: 0.5224 \t cumulative reward: -1.8\n","step: 5260 \t epsilon: 0.5219 \t cumulative reward:  1.8\n","step: 5265 \t epsilon: 0.5215 \t cumulative reward: -3.9\n","step: 5270 \t epsilon: 0.5210 \t cumulative reward:  7.9\n","step: 5275 \t epsilon: 0.5205 \t cumulative reward:  3.9\n","step: 5280 \t epsilon: 0.5201 \t cumulative reward: -0.5\n","step: 5285 \t epsilon: 0.5196 \t cumulative reward:  6.1\n","step: 5290 \t epsilon: 0.5192 \t cumulative reward:  7.8\n","step: 5295 \t epsilon: 0.5187 \t cumulative reward: -1.7\n","step: 5300 \t epsilon: 0.5183 \t cumulative reward:  8.0\n","step: 5305 \t epsilon: 0.5178 \t cumulative reward:  7.8\n","step: 5310 \t epsilon: 0.5174 \t cumulative reward:  8.0\n","step: 5315 \t epsilon: 0.5169 \t cumulative reward:  6.1\n","step: 5320 \t epsilon: 0.5165 \t cumulative reward: -2.2\n","step: 5325 \t epsilon: 0.5160 \t cumulative reward:  7.8\n","step: 5330 \t epsilon: 0.5155 \t cumulative reward:  2.1\n","step: 5335 \t epsilon: 0.5151 \t cumulative reward:  7.9\n","step: 5340 \t epsilon: 0.5146 \t cumulative reward:  3.9\n","step: 5345 \t epsilon: 0.5142 \t cumulative reward:  4.3\n","step: 5350 \t epsilon: 0.5137 \t cumulative reward: -1.8\n","step: 5355 \t epsilon: 0.5133 \t cumulative reward:  8.0\n","step: 5360 \t epsilon: 0.5128 \t cumulative reward:  2.3\n","step: 5365 \t epsilon: 0.5124 \t cumulative reward:  4.0\n","step: 5370 \t epsilon: 0.5119 \t cumulative reward:  3.8\n","step: 5375 \t epsilon: 0.5115 \t cumulative reward:  8.0\n","step: 5380 \t epsilon: 0.5110 \t cumulative reward: -2.0\n","step: 5385 \t epsilon: 0.5105 \t cumulative reward:  4.1\n","step: 5390 \t epsilon: 0.5101 \t cumulative reward:  0.2\n","step: 5395 \t epsilon: 0.5096 \t cumulative reward:  8.1\n","step: 5400 \t epsilon: 0.5092 \t cumulative reward:  2.1\n","step: 5405 \t epsilon: 0.5087 \t cumulative reward: -0.7\n","step: 5410 \t epsilon: 0.5083 \t cumulative reward:  4.0\n","step: 5415 \t epsilon: 0.5078 \t cumulative reward:  4.3\n","step: 5420 \t epsilon: 0.5074 \t cumulative reward:  3.8\n","step: 5425 \t epsilon: 0.5069 \t cumulative reward:  4.1\n","step: 5430 \t epsilon: 0.5065 \t cumulative reward:  3.4\n","step: 5435 \t epsilon: 0.5060 \t cumulative reward:  2.1\n","step: 5440 \t epsilon: 0.5055 \t cumulative reward:  2.0\n","step: 5445 \t epsilon: 0.5051 \t cumulative reward:  1.8\n","step: 5450 \t epsilon: 0.5046 \t cumulative reward:  8.1\n","step: 5455 \t epsilon: 0.5042 \t cumulative reward:  3.7\n","step: 5460 \t epsilon: 0.5037 \t cumulative reward:  6.0\n","step: 5465 \t epsilon: 0.5033 \t cumulative reward:  0.0\n","step: 5470 \t epsilon: 0.5028 \t cumulative reward:  8.1\n","step: 5475 \t epsilon: 0.5024 \t cumulative reward:  3.7\n","step: 5480 \t epsilon: 0.5019 \t cumulative reward:  2.1\n","step: 5485 \t epsilon: 0.5015 \t cumulative reward:  4.1\n","step: 5490 \t epsilon: 0.5010 \t cumulative reward:  3.9\n","step: 5495 \t epsilon: 0.5005 \t cumulative reward:  4.0\n","step: 5500 \t epsilon: 0.5001 \t cumulative reward:  7.8\n","step: 5505 \t epsilon: 0.4996 \t cumulative reward:  3.7\n","step: 5510 \t epsilon: 0.4992 \t cumulative reward:  4.0\n","step: 5515 \t epsilon: 0.4987 \t cumulative reward:  3.7\n","step: 5520 \t epsilon: 0.4983 \t cumulative reward: -2.1\n","step: 5525 \t epsilon: 0.4978 \t cumulative reward:  3.8\n","step: 5530 \t epsilon: 0.4974 \t cumulative reward:  1.9\n","step: 5535 \t epsilon: 0.4969 \t cumulative reward:  3.7\n","step: 5540 \t epsilon: 0.4965 \t cumulative reward: -1.0\n","step: 5545 \t epsilon: 0.4960 \t cumulative reward:  8.1\n","step: 5550 \t epsilon: 0.4955 \t cumulative reward:  8.0\n","step: 5555 \t epsilon: 0.4951 \t cumulative reward:  2.1\n","step: 5560 \t epsilon: 0.4946 \t cumulative reward:  3.8\n","step: 5565 \t epsilon: 0.4942 \t cumulative reward:  4.2\n","step: 5570 \t epsilon: 0.4937 \t cumulative reward:  8.1\n","step: 5575 \t epsilon: 0.4933 \t cumulative reward:  0.4\n","step: 5580 \t epsilon: 0.4928 \t cumulative reward:  0.4\n","step: 5585 \t epsilon: 0.4924 \t cumulative reward:  4.0\n","step: 5590 \t epsilon: 0.4919 \t cumulative reward:  7.8\n","step: 5595 \t epsilon: 0.4915 \t cumulative reward:  2.2\n","step: 5600 \t epsilon: 0.4910 \t cumulative reward:  4.0\n","step: 5605 \t epsilon: 0.4905 \t cumulative reward:  4.1\n","step: 5610 \t epsilon: 0.4901 \t cumulative reward:  0.0\n","step: 5615 \t epsilon: 0.4896 \t cumulative reward:  4.1\n","step: 5620 \t epsilon: 0.4892 \t cumulative reward:  0.5\n","step: 5625 \t epsilon: 0.4887 \t cumulative reward:  8.1\n","step: 5630 \t epsilon: 0.4883 \t cumulative reward:  2.2\n","step: 5635 \t epsilon: 0.4878 \t cumulative reward:  4.2\n","step: 5640 \t epsilon: 0.4874 \t cumulative reward:  4.0\n","step: 5645 \t epsilon: 0.4869 \t cumulative reward:  4.2\n","step: 5650 \t epsilon: 0.4865 \t cumulative reward: -2.1\n","step: 5655 \t epsilon: 0.4860 \t cumulative reward: -2.2\n","step: 5660 \t epsilon: 0.4855 \t cumulative reward:  3.9\n","step: 5665 \t epsilon: 0.4851 \t cumulative reward: -1.8\n","step: 5670 \t epsilon: 0.4846 \t cumulative reward:  4.3\n","step: 5675 \t epsilon: 0.4842 \t cumulative reward:  4.2\n","step: 5680 \t epsilon: 0.4837 \t cumulative reward:  5.8\n","step: 5685 \t epsilon: 0.4833 \t cumulative reward:  4.3\n","step: 5690 \t epsilon: 0.4828 \t cumulative reward: -2.1\n","step: 5695 \t epsilon: 0.4824 \t cumulative reward:  3.8\n","step: 5700 \t epsilon: 0.4819 \t cumulative reward: -2.1\n","step: 5705 \t epsilon: 0.4815 \t cumulative reward:  4.0\n","step: 5710 \t epsilon: 0.4810 \t cumulative reward:  2.0\n","step: 5715 \t epsilon: 0.4805 \t cumulative reward:  7.8\n","step: 5720 \t epsilon: 0.4801 \t cumulative reward:  4.0\n","step: 5725 \t epsilon: 0.4796 \t cumulative reward:  0.2\n","step: 5730 \t epsilon: 0.4792 \t cumulative reward: -2.1\n","step: 5735 \t epsilon: 0.4787 \t cumulative reward:  4.2\n","step: 5740 \t epsilon: 0.4783 \t cumulative reward:  4.3\n","step: 5745 \t epsilon: 0.4778 \t cumulative reward:  4.1\n","step: 5750 \t epsilon: 0.4774 \t cumulative reward:  2.0\n","step: 5755 \t epsilon: 0.4769 \t cumulative reward: -1.7\n","step: 5760 \t epsilon: 0.4765 \t cumulative reward:  1.8\n","step: 5765 \t epsilon: 0.4760 \t cumulative reward: -2.2\n","step: 5770 \t epsilon: 0.4755 \t cumulative reward:  3.8\n","step: 5775 \t epsilon: 0.4751 \t cumulative reward:  3.8\n","step: 5780 \t epsilon: 0.4746 \t cumulative reward:  7.9\n","step: 5785 \t epsilon: 0.4742 \t cumulative reward: -2.0\n","step: 5790 \t epsilon: 0.4737 \t cumulative reward: -1.8\n","step: 5795 \t epsilon: 0.4733 \t cumulative reward:  4.0\n","step: 5800 \t epsilon: 0.4728 \t cumulative reward:  4.1\n","step: 5805 \t epsilon: 0.4724 \t cumulative reward:  4.2\n","step: 5810 \t epsilon: 0.4719 \t cumulative reward:  3.7\n","step: 5815 \t epsilon: 0.4715 \t cumulative reward: -1.8\n","step: 5820 \t epsilon: 0.4710 \t cumulative reward:  4.2\n","step: 5825 \t epsilon: 0.4705 \t cumulative reward:  7.8\n","step: 5830 \t epsilon: 0.4701 \t cumulative reward:  8.1\n","step: 5835 \t epsilon: 0.4696 \t cumulative reward:  4.0\n","step: 5840 \t epsilon: 0.4692 \t cumulative reward:  8.0\n","step: 5845 \t epsilon: 0.4687 \t cumulative reward: -1.7\n","step: 5850 \t epsilon: 0.4683 \t cumulative reward:  4.2\n","step: 5855 \t epsilon: 0.4678 \t cumulative reward:  4.0\n","step: 5860 \t epsilon: 0.4674 \t cumulative reward:  4.2\n","step: 5865 \t epsilon: 0.4669 \t cumulative reward:  3.8\n","step: 5870 \t epsilon: 0.4665 \t cumulative reward:  4.1\n","step: 5875 \t epsilon: 0.4660 \t cumulative reward:  0.4\n","step: 5880 \t epsilon: 0.4655 \t cumulative reward:  1.9\n","step: 5885 \t epsilon: 0.4651 \t cumulative reward:  4.1\n","step: 5890 \t epsilon: 0.4646 \t cumulative reward:  8.0\n","step: 5895 \t epsilon: 0.4642 \t cumulative reward:  3.8\n","step: 5900 \t epsilon: 0.4637 \t cumulative reward:  4.2\n","step: 5905 \t epsilon: 0.4633 \t cumulative reward:  4.3\n","step: 5910 \t epsilon: 0.4628 \t cumulative reward:  8.1\n","step: 5915 \t epsilon: 0.4624 \t cumulative reward: -2.0\n","step: 5920 \t epsilon: 0.4619 \t cumulative reward:  4.2\n","step: 5925 \t epsilon: 0.4615 \t cumulative reward:  4.0\n","step: 5930 \t epsilon: 0.4610 \t cumulative reward:  4.0\n","step: 5935 \t epsilon: 0.4605 \t cumulative reward: -0.4\n","step: 5940 \t epsilon: 0.4601 \t cumulative reward:  0.2\n","step: 5945 \t epsilon: 0.4596 \t cumulative reward:  2.0\n","step: 5950 \t epsilon: 0.4592 \t cumulative reward:  2.2\n","step: 5955 \t epsilon: 0.4587 \t cumulative reward:  7.8\n","step: 5960 \t epsilon: 0.4583 \t cumulative reward:  0.0\n","step: 5965 \t epsilon: 0.4578 \t cumulative reward: -1.8\n","step: 5970 \t epsilon: 0.4574 \t cumulative reward:  4.3\n","step: 5975 \t epsilon: 0.4569 \t cumulative reward:  4.3\n","step: 5980 \t epsilon: 0.4565 \t cumulative reward:  4.2\n","step: 5985 \t epsilon: 0.4560 \t cumulative reward:  4.2\n","step: 5990 \t epsilon: 0.4555 \t cumulative reward:  4.2\n","step: 5995 \t epsilon: 0.4551 \t cumulative reward:  4.2\n","step: 6000 \t epsilon: 0.4546 \t cumulative reward: -1.8\n","step: 6005 \t epsilon: 0.4542 \t cumulative reward:  4.1\n","step: 6010 \t epsilon: 0.4537 \t cumulative reward:  2.2\n","step: 6015 \t epsilon: 0.4533 \t cumulative reward:  1.6\n","step: 6020 \t epsilon: 0.4528 \t cumulative reward:  3.8\n","step: 6025 \t epsilon: 0.4524 \t cumulative reward:  3.8\n","step: 6030 \t epsilon: 0.4519 \t cumulative reward:  3.4\n","step: 6035 \t epsilon: 0.4515 \t cumulative reward:  3.8\n","step: 6040 \t epsilon: 0.4510 \t cumulative reward:  7.9\n","step: 6045 \t epsilon: 0.4505 \t cumulative reward:  3.7\n","step: 6050 \t epsilon: 0.4501 \t cumulative reward:  7.8\n","step: 6055 \t epsilon: 0.4496 \t cumulative reward:  4.0\n","step: 6060 \t epsilon: 0.4492 \t cumulative reward:  3.9\n","step: 6065 \t epsilon: 0.4487 \t cumulative reward: -2.1\n","step: 6070 \t epsilon: 0.4483 \t cumulative reward:  7.9\n","step: 6075 \t epsilon: 0.4478 \t cumulative reward:  3.4\n","step: 6080 \t epsilon: 0.4474 \t cumulative reward:  4.0\n","step: 6085 \t epsilon: 0.4469 \t cumulative reward:  7.9\n","step: 6090 \t epsilon: 0.4465 \t cumulative reward:  8.1\n","step: 6095 \t epsilon: 0.4460 \t cumulative reward: -0.4\n","step: 6100 \t epsilon: 0.4455 \t cumulative reward:  7.9\n","step: 6105 \t epsilon: 0.4451 \t cumulative reward:  4.0\n","step: 6110 \t epsilon: 0.4446 \t cumulative reward:  4.1\n","step: 6115 \t epsilon: 0.4442 \t cumulative reward:  4.0\n","step: 6120 \t epsilon: 0.4437 \t cumulative reward:  2.1\n","step: 6125 \t epsilon: 0.4433 \t cumulative reward: -0.4\n","step: 6130 \t epsilon: 0.4428 \t cumulative reward:  4.1\n","step: 6135 \t epsilon: 0.4424 \t cumulative reward:  4.1\n","step: 6140 \t epsilon: 0.4419 \t cumulative reward:  7.9\n","step: 6145 \t epsilon: 0.4415 \t cumulative reward:  1.5\n","step: 6150 \t epsilon: 0.4410 \t cumulative reward:  4.1\n","step: 6155 \t epsilon: 0.4405 \t cumulative reward: -0.6\n","step: 6160 \t epsilon: 0.4401 \t cumulative reward:  3.9\n","step: 6165 \t epsilon: 0.4396 \t cumulative reward:  1.8\n","step: 6170 \t epsilon: 0.4392 \t cumulative reward:  1.8\n","step: 6175 \t epsilon: 0.4387 \t cumulative reward: -0.1\n","step: 6180 \t epsilon: 0.4383 \t cumulative reward:  4.2\n","step: 6185 \t epsilon: 0.4378 \t cumulative reward:  4.0\n","step: 6190 \t epsilon: 0.4374 \t cumulative reward:  1.9\n","step: 6195 \t epsilon: 0.4369 \t cumulative reward:  4.0\n","step: 6200 \t epsilon: 0.4365 \t cumulative reward:  4.0\n","step: 6205 \t epsilon: 0.4360 \t cumulative reward:  0.2\n","step: 6210 \t epsilon: 0.4355 \t cumulative reward:  5.9\n","step: 6215 \t epsilon: 0.4351 \t cumulative reward:  3.9\n","step: 6220 \t epsilon: 0.4346 \t cumulative reward:  7.8\n","step: 6225 \t epsilon: 0.4342 \t cumulative reward:  7.9\n","step: 6230 \t epsilon: 0.4337 \t cumulative reward:  2.1\n","step: 6235 \t epsilon: 0.4333 \t cumulative reward:  3.7\n","step: 6240 \t epsilon: 0.4328 \t cumulative reward: -2.3\n","step: 6245 \t epsilon: 0.4324 \t cumulative reward:  4.0\n","step: 6250 \t epsilon: 0.4319 \t cumulative reward: -2.0\n","step: 6255 \t epsilon: 0.4315 \t cumulative reward: -2.3\n","step: 6260 \t epsilon: 0.4310 \t cumulative reward:  3.9\n","step: 6265 \t epsilon: 0.4305 \t cumulative reward:  4.0\n","step: 6270 \t epsilon: 0.4301 \t cumulative reward:  4.1\n","step: 6275 \t epsilon: 0.4296 \t cumulative reward:  1.9\n","step: 6280 \t epsilon: 0.4292 \t cumulative reward:  3.9\n","step: 6285 \t epsilon: 0.4287 \t cumulative reward:  2.1\n","step: 6290 \t epsilon: 0.4283 \t cumulative reward: -2.1\n","step: 6295 \t epsilon: 0.4278 \t cumulative reward:  7.9\n","step: 6300 \t epsilon: 0.4274 \t cumulative reward:  4.1\n","step: 6305 \t epsilon: 0.4269 \t cumulative reward: -0.1\n","step: 6310 \t epsilon: 0.4265 \t cumulative reward: -0.2\n","step: 6315 \t epsilon: 0.4260 \t cumulative reward:  3.9\n","step: 6320 \t epsilon: 0.4255 \t cumulative reward:  2.1\n","step: 6325 \t epsilon: 0.4251 \t cumulative reward:  5.9\n","step: 6330 \t epsilon: 0.4246 \t cumulative reward:  6.1\n","step: 6335 \t epsilon: 0.4242 \t cumulative reward:  8.1\n","step: 6340 \t epsilon: 0.4237 \t cumulative reward:  3.9\n","step: 6345 \t epsilon: 0.4233 \t cumulative reward:  5.9\n","step: 6350 \t epsilon: 0.4228 \t cumulative reward:  2.1\n","step: 6355 \t epsilon: 0.4224 \t cumulative reward:  6.0\n","step: 6360 \t epsilon: 0.4219 \t cumulative reward:  5.8\n","step: 6365 \t epsilon: 0.4215 \t cumulative reward:  1.7\n","step: 6370 \t epsilon: 0.4210 \t cumulative reward:  4.0\n","step: 6375 \t epsilon: 0.4205 \t cumulative reward:  2.1\n","step: 6380 \t epsilon: 0.4201 \t cumulative reward:  7.9\n","step: 6385 \t epsilon: 0.4196 \t cumulative reward:  3.9\n","step: 6390 \t epsilon: 0.4192 \t cumulative reward:  7.8\n","step: 6395 \t epsilon: 0.4187 \t cumulative reward:  7.8\n","step: 6400 \t epsilon: 0.4183 \t cumulative reward:  3.9\n","step: 6405 \t epsilon: 0.4178 \t cumulative reward: -0.3\n","step: 6410 \t epsilon: 0.4174 \t cumulative reward:  2.1\n","step: 6415 \t epsilon: 0.4169 \t cumulative reward:  6.1\n","step: 6420 \t epsilon: 0.4165 \t cumulative reward:  4.0\n","step: 6425 \t epsilon: 0.4160 \t cumulative reward:  4.0\n","step: 6430 \t epsilon: 0.4155 \t cumulative reward:  3.7\n","step: 6435 \t epsilon: 0.4151 \t cumulative reward:  7.9\n","step: 6440 \t epsilon: 0.4146 \t cumulative reward:  4.0\n","step: 6445 \t epsilon: 0.4142 \t cumulative reward:  7.9\n","step: 6450 \t epsilon: 0.4137 \t cumulative reward: -0.5\n","step: 6455 \t epsilon: 0.4133 \t cumulative reward: -2.3\n","step: 6460 \t epsilon: 0.4128 \t cumulative reward:  4.1\n","step: 6465 \t epsilon: 0.4124 \t cumulative reward: -1.9\n","step: 6470 \t epsilon: 0.4119 \t cumulative reward:  6.1\n","step: 6475 \t epsilon: 0.4115 \t cumulative reward:  4.1\n","step: 6480 \t epsilon: 0.4110 \t cumulative reward:  4.0\n","step: 6485 \t epsilon: 0.4105 \t cumulative reward:  3.7\n","step: 6490 \t epsilon: 0.4101 \t cumulative reward:  3.4\n","step: 6495 \t epsilon: 0.4096 \t cumulative reward:  4.0\n","step: 6500 \t epsilon: 0.4092 \t cumulative reward:  3.7\n","step: 6505 \t epsilon: 0.4087 \t cumulative reward:  1.8\n","step: 6510 \t epsilon: 0.4083 \t cumulative reward:  2.0\n","step: 6515 \t epsilon: 0.4078 \t cumulative reward:  1.9\n","step: 6520 \t epsilon: 0.4074 \t cumulative reward:  8.0\n","step: 6525 \t epsilon: 0.4069 \t cumulative reward:  7.8\n","step: 6530 \t epsilon: 0.4065 \t cumulative reward:  5.9\n","step: 6535 \t epsilon: 0.4060 \t cumulative reward:  5.8\n","step: 6540 \t epsilon: 0.4055 \t cumulative reward:  8.1\n","step: 6545 \t epsilon: 0.4051 \t cumulative reward:  3.9\n","step: 6550 \t epsilon: 0.4046 \t cumulative reward:  3.9\n","step: 6555 \t epsilon: 0.4042 \t cumulative reward:  4.1\n","step: 6560 \t epsilon: 0.4037 \t cumulative reward:  8.0\n","step: 6565 \t epsilon: 0.4033 \t cumulative reward:  2.1\n","step: 6570 \t epsilon: 0.4028 \t cumulative reward:  3.8\n","step: 6575 \t epsilon: 0.4024 \t cumulative reward:  8.0\n","step: 6580 \t epsilon: 0.4019 \t cumulative reward:  4.0\n","step: 6585 \t epsilon: 0.4015 \t cumulative reward: -1.9\n","step: 6590 \t epsilon: 0.4010 \t cumulative reward:  7.9\n","step: 6595 \t epsilon: 0.4005 \t cumulative reward:  1.9\n","step: 6600 \t epsilon: 0.4001 \t cumulative reward:  3.8\n","step: 6605 \t epsilon: 0.3996 \t cumulative reward:  6.1\n","step: 6610 \t epsilon: 0.3992 \t cumulative reward:  8.0\n","step: 6615 \t epsilon: 0.3987 \t cumulative reward:  4.0\n","step: 6620 \t epsilon: 0.3983 \t cumulative reward:  5.8\n","step: 6625 \t epsilon: 0.3978 \t cumulative reward:  3.5\n","step: 6630 \t epsilon: 0.3974 \t cumulative reward:  2.1\n","step: 6635 \t epsilon: 0.3969 \t cumulative reward:  4.2\n","step: 6640 \t epsilon: 0.3965 \t cumulative reward:  0.0\n","step: 6645 \t epsilon: 0.3960 \t cumulative reward:  7.8\n","step: 6650 \t epsilon: 0.3955 \t cumulative reward:  4.0\n","step: 6655 \t epsilon: 0.3951 \t cumulative reward:  0.1\n","step: 6660 \t epsilon: 0.3946 \t cumulative reward:  8.0\n","step: 6665 \t epsilon: 0.3942 \t cumulative reward:  7.9\n","step: 6670 \t epsilon: 0.3937 \t cumulative reward:  2.1\n","step: 6675 \t epsilon: 0.3933 \t cumulative reward: -0.1\n","step: 6680 \t epsilon: 0.3928 \t cumulative reward:  2.1\n","step: 6685 \t epsilon: 0.3924 \t cumulative reward:  7.8\n","step: 6690 \t epsilon: 0.3919 \t cumulative reward:  3.7\n","step: 6695 \t epsilon: 0.3915 \t cumulative reward:  2.0\n","step: 6700 \t epsilon: 0.3910 \t cumulative reward:  8.0\n","step: 6705 \t epsilon: 0.3905 \t cumulative reward:  7.8\n","step: 6710 \t epsilon: 0.3901 \t cumulative reward:  7.8\n","step: 6715 \t epsilon: 0.3896 \t cumulative reward:  8.0\n","step: 6720 \t epsilon: 0.3892 \t cumulative reward:  4.0\n","step: 6725 \t epsilon: 0.3887 \t cumulative reward:  7.8\n","step: 6730 \t epsilon: 0.3883 \t cumulative reward:  3.5\n","step: 6735 \t epsilon: 0.3878 \t cumulative reward:  0.0\n","step: 6740 \t epsilon: 0.3874 \t cumulative reward:  1.9\n","step: 6745 \t epsilon: 0.3869 \t cumulative reward:  3.9\n","step: 6750 \t epsilon: 0.3865 \t cumulative reward:  7.8\n","step: 6755 \t epsilon: 0.3860 \t cumulative reward:  8.0\n","step: 6760 \t epsilon: 0.3855 \t cumulative reward:  7.8\n","step: 6765 \t epsilon: 0.3851 \t cumulative reward:  4.0\n","step: 6770 \t epsilon: 0.3846 \t cumulative reward:  3.7\n","step: 6775 \t epsilon: 0.3842 \t cumulative reward:  3.5\n","step: 6780 \t epsilon: 0.3837 \t cumulative reward:  6.1\n","step: 6785 \t epsilon: 0.3833 \t cumulative reward:  8.1\n","step: 6790 \t epsilon: 0.3828 \t cumulative reward:  3.9\n","step: 6795 \t epsilon: 0.3824 \t cumulative reward:  8.0\n","step: 6800 \t epsilon: 0.3819 \t cumulative reward:  8.0\n","step: 6805 \t epsilon: 0.3815 \t cumulative reward:  3.9\n","step: 6810 \t epsilon: 0.3810 \t cumulative reward:  2.1\n","step: 6815 \t epsilon: 0.3805 \t cumulative reward:  4.1\n","step: 6820 \t epsilon: 0.3801 \t cumulative reward:  8.1\n","step: 6825 \t epsilon: 0.3796 \t cumulative reward:  8.1\n","step: 6830 \t epsilon: 0.3792 \t cumulative reward:  3.7\n","step: 6835 \t epsilon: 0.3787 \t cumulative reward:  7.9\n","step: 6840 \t epsilon: 0.3783 \t cumulative reward:  4.0\n","step: 6845 \t epsilon: 0.3778 \t cumulative reward:  3.6\n","step: 6850 \t epsilon: 0.3774 \t cumulative reward:  5.8\n","step: 6855 \t epsilon: 0.3769 \t cumulative reward:  3.9\n","step: 6860 \t epsilon: 0.3765 \t cumulative reward:  1.8\n","step: 6865 \t epsilon: 0.3760 \t cumulative reward:  7.9\n","step: 6870 \t epsilon: 0.3755 \t cumulative reward:  8.1\n","step: 6875 \t epsilon: 0.3751 \t cumulative reward:  8.0\n","step: 6880 \t epsilon: 0.3746 \t cumulative reward:  8.1\n","step: 6885 \t epsilon: 0.3742 \t cumulative reward:  8.1\n","step: 6890 \t epsilon: 0.3737 \t cumulative reward:  4.1\n","step: 6895 \t epsilon: 0.3733 \t cumulative reward:  4.1\n","step: 6900 \t epsilon: 0.3728 \t cumulative reward:  8.0\n","step: 6905 \t epsilon: 0.3724 \t cumulative reward:  8.0\n","step: 6910 \t epsilon: 0.3719 \t cumulative reward:  2.1\n","step: 6915 \t epsilon: 0.3715 \t cumulative reward:  8.0\n","step: 6920 \t epsilon: 0.3710 \t cumulative reward:  2.1\n","step: 6925 \t epsilon: 0.3705 \t cumulative reward:  7.8\n","step: 6930 \t epsilon: 0.3701 \t cumulative reward:  8.1\n","step: 6935 \t epsilon: 0.3696 \t cumulative reward:  8.0\n","step: 6940 \t epsilon: 0.3692 \t cumulative reward:  2.0\n","step: 6945 \t epsilon: 0.3687 \t cumulative reward:  3.9\n","step: 6950 \t epsilon: 0.3683 \t cumulative reward:  8.0\n","step: 6955 \t epsilon: 0.3678 \t cumulative reward:  2.0\n","step: 6960 \t epsilon: 0.3674 \t cumulative reward:  8.1\n","step: 6965 \t epsilon: 0.3669 \t cumulative reward:  8.1\n","step: 6970 \t epsilon: 0.3665 \t cumulative reward:  2.1\n","step: 6975 \t epsilon: 0.3660 \t cumulative reward:  3.8\n","step: 6980 \t epsilon: 0.3655 \t cumulative reward:  8.1\n","step: 6985 \t epsilon: 0.3651 \t cumulative reward:  3.9\n","step: 6990 \t epsilon: 0.3646 \t cumulative reward:  8.1\n","step: 6995 \t epsilon: 0.3642 \t cumulative reward:  8.1\n","step: 7000 \t epsilon: 0.3637 \t cumulative reward:  4.1\n","step: 7005 \t epsilon: 0.3633 \t cumulative reward:  3.7\n","step: 7010 \t epsilon: 0.3628 \t cumulative reward:  8.0\n","step: 7015 \t epsilon: 0.3624 \t cumulative reward:  7.8\n","step: 7020 \t epsilon: 0.3619 \t cumulative reward:  2.0\n","step: 7025 \t epsilon: 0.3615 \t cumulative reward:  5.8\n","step: 7030 \t epsilon: 0.3610 \t cumulative reward:  8.0\n","step: 7035 \t epsilon: 0.3605 \t cumulative reward:  5.8\n","step: 7040 \t epsilon: 0.3601 \t cumulative reward:  3.8\n","step: 7045 \t epsilon: 0.3596 \t cumulative reward:  3.9\n","step: 7050 \t epsilon: 0.3592 \t cumulative reward:  8.1\n","step: 7055 \t epsilon: 0.3587 \t cumulative reward:  7.9\n","step: 7060 \t epsilon: 0.3583 \t cumulative reward:  7.8\n","step: 7065 \t epsilon: 0.3578 \t cumulative reward:  4.0\n","step: 7070 \t epsilon: 0.3574 \t cumulative reward:  3.8\n","step: 7075 \t epsilon: 0.3569 \t cumulative reward:  5.9\n","step: 7080 \t epsilon: 0.3565 \t cumulative reward:  1.8\n","step: 7085 \t epsilon: 0.3560 \t cumulative reward:  8.0\n","step: 7090 \t epsilon: 0.3555 \t cumulative reward:  8.1\n","step: 7095 \t epsilon: 0.3551 \t cumulative reward:  3.7\n","step: 7100 \t epsilon: 0.3546 \t cumulative reward:  2.0\n","step: 7105 \t epsilon: 0.3542 \t cumulative reward:  6.0\n","step: 7110 \t epsilon: 0.3537 \t cumulative reward:  4.0\n","step: 7115 \t epsilon: 0.3533 \t cumulative reward:  4.0\n","step: 7120 \t epsilon: 0.3528 \t cumulative reward: -1.9\n","step: 7125 \t epsilon: 0.3524 \t cumulative reward:  7.9\n","step: 7130 \t epsilon: 0.3519 \t cumulative reward:  3.9\n","step: 7135 \t epsilon: 0.3515 \t cumulative reward:  0.1\n","step: 7140 \t epsilon: 0.3510 \t cumulative reward:  6.1\n","step: 7145 \t epsilon: 0.3505 \t cumulative reward:  4.1\n","step: 7150 \t epsilon: 0.3501 \t cumulative reward:  7.8\n","step: 7155 \t epsilon: 0.3496 \t cumulative reward:  8.1\n","step: 7160 \t epsilon: 0.3492 \t cumulative reward:  7.8\n","step: 7165 \t epsilon: 0.3487 \t cumulative reward:  7.8\n","step: 7170 \t epsilon: 0.3483 \t cumulative reward:  8.1\n","step: 7175 \t epsilon: 0.3478 \t cumulative reward:  8.0\n","step: 7180 \t epsilon: 0.3474 \t cumulative reward:  7.8\n","step: 7185 \t epsilon: 0.3469 \t cumulative reward:  3.8\n","step: 7190 \t epsilon: 0.3465 \t cumulative reward:  8.1\n","step: 7195 \t epsilon: 0.3460 \t cumulative reward: -2.2\n","step: 7200 \t epsilon: 0.3455 \t cumulative reward:  8.0\n","step: 7205 \t epsilon: 0.3451 \t cumulative reward:  8.1\n","step: 7210 \t epsilon: 0.3446 \t cumulative reward:  4.2\n","step: 7215 \t epsilon: 0.3442 \t cumulative reward:  2.0\n","step: 7220 \t epsilon: 0.3437 \t cumulative reward:  2.1\n","step: 7225 \t epsilon: 0.3433 \t cumulative reward:  4.0\n","step: 7230 \t epsilon: 0.3428 \t cumulative reward:  7.8\n","step: 7235 \t epsilon: 0.3424 \t cumulative reward:  8.1\n","step: 7240 \t epsilon: 0.3419 \t cumulative reward:  3.9\n","step: 7245 \t epsilon: 0.3415 \t cumulative reward:  8.0\n","step: 7250 \t epsilon: 0.3410 \t cumulative reward:  3.6\n","step: 7255 \t epsilon: 0.3405 \t cumulative reward:  4.0\n","step: 7260 \t epsilon: 0.3401 \t cumulative reward:  4.2\n","step: 7265 \t epsilon: 0.3396 \t cumulative reward:  0.0\n","step: 7270 \t epsilon: 0.3392 \t cumulative reward:  0.0\n","step: 7275 \t epsilon: 0.3387 \t cumulative reward:  8.1\n","step: 7280 \t epsilon: 0.3383 \t cumulative reward:  3.9\n","step: 7285 \t epsilon: 0.3378 \t cumulative reward:  8.1\n","step: 7290 \t epsilon: 0.3374 \t cumulative reward:  7.8\n","step: 7295 \t epsilon: 0.3369 \t cumulative reward:  7.8\n","step: 7300 \t epsilon: 0.3365 \t cumulative reward:  7.9\n","step: 7305 \t epsilon: 0.3360 \t cumulative reward:  6.1\n","step: 7310 \t epsilon: 0.3355 \t cumulative reward:  8.1\n","step: 7315 \t epsilon: 0.3351 \t cumulative reward:  8.1\n","step: 7320 \t epsilon: 0.3346 \t cumulative reward:  2.1\n","step: 7325 \t epsilon: 0.3342 \t cumulative reward:  8.1\n","step: 7330 \t epsilon: 0.3337 \t cumulative reward:  3.8\n","step: 7335 \t epsilon: 0.3333 \t cumulative reward:  8.0\n","step: 7340 \t epsilon: 0.3328 \t cumulative reward:  7.9\n","step: 7345 \t epsilon: 0.3324 \t cumulative reward:  1.8\n","step: 7350 \t epsilon: 0.3319 \t cumulative reward:  8.1\n","step: 7355 \t epsilon: 0.3315 \t cumulative reward:  4.1\n","step: 7360 \t epsilon: 0.3310 \t cumulative reward: -1.7\n","step: 7365 \t epsilon: 0.3305 \t cumulative reward:  7.8\n","step: 7370 \t epsilon: 0.3301 \t cumulative reward:  3.7\n","step: 7375 \t epsilon: 0.3296 \t cumulative reward:  8.0\n","step: 7380 \t epsilon: 0.3292 \t cumulative reward:  8.0\n","step: 7385 \t epsilon: 0.3287 \t cumulative reward:  7.9\n","step: 7390 \t epsilon: 0.3283 \t cumulative reward:  7.8\n","step: 7395 \t epsilon: 0.3278 \t cumulative reward:  8.1\n","step: 7400 \t epsilon: 0.3274 \t cumulative reward:  8.1\n","step: 7405 \t epsilon: 0.3269 \t cumulative reward:  6.0\n","step: 7410 \t epsilon: 0.3265 \t cumulative reward:  8.1\n","step: 7415 \t epsilon: 0.3260 \t cumulative reward:  8.1\n","step: 7420 \t epsilon: 0.3255 \t cumulative reward:  1.9\n","step: 7425 \t epsilon: 0.3251 \t cumulative reward:  3.9\n","step: 7430 \t epsilon: 0.3246 \t cumulative reward:  3.8\n","step: 7435 \t epsilon: 0.3242 \t cumulative reward:  8.1\n","step: 7440 \t epsilon: 0.3237 \t cumulative reward:  3.7\n","step: 7445 \t epsilon: 0.3233 \t cumulative reward:  7.8\n","step: 7450 \t epsilon: 0.3228 \t cumulative reward:  7.9\n","step: 7455 \t epsilon: 0.3224 \t cumulative reward:  4.0\n","step: 7460 \t epsilon: 0.3219 \t cumulative reward:  4.2\n","step: 7465 \t epsilon: 0.3215 \t cumulative reward:  8.1\n","step: 7470 \t epsilon: 0.3210 \t cumulative reward:  3.9\n","step: 7475 \t epsilon: 0.3205 \t cumulative reward:  8.1\n","step: 7480 \t epsilon: 0.3201 \t cumulative reward:  8.0\n","step: 7485 \t epsilon: 0.3196 \t cumulative reward:  7.8\n","step: 7490 \t epsilon: 0.3192 \t cumulative reward:  8.0\n","step: 7495 \t epsilon: 0.3187 \t cumulative reward:  4.1\n","step: 7500 \t epsilon: 0.3183 \t cumulative reward:  7.8\n","step: 7505 \t epsilon: 0.3178 \t cumulative reward:  4.1\n","step: 7510 \t epsilon: 0.3174 \t cumulative reward:  6.1\n","step: 7515 \t epsilon: 0.3169 \t cumulative reward:  6.1\n","step: 7520 \t epsilon: 0.3165 \t cumulative reward:  2.1\n","step: 7525 \t epsilon: 0.3160 \t cumulative reward:  7.8\n","step: 7530 \t epsilon: 0.3155 \t cumulative reward:  8.0\n","step: 7535 \t epsilon: 0.3151 \t cumulative reward:  8.0\n","step: 7540 \t epsilon: 0.3146 \t cumulative reward:  8.1\n","step: 7545 \t epsilon: 0.3142 \t cumulative reward:  7.8\n","step: 7550 \t epsilon: 0.3137 \t cumulative reward:  7.9\n","step: 7555 \t epsilon: 0.3133 \t cumulative reward:  6.1\n","step: 7560 \t epsilon: 0.3128 \t cumulative reward:  8.1\n","step: 7565 \t epsilon: 0.3124 \t cumulative reward:  3.7\n","step: 7570 \t epsilon: 0.3119 \t cumulative reward:  7.8\n","step: 7575 \t epsilon: 0.3115 \t cumulative reward:  3.8\n","step: 7580 \t epsilon: 0.3110 \t cumulative reward:  3.8\n","step: 7585 \t epsilon: 0.3105 \t cumulative reward:  5.9\n","step: 7590 \t epsilon: 0.3101 \t cumulative reward:  7.8\n","step: 7595 \t epsilon: 0.3096 \t cumulative reward:  6.1\n","step: 7600 \t epsilon: 0.3092 \t cumulative reward:  3.8\n","step: 7605 \t epsilon: 0.3087 \t cumulative reward:  1.9\n","step: 7610 \t epsilon: 0.3083 \t cumulative reward:  3.7\n","step: 7615 \t epsilon: 0.3078 \t cumulative reward:  8.0\n","step: 7620 \t epsilon: 0.3074 \t cumulative reward:  4.1\n","step: 7625 \t epsilon: 0.3069 \t cumulative reward:  4.0\n","step: 7630 \t epsilon: 0.3065 \t cumulative reward:  7.9\n","step: 7635 \t epsilon: 0.3060 \t cumulative reward:  4.0\n","step: 7640 \t epsilon: 0.3055 \t cumulative reward:  8.0\n","step: 7645 \t epsilon: 0.3051 \t cumulative reward:  2.1\n","step: 7650 \t epsilon: 0.3046 \t cumulative reward:  7.8\n","step: 7655 \t epsilon: 0.3042 \t cumulative reward:  3.8\n","step: 7660 \t epsilon: 0.3037 \t cumulative reward:  8.1\n","step: 7665 \t epsilon: 0.3033 \t cumulative reward:  6.1\n","step: 7670 \t epsilon: 0.3028 \t cumulative reward:  8.1\n","step: 7675 \t epsilon: 0.3024 \t cumulative reward:  7.8\n","step: 7680 \t epsilon: 0.3019 \t cumulative reward:  3.7\n","step: 7685 \t epsilon: 0.3015 \t cumulative reward:  7.8\n","step: 7690 \t epsilon: 0.3010 \t cumulative reward:  6.1\n","step: 7695 \t epsilon: 0.3005 \t cumulative reward:  7.8\n","step: 7700 \t epsilon: 0.3001 \t cumulative reward:  2.0\n","step: 7705 \t epsilon: 0.2996 \t cumulative reward:  4.0\n","step: 7710 \t epsilon: 0.2992 \t cumulative reward:  8.1\n","step: 7715 \t epsilon: 0.2987 \t cumulative reward:  8.1\n","step: 7720 \t epsilon: 0.2983 \t cumulative reward:  6.1\n","step: 7725 \t epsilon: 0.2978 \t cumulative reward:  4.2\n","step: 7730 \t epsilon: 0.2974 \t cumulative reward:  6.1\n","step: 7735 \t epsilon: 0.2969 \t cumulative reward:  7.8\n","step: 7740 \t epsilon: 0.2965 \t cumulative reward:  8.1\n","step: 7745 \t epsilon: 0.2960 \t cumulative reward:  8.0\n","step: 7750 \t epsilon: 0.2955 \t cumulative reward:  8.0\n","step: 7755 \t epsilon: 0.2951 \t cumulative reward:  2.0\n","step: 7760 \t epsilon: 0.2946 \t cumulative reward:  7.8\n","step: 7765 \t epsilon: 0.2942 \t cumulative reward:  8.0\n","step: 7770 \t epsilon: 0.2937 \t cumulative reward:  2.1\n","step: 7775 \t epsilon: 0.2933 \t cumulative reward:  3.8\n","step: 7780 \t epsilon: 0.2928 \t cumulative reward:  7.9\n","step: 7785 \t epsilon: 0.2924 \t cumulative reward:  8.1\n","step: 7790 \t epsilon: 0.2919 \t cumulative reward:  5.9\n","step: 7795 \t epsilon: 0.2915 \t cumulative reward:  1.8\n","step: 7800 \t epsilon: 0.2910 \t cumulative reward:  7.8\n","step: 7805 \t epsilon: 0.2905 \t cumulative reward:  8.1\n","step: 7810 \t epsilon: 0.2901 \t cumulative reward:  7.8\n","step: 7815 \t epsilon: 0.2896 \t cumulative reward:  8.1\n","step: 7820 \t epsilon: 0.2892 \t cumulative reward:  7.8\n","step: 7825 \t epsilon: 0.2887 \t cumulative reward:  1.7\n","step: 7830 \t epsilon: 0.2883 \t cumulative reward:  8.1\n","step: 7835 \t epsilon: 0.2878 \t cumulative reward:  2.3\n","step: 7840 \t epsilon: 0.2874 \t cumulative reward:  8.1\n","step: 7845 \t epsilon: 0.2869 \t cumulative reward:  5.9\n","step: 7850 \t epsilon: 0.2865 \t cumulative reward:  7.8\n","step: 7855 \t epsilon: 0.2860 \t cumulative reward:  8.0\n","step: 7860 \t epsilon: 0.2855 \t cumulative reward:  2.1\n","step: 7865 \t epsilon: 0.2851 \t cumulative reward:  8.1\n","step: 7870 \t epsilon: 0.2846 \t cumulative reward:  8.1\n","step: 7875 \t epsilon: 0.2842 \t cumulative reward:  7.8\n","step: 7880 \t epsilon: 0.2837 \t cumulative reward:  7.9\n","step: 7885 \t epsilon: 0.2833 \t cumulative reward:  8.0\n","step: 7890 \t epsilon: 0.2828 \t cumulative reward:  4.2\n","step: 7895 \t epsilon: 0.2824 \t cumulative reward:  8.1\n","step: 7900 \t epsilon: 0.2819 \t cumulative reward:  2.1\n","step: 7905 \t epsilon: 0.2815 \t cumulative reward:  3.8\n","step: 7910 \t epsilon: 0.2810 \t cumulative reward:  4.0\n","step: 7915 \t epsilon: 0.2805 \t cumulative reward:  7.9\n","step: 7920 \t epsilon: 0.2801 \t cumulative reward:  2.1\n","step: 7925 \t epsilon: 0.2796 \t cumulative reward:  8.1\n","step: 7930 \t epsilon: 0.2792 \t cumulative reward:  7.8\n","step: 7935 \t epsilon: 0.2787 \t cumulative reward:  8.1\n","step: 7940 \t epsilon: 0.2783 \t cumulative reward:  3.6\n","step: 7945 \t epsilon: 0.2778 \t cumulative reward:  8.1\n","step: 7950 \t epsilon: 0.2774 \t cumulative reward:  4.2\n","step: 7955 \t epsilon: 0.2769 \t cumulative reward:  3.9\n","step: 7960 \t epsilon: 0.2765 \t cumulative reward:  2.1\n","step: 7965 \t epsilon: 0.2760 \t cumulative reward:  8.1\n","step: 7970 \t epsilon: 0.2755 \t cumulative reward:  8.0\n","step: 7975 \t epsilon: 0.2751 \t cumulative reward:  2.3\n","step: 7980 \t epsilon: 0.2746 \t cumulative reward:  3.7\n","step: 7985 \t epsilon: 0.2742 \t cumulative reward:  7.9\n","step: 7990 \t epsilon: 0.2737 \t cumulative reward:  7.8\n","step: 7995 \t epsilon: 0.2733 \t cumulative reward:  4.1\n","step: 8000 \t epsilon: 0.2728 \t cumulative reward:  2.2\n","step: 8005 \t epsilon: 0.2724 \t cumulative reward:  2.3\n","step: 8010 \t epsilon: 0.2719 \t cumulative reward:  8.0\n","step: 8015 \t epsilon: 0.2715 \t cumulative reward:  2.1\n","step: 8020 \t epsilon: 0.2710 \t cumulative reward:  2.1\n","step: 8025 \t epsilon: 0.2705 \t cumulative reward:  6.0\n","step: 8030 \t epsilon: 0.2701 \t cumulative reward:  3.5\n","step: 8035 \t epsilon: 0.2696 \t cumulative reward:  8.1\n","step: 8040 \t epsilon: 0.2692 \t cumulative reward:  8.1\n","step: 8045 \t epsilon: 0.2687 \t cumulative reward:  8.1\n","step: 8050 \t epsilon: 0.2683 \t cumulative reward:  8.1\n","step: 8055 \t epsilon: 0.2678 \t cumulative reward:  7.8\n","step: 8060 \t epsilon: 0.2674 \t cumulative reward:  7.9\n","step: 8065 \t epsilon: 0.2669 \t cumulative reward:  5.9\n","step: 8070 \t epsilon: 0.2665 \t cumulative reward:  3.8\n","step: 8075 \t epsilon: 0.2660 \t cumulative reward:  7.8\n","step: 8080 \t epsilon: 0.2655 \t cumulative reward:  8.1\n","step: 8085 \t epsilon: 0.2651 \t cumulative reward:  4.1\n","step: 8090 \t epsilon: 0.2646 \t cumulative reward:  7.9\n","step: 8095 \t epsilon: 0.2642 \t cumulative reward:  7.8\n","step: 8100 \t epsilon: 0.2637 \t cumulative reward:  4.0\n","step: 8105 \t epsilon: 0.2633 \t cumulative reward:  8.0\n","step: 8110 \t epsilon: 0.2628 \t cumulative reward:  4.0\n","step: 8115 \t epsilon: 0.2624 \t cumulative reward:  3.8\n","step: 8120 \t epsilon: 0.2619 \t cumulative reward:  4.0\n","step: 8125 \t epsilon: 0.2615 \t cumulative reward:  7.8\n","step: 8130 \t epsilon: 0.2610 \t cumulative reward:  8.1\n","step: 8135 \t epsilon: 0.2605 \t cumulative reward:  8.0\n","step: 8140 \t epsilon: 0.2601 \t cumulative reward:  8.1\n","step: 8145 \t epsilon: 0.2596 \t cumulative reward:  7.9\n","step: 8150 \t epsilon: 0.2592 \t cumulative reward:  7.8\n","step: 8155 \t epsilon: 0.2587 \t cumulative reward:  8.1\n","step: 8160 \t epsilon: 0.2583 \t cumulative reward:  4.1\n","step: 8165 \t epsilon: 0.2578 \t cumulative reward:  4.1\n","step: 8170 \t epsilon: 0.2574 \t cumulative reward:  7.8\n","step: 8175 \t epsilon: 0.2569 \t cumulative reward:  8.1\n","step: 8180 \t epsilon: 0.2565 \t cumulative reward:  8.1\n","step: 8185 \t epsilon: 0.2560 \t cumulative reward:  8.0\n","step: 8190 \t epsilon: 0.2555 \t cumulative reward:  7.8\n","step: 8195 \t epsilon: 0.2551 \t cumulative reward:  3.8\n","step: 8200 \t epsilon: 0.2546 \t cumulative reward:  4.1\n","step: 8205 \t epsilon: 0.2542 \t cumulative reward:  8.1\n","step: 8210 \t epsilon: 0.2537 \t cumulative reward:  6.1\n","step: 8215 \t epsilon: 0.2533 \t cumulative reward:  7.8\n","step: 8220 \t epsilon: 0.2528 \t cumulative reward:  8.1\n","step: 8225 \t epsilon: 0.2524 \t cumulative reward:  7.8\n","step: 8230 \t epsilon: 0.2519 \t cumulative reward:  4.0\n","step: 8235 \t epsilon: 0.2515 \t cumulative reward:  8.1\n","step: 8240 \t epsilon: 0.2510 \t cumulative reward:  1.9\n","step: 8245 \t epsilon: 0.2505 \t cumulative reward:  8.1\n","step: 8250 \t epsilon: 0.2501 \t cumulative reward:  2.0\n","step: 8255 \t epsilon: 0.2496 \t cumulative reward:  7.8\n","step: 8260 \t epsilon: 0.2492 \t cumulative reward:  7.8\n","step: 8265 \t epsilon: 0.2487 \t cumulative reward:  3.9\n","step: 8270 \t epsilon: 0.2483 \t cumulative reward:  7.8\n","step: 8275 \t epsilon: 0.2478 \t cumulative reward:  8.1\n","step: 8280 \t epsilon: 0.2474 \t cumulative reward:  7.8\n","step: 8285 \t epsilon: 0.2469 \t cumulative reward:  8.1\n","step: 8290 \t epsilon: 0.2465 \t cumulative reward:  7.8\n","step: 8295 \t epsilon: 0.2460 \t cumulative reward:  7.8\n","step: 8300 \t epsilon: 0.2455 \t cumulative reward:  8.1\n","step: 8305 \t epsilon: 0.2451 \t cumulative reward:  1.8\n","step: 8310 \t epsilon: 0.2446 \t cumulative reward:  8.0\n","step: 8315 \t epsilon: 0.2442 \t cumulative reward:  8.0\n","step: 8320 \t epsilon: 0.2437 \t cumulative reward:  1.8\n","step: 8325 \t epsilon: 0.2433 \t cumulative reward:  7.8\n","step: 8330 \t epsilon: 0.2428 \t cumulative reward:  7.9\n","step: 8335 \t epsilon: 0.2424 \t cumulative reward:  7.9\n","step: 8340 \t epsilon: 0.2419 \t cumulative reward:  2.1\n","step: 8345 \t epsilon: 0.2415 \t cumulative reward:  7.8\n","step: 8350 \t epsilon: 0.2410 \t cumulative reward:  8.0\n","step: 8355 \t epsilon: 0.2405 \t cumulative reward:  8.1\n","step: 8360 \t epsilon: 0.2401 \t cumulative reward:  7.9\n","step: 8365 \t epsilon: 0.2396 \t cumulative reward:  8.0\n","step: 8370 \t epsilon: 0.2392 \t cumulative reward:  8.1\n","step: 8375 \t epsilon: 0.2387 \t cumulative reward:  7.8\n","step: 8380 \t epsilon: 0.2383 \t cumulative reward:  7.9\n","step: 8385 \t epsilon: 0.2378 \t cumulative reward:  7.8\n","step: 8390 \t epsilon: 0.2374 \t cumulative reward:  7.9\n","step: 8395 \t epsilon: 0.2369 \t cumulative reward:  5.8\n","step: 8400 \t epsilon: 0.2365 \t cumulative reward:  8.0\n","step: 8405 \t epsilon: 0.2360 \t cumulative reward:  8.0\n","step: 8410 \t epsilon: 0.2355 \t cumulative reward:  7.9\n","step: 8415 \t epsilon: 0.2351 \t cumulative reward:  7.8\n","step: 8420 \t epsilon: 0.2346 \t cumulative reward:  1.8\n","step: 8425 \t epsilon: 0.2342 \t cumulative reward:  8.1\n","step: 8430 \t epsilon: 0.2337 \t cumulative reward:  4.2\n","step: 8435 \t epsilon: 0.2333 \t cumulative reward:  8.0\n","step: 8440 \t epsilon: 0.2328 \t cumulative reward:  3.9\n","step: 8445 \t epsilon: 0.2324 \t cumulative reward:  3.8\n","step: 8450 \t epsilon: 0.2319 \t cumulative reward:  2.1\n","step: 8455 \t epsilon: 0.2315 \t cumulative reward:  7.9\n","step: 8460 \t epsilon: 0.2310 \t cumulative reward:  2.1\n","step: 8465 \t epsilon: 0.2305 \t cumulative reward:  8.1\n","step: 8470 \t epsilon: 0.2301 \t cumulative reward:  3.8\n","step: 8475 \t epsilon: 0.2296 \t cumulative reward:  2.0\n","step: 8480 \t epsilon: 0.2292 \t cumulative reward:  7.8\n","step: 8485 \t epsilon: 0.2287 \t cumulative reward:  7.8\n","step: 8490 \t epsilon: 0.2283 \t cumulative reward:  4.0\n","step: 8495 \t epsilon: 0.2278 \t cumulative reward:  7.8\n","step: 8500 \t epsilon: 0.2274 \t cumulative reward:  8.1\n","step: 8505 \t epsilon: 0.2269 \t cumulative reward:  2.0\n","step: 8510 \t epsilon: 0.2265 \t cumulative reward:  3.9\n","step: 8515 \t epsilon: 0.2260 \t cumulative reward:  2.0\n","step: 8520 \t epsilon: 0.2255 \t cumulative reward:  6.0\n","step: 8525 \t epsilon: 0.2251 \t cumulative reward: -2.1\n","step: 8530 \t epsilon: 0.2246 \t cumulative reward: -1.8\n","step: 8535 \t epsilon: 0.2242 \t cumulative reward:  4.0\n","step: 8540 \t epsilon: 0.2237 \t cumulative reward:  7.8\n","step: 8545 \t epsilon: 0.2233 \t cumulative reward:  8.0\n","step: 8550 \t epsilon: 0.2228 \t cumulative reward:  8.0\n","step: 8555 \t epsilon: 0.2224 \t cumulative reward:  8.0\n","step: 8560 \t epsilon: 0.2219 \t cumulative reward:  7.9\n","step: 8565 \t epsilon: 0.2215 \t cumulative reward:  8.0\n","step: 8570 \t epsilon: 0.2210 \t cumulative reward:  7.9\n","step: 8575 \t epsilon: 0.2205 \t cumulative reward:  6.0\n","step: 8580 \t epsilon: 0.2201 \t cumulative reward:  8.0\n","step: 8585 \t epsilon: 0.2196 \t cumulative reward:  8.0\n","step: 8590 \t epsilon: 0.2192 \t cumulative reward:  3.9\n","step: 8595 \t epsilon: 0.2187 \t cumulative reward:  8.0\n","step: 8600 \t epsilon: 0.2183 \t cumulative reward:  8.0\n","step: 8605 \t epsilon: 0.2178 \t cumulative reward:  8.0\n","step: 8610 \t epsilon: 0.2174 \t cumulative reward:  8.0\n","step: 8615 \t epsilon: 0.2169 \t cumulative reward:  4.1\n","step: 8620 \t epsilon: 0.2165 \t cumulative reward:  8.0\n","step: 8625 \t epsilon: 0.2160 \t cumulative reward:  1.9\n","step: 8630 \t epsilon: 0.2155 \t cumulative reward:  8.0\n","step: 8635 \t epsilon: 0.2151 \t cumulative reward:  7.8\n","step: 8640 \t epsilon: 0.2146 \t cumulative reward:  0.3\n","step: 8645 \t epsilon: 0.2142 \t cumulative reward:  8.0\n","step: 8650 \t epsilon: 0.2137 \t cumulative reward:  8.1\n","step: 8655 \t epsilon: 0.2133 \t cumulative reward:  8.1\n","step: 8660 \t epsilon: 0.2128 \t cumulative reward:  4.0\n","step: 8665 \t epsilon: 0.2124 \t cumulative reward:  7.8\n","step: 8670 \t epsilon: 0.2119 \t cumulative reward:  2.0\n","step: 8675 \t epsilon: 0.2115 \t cumulative reward:  8.0\n","step: 8680 \t epsilon: 0.2110 \t cumulative reward:  8.1\n","step: 8685 \t epsilon: 0.2105 \t cumulative reward:  8.1\n","step: 8690 \t epsilon: 0.2101 \t cumulative reward:  4.0\n","step: 8695 \t epsilon: 0.2096 \t cumulative reward:  8.0\n","step: 8700 \t epsilon: 0.2092 \t cumulative reward:  6.0\n","step: 8705 \t epsilon: 0.2087 \t cumulative reward:  1.9\n","step: 8710 \t epsilon: 0.2083 \t cumulative reward:  8.0\n","step: 8715 \t epsilon: 0.2078 \t cumulative reward:  8.0\n","step: 8720 \t epsilon: 0.2074 \t cumulative reward:  4.2\n","step: 8725 \t epsilon: 0.2069 \t cumulative reward:  8.0\n","step: 8730 \t epsilon: 0.2065 \t cumulative reward:  2.0\n","step: 8735 \t epsilon: 0.2060 \t cumulative reward:  8.0\n","step: 8740 \t epsilon: 0.2055 \t cumulative reward:  4.1\n","step: 8745 \t epsilon: 0.2051 \t cumulative reward:  8.0\n","step: 8750 \t epsilon: 0.2046 \t cumulative reward:  4.2\n","step: 8755 \t epsilon: 0.2042 \t cumulative reward:  3.9\n","step: 8760 \t epsilon: 0.2037 \t cumulative reward:  4.2\n","step: 8765 \t epsilon: 0.2033 \t cumulative reward:  3.9\n","step: 8770 \t epsilon: 0.2028 \t cumulative reward:  4.2\n","step: 8775 \t epsilon: 0.2024 \t cumulative reward:  6.0\n","step: 8780 \t epsilon: 0.2019 \t cumulative reward:  3.6\n","step: 8785 \t epsilon: 0.2015 \t cumulative reward:  4.2\n","step: 8790 \t epsilon: 0.2010 \t cumulative reward:  3.7\n","step: 8795 \t epsilon: 0.2005 \t cumulative reward:  8.0\n","step: 8800 \t epsilon: 0.2001 \t cumulative reward:  8.0\n","step: 8805 \t epsilon: 0.1996 \t cumulative reward:  6.1\n","step: 8810 \t epsilon: 0.1992 \t cumulative reward:  8.0\n","step: 8815 \t epsilon: 0.1987 \t cumulative reward:  8.0\n","step: 8820 \t epsilon: 0.1983 \t cumulative reward:  4.0\n","step: 8825 \t epsilon: 0.1978 \t cumulative reward:  8.0\n","step: 8830 \t epsilon: 0.1974 \t cumulative reward:  0.0\n","step: 8835 \t epsilon: 0.1969 \t cumulative reward:  8.0\n","step: 8840 \t epsilon: 0.1965 \t cumulative reward:  4.0\n","step: 8845 \t epsilon: 0.1960 \t cumulative reward:  8.0\n","step: 8850 \t epsilon: 0.1955 \t cumulative reward:  4.1\n","step: 8855 \t epsilon: 0.1951 \t cumulative reward:  2.0\n","step: 8860 \t epsilon: 0.1946 \t cumulative reward:  4.1\n","step: 8865 \t epsilon: 0.1942 \t cumulative reward:  8.0\n","step: 8870 \t epsilon: 0.1937 \t cumulative reward:  7.9\n","step: 8875 \t epsilon: 0.1933 \t cumulative reward:  8.0\n","step: 8880 \t epsilon: 0.1928 \t cumulative reward:  3.9\n","step: 8885 \t epsilon: 0.1924 \t cumulative reward: -2.2\n","step: 8890 \t epsilon: 0.1919 \t cumulative reward:  8.0\n","step: 8895 \t epsilon: 0.1915 \t cumulative reward:  8.0\n","step: 8900 \t epsilon: 0.1910 \t cumulative reward:  8.0\n","step: 8905 \t epsilon: 0.1905 \t cumulative reward:  8.0\n","step: 8910 \t epsilon: 0.1901 \t cumulative reward:  8.0\n","step: 8915 \t epsilon: 0.1896 \t cumulative reward:  8.0\n","step: 8920 \t epsilon: 0.1892 \t cumulative reward:  8.0\n","step: 8925 \t epsilon: 0.1887 \t cumulative reward:  4.0\n","step: 8930 \t epsilon: 0.1883 \t cumulative reward:  8.1\n","step: 8935 \t epsilon: 0.1878 \t cumulative reward:  3.9\n","step: 8940 \t epsilon: 0.1874 \t cumulative reward:  8.0\n","step: 8945 \t epsilon: 0.1869 \t cumulative reward:  8.0\n","step: 8950 \t epsilon: 0.1865 \t cumulative reward:  8.1\n","step: 8955 \t epsilon: 0.1860 \t cumulative reward:  7.8\n","step: 8960 \t epsilon: 0.1855 \t cumulative reward:  4.1\n","step: 8965 \t epsilon: 0.1851 \t cumulative reward:  8.0\n","step: 8970 \t epsilon: 0.1846 \t cumulative reward:  4.0\n","step: 8975 \t epsilon: 0.1842 \t cumulative reward:  4.1\n","step: 8980 \t epsilon: 0.1837 \t cumulative reward:  8.0\n","step: 8985 \t epsilon: 0.1833 \t cumulative reward:  2.1\n","step: 8990 \t epsilon: 0.1828 \t cumulative reward:  4.1\n","step: 8995 \t epsilon: 0.1824 \t cumulative reward:  8.0\n","step: 9000 \t epsilon: 0.1819 \t cumulative reward:  8.1\n","step: 9005 \t epsilon: 0.1815 \t cumulative reward:  4.3\n","step: 9010 \t epsilon: 0.1810 \t cumulative reward:  8.1\n","step: 9015 \t epsilon: 0.1805 \t cumulative reward:  8.1\n","step: 9020 \t epsilon: 0.1801 \t cumulative reward:  6.1\n","step: 9025 \t epsilon: 0.1796 \t cumulative reward:  7.9\n","step: 9030 \t epsilon: 0.1792 \t cumulative reward:  7.9\n","step: 9035 \t epsilon: 0.1787 \t cumulative reward:  7.9\n","step: 9040 \t epsilon: 0.1783 \t cumulative reward:  7.9\n","step: 9045 \t epsilon: 0.1778 \t cumulative reward:  7.9\n","step: 9050 \t epsilon: 0.1774 \t cumulative reward:  4.1\n","step: 9055 \t epsilon: 0.1769 \t cumulative reward:  3.8\n","step: 9060 \t epsilon: 0.1765 \t cumulative reward:  7.9\n","step: 9065 \t epsilon: 0.1760 \t cumulative reward:  7.9\n","step: 9070 \t epsilon: 0.1755 \t cumulative reward:  7.9\n","step: 9075 \t epsilon: 0.1751 \t cumulative reward:  3.8\n","step: 9080 \t epsilon: 0.1746 \t cumulative reward:  3.9\n","step: 9085 \t epsilon: 0.1742 \t cumulative reward:  7.8\n","step: 9090 \t epsilon: 0.1737 \t cumulative reward:  7.9\n","step: 9095 \t epsilon: 0.1733 \t cumulative reward:  7.9\n","step: 9100 \t epsilon: 0.1728 \t cumulative reward:  5.9\n","step: 9105 \t epsilon: 0.1724 \t cumulative reward:  7.9\n","step: 9110 \t epsilon: 0.1719 \t cumulative reward:  7.9\n","step: 9115 \t epsilon: 0.1715 \t cumulative reward:  1.9\n","step: 9120 \t epsilon: 0.1710 \t cumulative reward:  3.6\n","step: 9125 \t epsilon: 0.1705 \t cumulative reward: -2.1\n","step: 9130 \t epsilon: 0.1701 \t cumulative reward:  7.9\n","step: 9135 \t epsilon: 0.1696 \t cumulative reward:  7.9\n","step: 9140 \t epsilon: 0.1692 \t cumulative reward:  7.9\n","step: 9145 \t epsilon: 0.1687 \t cumulative reward:  4.0\n","step: 9150 \t epsilon: 0.1683 \t cumulative reward:  7.9\n","step: 9155 \t epsilon: 0.1678 \t cumulative reward:  4.0\n","step: 9160 \t epsilon: 0.1674 \t cumulative reward:  7.9\n","step: 9165 \t epsilon: 0.1669 \t cumulative reward:  4.0\n","step: 9170 \t epsilon: 0.1665 \t cumulative reward:  4.0\n","step: 9175 \t epsilon: 0.1660 \t cumulative reward:  4.0\n","step: 9180 \t epsilon: 0.1655 \t cumulative reward:  7.9\n","step: 9185 \t epsilon: 0.1651 \t cumulative reward:  7.9\n","step: 9190 \t epsilon: 0.1646 \t cumulative reward:  4.0\n","step: 9195 \t epsilon: 0.1642 \t cumulative reward:  8.1\n","step: 9200 \t epsilon: 0.1637 \t cumulative reward:  3.8\n","step: 9205 \t epsilon: 0.1633 \t cumulative reward:  7.9\n","step: 9210 \t epsilon: 0.1628 \t cumulative reward: -2.2\n","step: 9215 \t epsilon: 0.1624 \t cumulative reward:  8.1\n","step: 9220 \t epsilon: 0.1619 \t cumulative reward:  7.9\n","step: 9225 \t epsilon: 0.1615 \t cumulative reward:  7.9\n","step: 9230 \t epsilon: 0.1610 \t cumulative reward:  7.9\n","step: 9235 \t epsilon: 0.1605 \t cumulative reward:  7.9\n","step: 9240 \t epsilon: 0.1601 \t cumulative reward:  7.9\n","step: 9245 \t epsilon: 0.1596 \t cumulative reward:  7.9\n","step: 9250 \t epsilon: 0.1592 \t cumulative reward:  7.9\n","step: 9255 \t epsilon: 0.1587 \t cumulative reward:  7.9\n","step: 9260 \t epsilon: 0.1583 \t cumulative reward:  8.1\n","step: 9265 \t epsilon: 0.1578 \t cumulative reward:  3.9\n","step: 9270 \t epsilon: 0.1574 \t cumulative reward:  7.9\n","step: 9275 \t epsilon: 0.1569 \t cumulative reward:  5.9\n","step: 9280 \t epsilon: 0.1565 \t cumulative reward:  7.9\n","step: 9285 \t epsilon: 0.1560 \t cumulative reward: -2.2\n","step: 9290 \t epsilon: 0.1555 \t cumulative reward:  4.0\n","step: 9295 \t epsilon: 0.1551 \t cumulative reward:  7.9\n","step: 9300 \t epsilon: 0.1546 \t cumulative reward:  7.9\n","step: 9305 \t epsilon: 0.1542 \t cumulative reward:  3.9\n","step: 9310 \t epsilon: 0.1537 \t cumulative reward:  7.9\n","step: 9315 \t epsilon: 0.1533 \t cumulative reward:  7.9\n","step: 9320 \t epsilon: 0.1528 \t cumulative reward:  7.9\n","step: 9325 \t epsilon: 0.1524 \t cumulative reward:  7.9\n","step: 9330 \t epsilon: 0.1519 \t cumulative reward:  7.9\n","step: 9335 \t epsilon: 0.1515 \t cumulative reward:  3.8\n","step: 9340 \t epsilon: 0.1510 \t cumulative reward:  7.9\n","step: 9345 \t epsilon: 0.1505 \t cumulative reward:  4.1\n","step: 9350 \t epsilon: 0.1501 \t cumulative reward:  8.0\n","step: 9355 \t epsilon: 0.1496 \t cumulative reward:  7.9\n","step: 9360 \t epsilon: 0.1492 \t cumulative reward:  7.9\n","step: 9365 \t epsilon: 0.1487 \t cumulative reward:  3.8\n","step: 9370 \t epsilon: 0.1483 \t cumulative reward:  7.9\n","step: 9375 \t epsilon: 0.1478 \t cumulative reward:  7.9\n","step: 9380 \t epsilon: 0.1474 \t cumulative reward:  7.9\n","step: 9385 \t epsilon: 0.1469 \t cumulative reward:  3.8\n","step: 9390 \t epsilon: 0.1465 \t cumulative reward:  1.9\n","step: 9395 \t epsilon: 0.1460 \t cumulative reward:  5.8\n","step: 9400 \t epsilon: 0.1455 \t cumulative reward:  1.9\n","step: 9405 \t epsilon: 0.1451 \t cumulative reward:  7.9\n","step: 9410 \t epsilon: 0.1446 \t cumulative reward:  7.9\n","step: 9415 \t epsilon: 0.1442 \t cumulative reward:  3.9\n","step: 9420 \t epsilon: 0.1437 \t cumulative reward:  7.9\n","step: 9425 \t epsilon: 0.1433 \t cumulative reward:  7.9\n","step: 9430 \t epsilon: 0.1428 \t cumulative reward:  8.0\n","step: 9435 \t epsilon: 0.1424 \t cumulative reward:  7.9\n","step: 9440 \t epsilon: 0.1419 \t cumulative reward:  4.0\n","step: 9445 \t epsilon: 0.1415 \t cumulative reward:  7.9\n","step: 9450 \t epsilon: 0.1410 \t cumulative reward:  7.9\n","step: 9455 \t epsilon: 0.1405 \t cumulative reward:  7.9\n","step: 9460 \t epsilon: 0.1401 \t cumulative reward:  4.0\n","step: 9465 \t epsilon: 0.1396 \t cumulative reward:  3.8\n","step: 9470 \t epsilon: 0.1392 \t cumulative reward:  7.9\n","step: 9475 \t epsilon: 0.1387 \t cumulative reward:  3.8\n","step: 9480 \t epsilon: 0.1383 \t cumulative reward:  7.9\n","step: 9485 \t epsilon: 0.1378 \t cumulative reward:  7.9\n","step: 9490 \t epsilon: 0.1374 \t cumulative reward:  7.9\n","step: 9495 \t epsilon: 0.1369 \t cumulative reward:  7.9\n","step: 9500 \t epsilon: 0.1365 \t cumulative reward:  7.9\n","step: 9505 \t epsilon: 0.1360 \t cumulative reward:  7.9\n","step: 9510 \t epsilon: 0.1355 \t cumulative reward:  8.1\n","step: 9515 \t epsilon: 0.1351 \t cumulative reward:  8.1\n","step: 9520 \t epsilon: 0.1346 \t cumulative reward:  7.8\n","step: 9525 \t epsilon: 0.1342 \t cumulative reward:  7.8\n","step: 9530 \t epsilon: 0.1337 \t cumulative reward:  8.1\n","step: 9535 \t epsilon: 0.1333 \t cumulative reward:  4.2\n","step: 9540 \t epsilon: 0.1328 \t cumulative reward:  8.1\n","step: 9545 \t epsilon: 0.1324 \t cumulative reward:  8.1\n","step: 9550 \t epsilon: 0.1319 \t cumulative reward:  8.1\n","step: 9555 \t epsilon: 0.1315 \t cumulative reward:  8.1\n","step: 9560 \t epsilon: 0.1310 \t cumulative reward:  8.1\n","step: 9565 \t epsilon: 0.1305 \t cumulative reward:  8.1\n","step: 9570 \t epsilon: 0.1301 \t cumulative reward:  3.8\n","step: 9575 \t epsilon: 0.1296 \t cumulative reward:  8.1\n","step: 9580 \t epsilon: 0.1292 \t cumulative reward:  8.1\n","step: 9585 \t epsilon: 0.1287 \t cumulative reward:  8.1\n","step: 9590 \t epsilon: 0.1283 \t cumulative reward:  8.1\n","step: 9595 \t epsilon: 0.1278 \t cumulative reward:  7.9\n","step: 9600 \t epsilon: 0.1274 \t cumulative reward:  4.2\n","step: 9605 \t epsilon: 0.1269 \t cumulative reward:  8.1\n","step: 9610 \t epsilon: 0.1265 \t cumulative reward:  8.1\n","step: 9615 \t epsilon: 0.1260 \t cumulative reward:  4.0\n","step: 9620 \t epsilon: 0.1255 \t cumulative reward:  8.1\n","step: 9625 \t epsilon: 0.1251 \t cumulative reward:  4.2\n","step: 9630 \t epsilon: 0.1246 \t cumulative reward:  8.1\n","step: 9635 \t epsilon: 0.1242 \t cumulative reward:  8.1\n","step: 9640 \t epsilon: 0.1237 \t cumulative reward:  8.0\n","step: 9645 \t epsilon: 0.1233 \t cumulative reward:  7.8\n","step: 9650 \t epsilon: 0.1228 \t cumulative reward:  8.1\n","step: 9655 \t epsilon: 0.1224 \t cumulative reward:  8.1\n","step: 9660 \t epsilon: 0.1219 \t cumulative reward:  8.1\n","step: 9665 \t epsilon: 0.1215 \t cumulative reward:  8.1\n","step: 9670 \t epsilon: 0.1210 \t cumulative reward:  6.1\n","step: 9675 \t epsilon: 0.1205 \t cumulative reward:  8.1\n","step: 9680 \t epsilon: 0.1201 \t cumulative reward:  4.2\n","step: 9685 \t epsilon: 0.1196 \t cumulative reward:  8.1\n","step: 9690 \t epsilon: 0.1192 \t cumulative reward:  8.1\n","step: 9695 \t epsilon: 0.1187 \t cumulative reward:  7.8\n","step: 9700 \t epsilon: 0.1183 \t cumulative reward:  7.8\n","step: 9705 \t epsilon: 0.1178 \t cumulative reward:  4.2\n","step: 9710 \t epsilon: 0.1174 \t cumulative reward:  8.1\n","step: 9715 \t epsilon: 0.1169 \t cumulative reward:  8.0\n","step: 9720 \t epsilon: 0.1165 \t cumulative reward:  2.1\n","step: 9725 \t epsilon: 0.1160 \t cumulative reward:  3.8\n","step: 9730 \t epsilon: 0.1155 \t cumulative reward:  8.1\n","step: 9735 \t epsilon: 0.1151 \t cumulative reward:  2.1\n","step: 9740 \t epsilon: 0.1146 \t cumulative reward:  8.0\n","step: 9745 \t epsilon: 0.1142 \t cumulative reward:  7.9\n","step: 9750 \t epsilon: 0.1137 \t cumulative reward:  8.1\n","step: 9755 \t epsilon: 0.1133 \t cumulative reward:  8.1\n","step: 9760 \t epsilon: 0.1128 \t cumulative reward:  7.8\n","step: 9765 \t epsilon: 0.1124 \t cumulative reward:  8.1\n","step: 9770 \t epsilon: 0.1119 \t cumulative reward:  4.0\n","step: 9775 \t epsilon: 0.1115 \t cumulative reward:  8.1\n","step: 9780 \t epsilon: 0.1110 \t cumulative reward:  8.1\n","step: 9785 \t epsilon: 0.1105 \t cumulative reward:  8.1\n","step: 9790 \t epsilon: 0.1101 \t cumulative reward: -1.8\n","step: 9795 \t epsilon: 0.1096 \t cumulative reward:  8.1\n","step: 9800 \t epsilon: 0.1092 \t cumulative reward:  7.8\n","step: 9805 \t epsilon: 0.1087 \t cumulative reward:  8.0\n","step: 9810 \t epsilon: 0.1083 \t cumulative reward:  8.1\n","step: 9815 \t epsilon: 0.1078 \t cumulative reward:  8.1\n","step: 9820 \t epsilon: 0.1074 \t cumulative reward:  7.8\n","step: 9825 \t epsilon: 0.1069 \t cumulative reward:  8.1\n","step: 9830 \t epsilon: 0.1065 \t cumulative reward:  8.1\n","step: 9835 \t epsilon: 0.1060 \t cumulative reward:  8.1\n","step: 9840 \t epsilon: 0.1055 \t cumulative reward:  8.1\n","step: 9845 \t epsilon: 0.1051 \t cumulative reward:  2.1\n","step: 9850 \t epsilon: 0.1046 \t cumulative reward:  8.1\n","step: 9855 \t epsilon: 0.1042 \t cumulative reward:  7.8\n","step: 9860 \t epsilon: 0.1037 \t cumulative reward:  8.0\n","step: 9865 \t epsilon: 0.1033 \t cumulative reward:  2.1\n","step: 9870 \t epsilon: 0.1028 \t cumulative reward:  8.1\n","step: 9875 \t epsilon: 0.1024 \t cumulative reward:  8.1\n","step: 9880 \t epsilon: 0.1019 \t cumulative reward:  8.1\n","step: 9885 \t epsilon: 0.1015 \t cumulative reward:  8.0\n","step: 9890 \t epsilon: 0.1010 \t cumulative reward:  8.1\n","step: 9895 \t epsilon: 0.1005 \t cumulative reward:  8.1\n","step: 9900 \t epsilon: 0.1001 \t cumulative reward:  8.1\n","step: 9905 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9910 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9915 \t epsilon: 0.1000 \t cumulative reward:  7.8\n","step: 9920 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9925 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9930 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9935 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9940 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9945 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9950 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9955 \t epsilon: 0.1000 \t cumulative reward:  7.8\n","step: 9960 \t epsilon: 0.1000 \t cumulative reward:  4.0\n","step: 9965 \t epsilon: 0.1000 \t cumulative reward:  8.0\n","step: 9970 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9975 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9980 \t epsilon: 0.1000 \t cumulative reward:  2.1\n","step: 9985 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9990 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 9995 \t epsilon: 0.1000 \t cumulative reward:  8.1\n","step: 10000 \t epsilon: 0.1000 \t cumulative reward:  8.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"small-peter","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1620884700513,"user_tz":240,"elapsed":536585,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"47dcce33-7adc-41f3-e10d-e83fda5a1e17"},"source":["plt.plot(rewards)\n","plt.xlabel('Episode')\n","plt.ylabel('Reward')\n","plt.show()"],"id":"small-peter","execution_count":15,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"differential-scholar"},"source":["## Evaluation\n","\n","Your model should be able to achieve the max possible sum of rewards."],"id":"differential-scholar"},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"c98cb3b95c071dcc3a05a2581b4efbf1","grade":false,"grade_id":"cell-2655f0a36b67a997","locked":true,"schema_version":3,"solution":false,"task":false},"id":"third-cattle","executionInfo":{"status":"ok","timestamp":1620884700515,"user_tz":240,"elapsed":536563,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}}},"source":["def evaluate(dqn, config):\n","    \n","    dqn.q_net.eval()\n","    rewards = []  # to store the cumulative reward for each episode\n","    for _ in range(config['num_episodes_eval']):\n","        gamma = 1.\n","        total_reward = 0.\n","        \n","        # start a new episode\n","        state = dqn.env.reset()\n","        dqn.replay_buffer.store_frame(state)\n","        \n","        # while episode does not terminate\n","        while True:\n","            # get `frame_history_len` most recent frames\n","            # as an input observation to the Q-network\n","            q_input = dqn.replay_buffer.get_observation()\n","            q_input = torch.from_numpy(q_input).to(config['device']).float()\n","            \n","            # during evaluation, we always select the best action\n","            q_values = dqn.q_net(q_input)\n","            action = torch.argmax(q_values).item()\n","            \n","            # execute action and store transition\n","            new_state, reward, done, info = dqn.env.step(action)\n","            dqn.replay_buffer.store_effect(action, reward, done)\n","            state = new_state\n","            dqn.replay_buffer.store_frame(state)\n","            \n","            # cumulative reward\n","            total_reward += gamma * reward\n","            gamma *= config['gamma']\n","            \n","            if done:\n","                break\n","        \n","        # end of episode\n","        rewards.append(total_reward)\n","    \n","    avg_reward = np.mean(rewards)\n","    print(f'average reward: {avg_reward}')\n","    return avg_reward"],"id":"third-cattle","execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"deletable":false,"editable":false,"nbgrader":{"cell_type":"code","checksum":"13fdf46eb1b57c00fadaad20a53e914e","grade":true,"grade_id":"cell-f81ee01dc87f6cbb","locked":true,"points":45,"schema_version":3,"solution":false,"task":false},"id":"excess-superintendent","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1620884700516,"user_tz":240,"elapsed":536546,"user":{"displayName":"ASHISH REDDY PODDUTURI","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5AVaIc1nlKC5qSz1WUSRIPcQ5ew4BySXXBuPH=s64","userId":"01854260805396608806"}},"outputId":"f250021c-ebc8-413a-8804-ccb0937cc6ec"},"source":["avg_reward = evaluate(dqn, config)"],"id":"excess-superintendent","execution_count":17,"outputs":[{"output_type":"stream","text":["average reward: 8.1\n"],"name":"stdout"}]}]}